%$$\lay{z}{1}{^i_j}$$
%$$\tensor{z}{^{[1]}^i_j}$$
%$$\tensor{\lay{z}{1}}{^i_j}$$


\section{Neural Networks: A Single Hidden Layer}
Suppose we wish to consider the binary classification problem given the training set $(x,y)$ with $x\in\R^{s_0\times n}$ and $y\in\{0,1\}^n$.  Usually with logistic regression we have the following type of structure:
$$[x^1,...,x^{s_0}]\layerfctn{\varphi}[z]\layerfctn{g}[a]\layerfctn{=}\hat{y},$$
where
$$z=\varphi(x)=w^Tx+b,$$
is our affine-linear transformation, and 
$$a=g(z)=\sigma(z)$$
is our sigmoid function.  Such a structure will be called a \textit{network}, and the $[a]$ is known as the \textit{activation node}.  Logistic regression can be too simplistic of a model for many situations.  To modify this model to handle more complex situations, we introduce a new ``hidden layer'' of nodes with their own (possibly different) activation functions.  That is, we consider a network of the following form:
\begin{align*}
	\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{s_0}
	\end{bmatrix}}_{\text{Layer } 0}&\layerfctn{\lay{\varphi}{1}}\underbrace{\begin{bmatrix}
		\lay{z}{1}{^1}\\
		\vdots\\
		\lay{z}{1}{^{s_1}}
	\end{bmatrix}\layerfctn{\lay{g}{1}}
	\begin{bmatrix}
		\lay{a}{1}{^1}\\
		\vdots\\
		\lay{a}{1}{^{s_1}}
	\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\varphi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}
	\end{bmatrix}\layerfctn{\lay{g}{2}}
	\begin{bmatrix}
		\lay{a}{2}
	\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\hat{y},
\end{align*}
where
$$\lay{\varphi}{1}:\R^{s_0}\to\R^{s_1},\qquad \lay{\varphi}{1}(x)=\lay{W}{1}x+\lay{b}{1},$$
$$\lay{\varphi}{2}:\R^{s_1}\to\R,\qquad \lay{\varphi}{2}(x)=\lay{W}{2}x+\lay{b}{2},$$
and $\lay{W}{1}\in\R^{s_1\times s_0}, \lay{W}{2}\in\R^{1\times s_1}, \lay{b}{1}\in\R^{s_1}, \lay{b}{2}\in\R$, and $\lay{g}{\ell}$ is a \textit{broadcasted} activator function (e.g., the sigmoid function $\sigma(z)$, or $\tanh(z)$, or $\relu(z)$).  Such a network is called a $2$-layer neural network where $x$ is the input layer (called layer-$0$), $\lay{a}{1}$ is a hidden layer (called layer-$1$), and $\lay{a}{2}$ is the output layer (called layer-$2$).

\begin{defn}
	Suppose $g:\R\to\R$ is any function.  Then we say $\cl{g}:\R^{m\times n}\to\R^{m\times n}$ is the \textbf{broadcast} of $g$ if
	\begin{align*}
		\cl{g}(A)&=\cl{g}(A^i_je_i^j)\\
		&=g(A^i_j)e_i^j,
	\end{align*}
	where $A\in\R^{m\times n}$ and $\{e_i^j:1\leq i\leq m, 1\leq j\leq n\}$ is the standard basis for $\R^{m\times n}$.  In practice, we will write $g=\cl{g}$ for a broadcasted function, and let the context determine the meaning of $g$.
\end{defn}

Let us lay out all of these functions explicitly (in the Smooth Category) as to facilitate our later computations for our cost function and our gradients.
To this end:
\begin{align*}
	&\lay{\varphi}{1}:\R^{s_0}\to\R^{s_1},\qquad &d\lay{\varphi}{1}:T\R^{s_0}\to T\R^{s_1},\\
	&\lay{z}{1}=\lay{\varphi}{1}(x)=\lay{W}{1}x+\lay{b}{1},&d\lay{\varphi}{1}_x(v)=\lay{W}{1}v;
\end{align*}

\begin{align*}
	&\lay{g}{1}:\R^{s_1}\to\R^{s_1},\qquad &
	d\lay{g}{1}:T\R^{s_1}\to T\R^{s_1},\\
	&\lay{a}{1}=\lay{g}{1}(\lay{z}{1}),\qquad 
	&\frac{\partial\lay{a}{1}{^\mu}}{\partial \lay{z}{1}{^\nu}}=\delta^\mu_\nu\lay{g}{1}{'}(\lay{z}{1}{^\mu});
\end{align*}

\begin{align*}
&\lay{\varphi}{2}:\R^{s_1}\to\R^{s_2},\qquad	
&d\lay{\varphi}{2}:T\R^{s_1}\to T\R^{s_2},\\
&\lay{z}{2}=\lay{\varphi}{2}(\lay{a}{1})=\lay{W}{2}\lay{a}{1}+\lay{b}{2},\qquad &d\lay{\varphi}{2}_{\lay{a}{2}}(v)=\lay{W}{2}v;
\end{align*}

\begin{align*}
	&\lay{g}{2}:\R^{s_2}\to\R^{s_2},\qquad 
	&d\lay{g}{2}:T\R^{s_2}\to T\R^{s_2},\\
	&\lay{a}{2}=\lay{g}{2}(\lay{z}{2}),\qquad 
	&\frac{\partial\lay{a}{2}{^\mu}}{\partial \lay{z}{2}{^\nu}}=\delta^\mu_\nu\lay{g}{2}{'}(\lay{z}{2}{^\mu}).
\end{align*}
That is, given an input $x\in\R^{s_0}$, we get a predicted value $\hat{y}\in\R^{s_2}$ of the form
$$\hat{y}=\lay{g}{2}\circ\lay{\varphi}{2}\circ\lay{g}{1}\circ\lay{\varphi}{1}(x).$$
This compositional function is known as \textit{forward propagation}.

\begin{comment}
Since we wish to optimize our model with respect to our parameter $\lay{W}{\ell}$ and $\lay{b}{\ell}$,  we consider a generic loss function $\L:\R\times\R\to\R$, $\L(\hat{y},y)$, and by acknowledging the potential abuse of notation, we assume $y$ is fixed, and consider the aforementioned as a function of a single-variable
$$\L_y:\R\to\R, \qquad \L_y(\hat{y})=\L(\hat{y},y).$$
We now define the compositional function
$$F:\R^{s_0}\to\R,\qquad F(x)=\L_y\circ\lay{g}{2}\circ\lay{\varphi}{2}\circ\lay{g}{1}\circ\lay{\varphi}{1}(x).$$


As we mentioned before, we wish to optimize with respect to our parameters, but our above composition doesn't make this dependence explicit for computations.  To this end, we first previously considered the generic affine-linear transformation
$$\varphi:\R^m\to\R^k,\qquad\varphi(x)=Wx+b,$$
with $W\in\R^{k\times m}, b\in\R^k$.  We now change our point-of-view and consider the related
$$\phi:\R^{k\times m}\times\R^k\to\R^k,\qquad \phi(W, b)=Wx+b,$$
for some fixed $x\in\R^m$.  Then we see that
$$d\phi:T\R^{k\times m}\times T\R^k\to T\R^k,$$
\begin{align*}
	d\phi_{(W,b)}(V,v)&=\rest{\frac{d}{dt}}_{t=0}\phi(W+tV, b+tv)\\
	&=\rest{\frac{d}{dt}}_{t=0}(W+tV)x+(b+tv)\\
	&=Vx+v.
\end{align*}
This leads to the further decomposition of the form
$$\phi(W,b)=\psi(W)+\id_{\R^k}(b),$$
where $\id_{\R^k}:\R^k\to\R^k$ is the identity function, and $\psi:\R^{k\times m}\to\R^k$ is given by
$$\psi(W)=Wx.$$
Then by the above computation, we have that
$$d\psi_W(V)=Vx.$$
Moreover, suppose
$$\{E_1^1,E_1^2,...,E_1^m,E_2^1,E_2^2,...,E_2^m,...,E_k^1,E_k^2,...,E_k^m\}$$
is an ordering of the standard basis $\{E_\alpha^\beta\}$ for $\R^{k\times m}$ with
$$[E_\alpha^\beta]^i_j=\delta^i_\alpha\delta_j^\beta,$$ and
$$V=V^i_jE_i^j,$$
then $d\psi_W\in\R^{k\times mk}$ with matrix-representation
$$d\psi_W=\begin{bmatrix}
	x^T&0&0&\cdots &0\\
	0&x^T&0&\cdots &0\\
	0&0&x^T&\cdots &0\\
	\vdots &\vdots &\ddots &\ddots &\vdots\\
	0&0&0&\cdots &x^T
\end{bmatrix},$$
where each $0$ represents $0\in\R^{1\times m}$, and
\begin{align*}
	d\psi_W(V)&=\begin{bmatrix}
	x^T&0&0&\cdots &0\\
	0&x^T&0&\cdots &0\\
	0&0&x^T&\cdots &0\\
	\vdots &\vdots &\ddots &\ddots &\vdots\\
	0&0&0&\cdots &x^T
\end{bmatrix}\begin{bmatrix}
	V_1^1\\
	V_2^1\\
	\vdots\\
	V_m^1\\
	\vdots\\
	V_1^k\\
	V_2^k\\
	\vdots\\
	V_m^k
\end{bmatrix}\\
&=\begin{bmatrix}
	x^T(V^1)^T\\
	x^T(V^2)^T\\
	\vdots\\
	x^T(V^k)^T
\end{bmatrix}.
\end{align*}
Finally, since for any smooth $f:\R^m\to\R^n$, we have that






Taking this further, we now consider the map
$$\Phi:\R^{k\times m}\times\R^k\times\R^m\to\R^k,\qquad \Phi(W, b, x)=Wx+b,$$
and then computing our differential for 
$$d\Phi:T\R^{k\times m}\times T\R^k\times T\R^m\to T\R^k,$$
yields
\begin{align*}
	d\Phi_{(W,b,x)}(V,v,p)&=\rest{\frac{d}{dt}}_{t=0}\Phi(W+tV, b+tv, x+tp)\\
	&=\rest{\frac{d}{dt}}_{t=0}\left((W+tV)(x+tp)+(b+tv)\right)\\
	&=\rest{\frac{d}{dt}}_{t=0}\left(Wx+tVx+tWv+t^2Vp+b+tv\right)\\
	&=Vx+v+Wp\\
	&=d\phi_{(W,b)}(V,v)+d\varphi_x(p)
\end{align*}
This function $\Phi$ is what we want in our compositional-function, and so we redefine $F$ as
\small
\begin{align*}
	F(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}\circ(\lay{W}{2},\lay{b}{2},\lay{g}{1}\circ\lay{\Phi}{1}\circ(\lay{W}{1},\lay{b}{1},x))
\end{align*}
\normalsize
Taking the exterior derivative, and noting the composition turns into matrix multiplication on the tangent space, we get
\small
\begin{align*}
	d&F_{(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x)}(U,u,V,v,p)\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\Phi}{2}_{(\lay{W}{2},\lay{b}{2},\lay{a}{1})}\cdot (U, u, d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\Phi}{1}_{(\lay{W}{1},\lay{b}{1},x)}(V,v,p))\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot(d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2})}(U,u)+d\lay{\varphi}{2}_{\lay{a}{1}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot(d\lay{\phi}{1}_{(\lay{W}{1},\lay{b}{1})}(V,v)+d\lay{\varphi}{1}_x(p)))\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2},\{\lay{a}{1}\})}(U,u)\\
	&\qquad +d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\varphi}{2}_{\lay{a}{1},\{\lay{W}{2},\lay{b}{2}\}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\phi}{1}_{(\lay{W}{1},\lay{b}{1}),\{x\}}(V,v)\\
	&\qquad+d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\varphi}{2}_{\lay{a}{1},\{\lay{W}{2},\lay{b}{2}\}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\varphi}{1}_{x,\{\lay{W}{1},\lay{b}{1}\}}(p)\\
	&=:\lay{dF}{2}+\lay{dF}{1}+\lay{dF}{0},
\end{align*}
\normalsize
where $\lay{dF}{2}$ represents the differential with respect to the parameters going from layer-$1$ to layer-$2$, $\lay{dF}{1}$ represents the differential with respect to the parameters going from layer-$0$ to layer-$1$, and $\lay{dF}{0}$ represents the differential with respect to $x$.

Recalling that the gradient is the transpose of the exterior derivative in Euclidean space, we then conclude that
\begin{align*}
	\nabla F&=(dF)^T\\
	&=\left(\lay{dF}{2}+\lay{dF}{1}+\lay{dF}{0}\right)^T\\
	&=\nabla\lay{F}{2}+\nabla\lay{F}{1}+\nabla\lay{F}{0},
\end{align*}
and respectively,
\begin{align*}
	\nabla\lay{F}{2}&=\left(d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2},\{\lay{a}{1}\})}\right)^T
\end{align*}


\end{comment}

\subsection{Backpropagation}

Since we wish to optimize our model with respect to our parameter $\lay{W}{\ell}$ and $\lay{b}{\ell}$,  we consider a generic loss function $\L:\R^{s_2}\times\R^{s_2}\to\R$, $\L(\hat{y},y)$, and by acknowledging the potential abuse of notation, we assume $y$ is fixed, and consider the aforementioned as a function of a single-variable
$$\L_y:\R^{s_2}\to\R, \qquad \L_y(\hat{y})=\L(\hat{y},y).$$
We also define the function
$$\Phi(A,u,\xi)=A\xi+u,$$
and note that we're suppressing a dependence on the layer $\ell$ which only affects our domain and range of $\Phi$ (and not the actual calculations involving the derivatives).  Moreover, in coordinates we see that
\begin{align*}
	\frac{\partial\Phi^i}{\partial A^\mu_\nu}&=\frac{\partial}{\partial A^\mu_\nu}(A^i_j\xi^j+u^i)\\
	&=(\delta^i_\mu\delta_j^\nu \xi^j)\\
	&=\delta^i_\mu \xi^\nu;
\end{align*}
\begin{align*}
	\frac{\partial\Phi^i}{\partial u^\mu}&=\frac{\partial}{\partial u^\mu}(A^i_j\xi^j+u^i)\\
	&=\delta^i_\mu;
\end{align*}
and
\begin{align*}
	\frac{\partial\Phi^i}{\xi^\mu}&=\frac{\partial}{\partial \xi^\mu}(A^i_j\xi^j+u^i)\\
	&=A^i_j\delta^j_\mu\\
	&=A^i_\mu.
\end{align*}





We now define the compositional function
$$F:\R^{s_2\times s_1}\times \R^{s_2}\times \R^{s_1\times s_0}\times \R^{s_1}\times \R^{s_0}\to\R$$
given by
$$F(C,c,B,b,x)=\L_y\circ\lay{g}{2}\circ\Phi\circ(\id\times\id\times (\lay{g}{1}\circ\Phi))(C,c,B,b,x).$$
We first introduce an error term $\lay{\delta}{2}\in\R^{s_2}$ defined by
\begin{align*}
	\lay{\delta}{2}:&=\nabla (\L_y\circ\lay{g}{2})(\lay{z}{2})\\
	&=(d\L_y\circ\lay{g}{2})_{\lay{z}{2}})^T.
\end{align*}
Now we calculate the gradient $\frac{\partial F}{\partial C}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial C^\mu_\nu}&=\frac{\partial}{\partial C^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{a}{1})\right]\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\frac{\partial}{\partial C^\mu_\nu}(C^j_i\lay{a}{1}{^i}+c^j)\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\delta^j_\mu \lay{a}{1}{^\nu}\\
	&=\lay{\delta}{2}{_\mu}\lay{a}{1}{^\nu}\\
	&=[\lay{a}{1}\lay{\delta}{2}{^T}]_\mu^\nu
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial C}&=\left[
		\frac{\partial F}{\partial C^\mu_\nu}
	\right]^T\\
	&=\left[\lay{\delta}{2}_\mu \lay{a}{1}{^\nu}\right]^T\\
	&=\lay{\delta}{2}\lay{a}{1}{^T}.
\end{align*}
Moreover, we also calculate
\begin{align*}
	\frac{\partial F}{\partial c^\mu}&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\delta^j_\mu,
\end{align*}
and hence that
$$\frac{\partial F}{\partial c}=\lay{\delta}{2}.$$

Next we introduce another error term $\lay{\delta}{1}\in\R^{s_1}$ defined by
\begin{align*}
	\lay{\delta}{1}=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}
\end{align*}
with coordinates
\begin{align*}
	(\lay{\delta}{1}{^\mu})^T&=\sum_{i=1}^{s_2}\sum_{j=1}^{s_1}\lay{\delta}{2}{^i}C^i_j\lay{g}{1}{'}(\lay{z}{1}{^j})\delta^j_\mu\\
	&=\sum_{i=1}^{s_2}\lay{\delta}{2}{^i}C^i_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})
\end{align*}

and now calculate the gradient $\frac{\partial F}{\partial B}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial B^\mu_\nu}&=\frac{\partial}{B^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{g}{1}(Bx+b))\right]\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{s_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{s_1}\frac{\partial \lay{a}{1}{^\rho}}{\partial \lay{z}{1}{^\lambda}}\frac{\partial \Phi^\lambda}{\partial B^\mu_\nu}\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{s_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{s_1}
	\delta^\rho_\lambda \lay{g}{1}{'}(\lay{z}{1}{^\rho})
	\delta^{\lambda}_\mu x^\nu\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{s_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{s_1}C^j_\rho\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{s_2}\lay{\delta}{2}{^j}C^j_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})x^\nu\\
	&=\lay{\delta}{1}{_\mu}x^\nu\\
	&=\left[x\lay{\delta}{1}{^T}\right]^\nu_\mu,
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial B}&=\left[\frac{\partial F}{\partial B^\mu_\nu}\right]^T\\
	&=\lay{\delta}{2}x^T.
\end{align*}
Moreover, from the above calculation, we immediately see that
\begin{align*}
	\frac{\partial F}{\partial b^\mu}&=\lay{\delta}{1}.
\end{align*}

In summary, we've computed the following gradients
\begin{align*}
	\frac{\partial F}{\partial\lay{W}{2}}&=\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial F}{\partial\lay{b}{2}}&=\lay{\delta}{2}\\
	\frac{\partial F}{\partial\lay{W}{1}}&=\lay{\delta}{1}x^T\\
	\frac{\partial F}{\partial\lay{b}{1}}&=\lay{\delta}{1},
\end{align*}
where
\begin{align*}
	\lay{\delta}{2}&=[d(\L_y\circ\lay{g}{2})_{\lay{z}{2}}]^T\\
	\lay{\delta}{1}&=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}.
\end{align*}

Finally, we recall that our cost function $\J$ is the average sum of our loss function $\L$ over our training set, we get that
\begin{align*}
	\J(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1})&=\frac{1}{n}\sum_{j=1}^nF(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x_j),
\end{align*}
and hence that
\begin{align*}
	\frac{\partial\J}{\partial\lay{W}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\lay{a}{1}{_j}^T=\frac{1}{n}\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial\J}{\partial\lay{b}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\\
	\frac{\partial\J}{\partial\lay{W}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}x_j^T=\frac{1}{n}\lay{\delta}{1}x^T\\
	\frac{\partial\J}{\partial\lay{b}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}
\end{align*}




\subsection{Vectorization in Python}
We return the network given by
\begin{align*}
	\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{s_0}
	\end{bmatrix}}_{\text{Layer } 0}&\layerfctn{\lay{\varphi}{1}}\underbrace{\begin{bmatrix}
		\lay{z}{1}{^1}\\
		\vdots\\
		\lay{z}{1}{^{s_1}}
	\end{bmatrix}\layerfctn{\lay{g}{1}}
	\begin{bmatrix}
		\lay{a}{1}{^1}\\
		\vdots\\
		\lay{a}{1}{^{s_1}}
	\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\varphi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}
	\end{bmatrix}\layerfctn{\lay{g}{2}}
	\begin{bmatrix}
		\lay{a}{2}
	\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\hat{y},
\end{align*}
and show how such a model would be trained using python below.
