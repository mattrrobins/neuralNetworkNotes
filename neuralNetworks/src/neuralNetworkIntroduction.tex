


\section{Neural Networks: A Single Hidden Layer}
Suppose we wish to consider the binary classification problem given the training set $(x,y)$ with $x\in\R^{\lay{n}{0}\times N}$ and $y\in\{0,1\}^{1\times N}$.  Usually with logistic regression we have the following type of structure:
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{1\times\lay{n}{0}}
		\arrow[d, swap, "w"]
		&\R^{}
		\arrow[d, swap, "b"]
		&{}
		&\{0,1\}
		\arrow[d, swap, "y"]
		&{}
		\\
		\R^{\lay{n}{0}}
		\arrow[r, "x"]
		&\boxed{\phi}
		\arrow[r, "u"]
		&\boxed{\psi}
		\arrow[r, "z"]
		&\boxed{\sigma}
		\arrow[r, "a"]
		&\boxed{\L}
		\arrow[r, "\text{loss}"]
		&\R
	\end{tikzcd}
\end{equation*}
Such a structure will be called a \textit{network}, and the $a$ is known as the \textit{activation node}.  Logistic regression can be too simplistic of a model for many situations, e.g., if the dataset isn't linearly separable (i.e., there doesn't exist some well-defined decision boundary built from a linear-surface), then logistic regression won't give a high-accuracy model.  To modify this model to handle more complex situations, we introduce a new ``hidden layer'' of nodes with their own (possibly different) activation functions.  That is, we consider a network of the following form:
{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{b}{1}"]
		&{}
		&\R^{1\times\lay{n}{1}}
		\arrow[d, swap, "\lay{w}{2}"]
		&\R
		\arrow[d, swap, "\lay{b}{2}"]
		&{}
		&\{0,1\}
		\arrow[d, swap, "y"]
		&{}
		\\
		\R^{\lay{n}{0}}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{\psi}{2}}
		\arrow[r, "\lay{z}{2}"]
		&\boxed{\lay{G}{2}}
		\arrow[r, "\lay{a}{2}"]
		&\boxed{\L}
		\arrow[r, "\text{loss}"]
		&\R
	\end{tikzcd}
\end{equation*}
}

In the above diagram, we use $\lay{\cdot}{0}$ to denote everything in layer-$0$, i.e., the input layer; we use $\lay{\cdot}{1}$ to denote everything in layer-$1$, i.e., the hidden layer; and we use $\lay{\cdot}{2}$ to denote everything in layer-$2$, i.e., the output layer.  Moreover, we have the functions (where we suppress the layer-notation)
\begin{itemize}
	\item $$\phi:\R^{n\times m}\times\R^m\to\R^n,\qquad u:=\phi(w,a)=wa,$$
	\item $$\psi:\R^n\times\R^n\to\R^n,\qquad z:=\psi(b,u)=u+b,$$
	\item $$G:\R^n\to\R^n,\qquad a:=G(z),$$
	where $G$ is the broadcasting of some activating function $g:\R\to\R$.
\end{itemize}


\begin{defn}
	Suppose $g:\R\to\R$ is any function.  Then we say $G:\R^n\to\R^n$ is the \textbf{broadcast} of $g$ from $\R$ to $\R^n$ if
	\begin{align*}
		G(v)&=G(v^ie_i)\\
		&=g(v^i)e_i,
	\end{align*}
	where $v\in\R^{n}$ and $\{e_i:1\leq i\leq n\}$ is the standard basis for $\R^n$.  In practice, we will sometimes write $g=G$ for a broadcasted function, and let the context determine the meaning of $g$.
\end{defn}

\begin{lem}\label{lem: broadcastingDifferential}
	Suppose $g:\R\to\R$ is any smooth function and $G:\R^n\to\R^n$ is the broadcasting of $g$ from $\R$ to $\R^n$.  Then the differential $dG_z:T_z\R^n\to T_{G(z)}\R^n$ is given by
	$$dG_z(\xi)=[g'(z^i)]\odot [\xi^i],$$
	where $\odot$ is the Hadamard product (also know as component-wise multiplication), and has matrix-representation in $\R^{m\times m}$ given by
	$$[dG_z]^i_j =\delta^i_j g'(z^i).$$
	We use the notation
	$$G'(z):=[g'(z^i)]\in\R^n,$$
	and thus may write
	$$dG_z(v)=G'(z)\odot \xi.$$
	Furthermore, we have that for $\zeta\in T_{G(z)}\R^n$,
	$$rG_z(\zeta)=G'(z)\odot\zeta.$$
\end{lem}

\begin{proof}
	We calculate
	\begin{align*}
		dG_z(\xi)&=\rest{\frac{d}{dt}}_{t=0}G(z+t\xi)\\
		&=\rest{\frac{d}{dt}}_{t=0}(g(z^i+t\xi^i))\\
		&=(g'(z^i)\xi^i)\\
		&=[g'(z^i)]\odot[\xi^i],
	\end{align*}
	and letting $e_1,...e_m$ denote the usual basis for $T_z\R^m$ (identified with $\R^m$), we see that
	\begin{align*}
		dG_z(e_j)&=[g'(z^i)]\odot e_j\\
		&=g'(z^j)e_j,
	\end{align*}
	from which conclude that $dG_z$ is diagonal with $(j,j)$-th entry $g'(z^j)$ as desired.
	
	Furthermore, for $\zeta\in T_{G(z)}\R^n$, we have that
	\begin{align*}
		\ip{rG_z(\zeta),\xi}_{\R^n}&=\ip{\zeta,dG_z(\xi)}_{\R^n}\\
		&=\ip{\zeta,G'(z)\odot\xi}_{\R^n}\\
		&=\ip{G'(z)\odot\zeta,\xi}_{\R^n},
	\end{align*}
	and the result follows.
\end{proof}

Returning to our network, we see call the full composition of network functions resulting in $\lay{a}{2}$, the \textit{forward propagation}.  That is, given an example $x\in\R^{\lay{n}{0}}$, we have that
$$\lay{a}{2}=\lay{G}{2}(\lay{\psi}{2}(\lay{b}{2},\lay{\phi}{2}(\lay{w}{2},\lay{G}{1}(\lay{\psi}{1}(\lay{b}{1},\lay{\phi}{1}(\lay{w}{1},x)))))).$$



\subsection{Activation Functions}
There are mainly only a handful of activating functions we consider for our non-linearity conditions (but many more built from these that follow).

\subsubsection{The Sigmoid Function}
We have the sigmoid function $\sigma(z)$ given by
$$\sigma:\R\to(0,1),\qquad \sigma(z)=\frac{1}{1+e^{-z}}.$$
We note that since
\begin{align*}
	1-\sigma(z)&=1-\frac{1}{1+e^{-z}}\\
	&=\frac{e^{-z}}{1+e^{-z}}
\end{align*}
\begin{align*}
	\sigma'(z)&=\frac{e^{-z}}{(1+e^{-z})^2}\\
	&=\frac{1}{1+e^{-z}}\cdot\frac{e^{-z}}{1+e^{-z}}\\
	&=\sigma(z)(1-\sigma(z))
\end{align*}



\subsubsection{The Hyperbolic Tangent Function}
We have the hyperbolic tangent function $\tanh(z)$ given by
$$\tanh:\R\to(-1,1),\qquad\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}.$$
We then calculate
\begin{align*}
	\tanh'(z)&=\frac{(e^z+e^{-z})(e^z+e^{-z})-(e^z-e^{-z})(e^z-e^{-z})}{(e^z+e^{-z})^2}\\
	&=\frac{(e^z+e^{-z})^2}{(e^z+e^{-z})^2}-\frac{e^z-e^{-z})^2}{(e^z+e^{-z})^2}\\
	&=1-\tanh^2(z).
\end{align*}

Furthermore, we note that
$$\frac{1}{2}\left(\tanh\left(\frac{z}{2}\right)+1\right)=\sigma(z).$$
Indeed,
\begin{align*}
	1+\tanh{\frac{z}{2}}&=1+\frac{e^{\frac{z}{2}}-e^{-\frac{z}{2}}}{e^{\frac{z}{2}}+e^{-\frac{z}{2}}}\\
	&=\frac{e^{\frac{z}{2}}+e^{-\frac{z}{2}}+e^{\frac{z}{2}}-e^{-\frac{z}{2}}}{e^{\frac{z}{2}}+e^{-\frac{z}{2}}}\\
	&=2\frac{e^{\frac{z}{2}}}{e^{\frac{z}{2}}+e^{-\frac{z}{2}}}\\
	&=2\frac{1}{1+e^{-z}}\\
	&=2\sigma(z),
\end{align*}
as desired.


\subsubsection{The Rectified Linear Unit Function}
We have the leaky-ReLU function $\relu(z;\beta)$ given by
$$\relu:\R\to\R,\qquad \relu(z;\beta)=\max\{\beta z, z\},$$
for some $\beta>0$ (typically chosen very small).

We have the rectified linear unit function $\relu(z)$ given by setting $\beta=0$ in the leaky-ReLu function, i.e.,
$$\relu:\R\to[0,\infty),\qquad\relu(z)=\relu(z;\beta=0)=\max\{0,z\}.$$
We then calculate
\begin{align*}
	\relu'(z;\beta)&=\begin{cases}
		\beta&z<0\\
		1&z\geq0
	\end{cases}\\
	&=\beta\chi_{(-\infty,0)}(z)+\chi_{[0,\infty)}(z),
\end{align*}
where
$$\chi_A(z)=\begin{cases}
	1&z\in A\\
	0&z\notin A
\end{cases},$$
is the indicator function.


\subsubsection{The Softmax Function}
We finally have the softmax function $\softmax(z)$ given by
$$\softmax:\R^n\to\R^n,\qquad \softmax(z)=\frac{1}{\sum_{j=1}^ne^{z^j}}\begin{pmatrix}
	e^{z^1}\\
	e^{z^2}\\
	\vdots\\
	e^{z^n}
\end{pmatrix},$$
which we typically use this function on the outer-layer to obtain a probability distribution over our predicted labels when dealing with multi-class regression.  Let
$$S^i=x^i\circ\softmax(z),$$
denote the $i$-th component of $\softmax(z)$, and so we calculate
\begin{align*}
	\frac{\partial S^i}{\partial z^j}&=\frac{\partial}{\partial z^j}\left[\left(\sum_{k=1}^me^{z^k}\right)^{-1}e^{z^i}\right]\\
	&=-\left(\sum_{k=1}^me^{z^k}\right)^{-2}\left(\sum_{k=1}^me^{z^k}\delta_j^k\right)e^{z^i}+\left(\sum_{k=1}^me^{z^k}\right)^{-1}e^{z^i}\delta^i_j\\
	&=-\left(\sum_{k=1}^me^{z^k}\right)^{-2}e^{z^j}e^{z^i}+S^i\delta^i_j\\
	&=-S^jS^i+S^i\delta^i_j\\
	&=S^i(\delta^i_j-S^j).
\end{align*}
That is, as a map $dS_z:T_z\R^m\to T_{S(z)}\R^m$, we have that
$$dS_z=[S^i(\delta^i_j-S_j)]^i_j,$$
and we make note that $dS_z$ is symmetric (i.e., it's also the reverse differential).



\subsection{Backward Propagation}

We consider a neural network of the form
{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{b}{1}"]
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[d, swap, "\lay{w}{2}"]
		&\R^{\lay{n}{2}}
		\arrow[d, swap, "\lay{b}{2}"]
		&{}
		&\R^{\lay{n}{2}}
		\arrow[d, swap, "y"]
		&{}
		\\
		\R^{\lay{n}{0}}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{\psi}{2}}
		\arrow[r, "\lay{z}{2}"]
		&\boxed{\lay{G}{2}}
		\arrow[r, "\lay{a}{2}"]
		&\boxed{\L}
		\arrow[r, "\text{loss}"]
		&\R
	\end{tikzcd}
\end{equation*}
}

where we have the functions:
\begin{enumerate}
	\item $$\lay{G}{\ell}:\R^{\lay{n}{\ell}}\to\R^{\lay{n}{\ell}}$$
is the broadcasting of the activation unit $\lay{g}{\ell}:\R\to\R$.
	\item $$\lay{\phi}{\ell}:\R^{\lay{n}{\ell}\times\lay{n}{\ell-1}}\times\R^{\lay{n}{\ell-1}}\to\R^{\lay{n}{\ell}}$$
	is given by
	$$\lay{\phi}{\ell}(w,x)=wx.$$
	\item $$\lay{\psi}{\ell}:\R^{\lay{n}{\ell}}\times\R^{\lay{n}{\ell}}\to\R^{\lay{n}{\ell}}$$
	is given by
	$$\lay{\psi}{\ell}(b,x)=x+b.$$
	\item $$\L:\R^{\lay{n}{2}}\times\R^{\lay{n}{2}}\to\R$$
	is the given loss-function.
\end{enumerate}


We now consider back-propagating through the neural network via ``reverse exterior differentiation''.  We represent our various reverse derivatives via the following diagram:

{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		&\R^{\lay{n}{1}}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		&\R^{\lay{n}{2}}
		&{}
		&\R^{\lay{n}{2}}
		\arrow[d]
		&{}
		\\
		\R^{\lay{n}{0}}
		&\boxed{\lay{\phi}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "r_1"]
		&\boxed{\lay{\psi}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "\cl{r}_1"]
		&\boxed{\lay{G}{1}}
		\arrow[l, swap, "r"]
		&\boxed{\lay{\phi}{2}}
		\arrow[l, swap, "r"]
		\arrow[u, "r_2"]
		&\boxed{\lay{\psi}{2}}
		\arrow[l, swap, "r"]
		\arrow[u, "\cl{r}_2"]
		&\boxed{\lay{G}{2}}
		\arrow[l, swap, "r"]
		&\boxed{\L}
		\arrow[l, swap, "r"]
		&\R
		\arrow[l]
	\end{tikzcd}
\end{equation*}
}

First, we need to consider our individual derivatives:
\begin{enumerate}
	\item Suppose $G:\R^n\to\R^n$ is the broadcasting of $g:\R\to\R$.  Then for $(x,\xi)\in T\R^n$, we have that
	\begin{align*}
		dG_x(\xi)&=G'(x)\odot\xi\\
		&=\diag(G'(x))\cdot\xi
	\end{align*}
	and for any $\zeta\in T_{G(x)}\R^n$, the reverse derivative is given by
	\begin{align*}
		rG_x(\zeta)&=G'(x)\odot\zeta\\
		&=\diag(G'(x))\cdot\zeta.
	\end{align*}
	
	\item Suppose $\phi:\R^{m\times n}\times\R^n\to\R^m$ is given by $$\phi(w,x)=wx.$$
	Then we have two differentials to consider:
	\begin{enumerate}
		\item For any $(w,x)\in\R^{m\times n}\times\R^n$ and any $\xi\in T_x\R^n$, we have that
		\begin{align*}
			d\phi_{(w,x)}(\xi)&=w\xi\\
			&=L_w(\xi);
		\end{align*}
		and for any $\zeta\in T_{\phi(w,x)}\R^m$, we have the reverse derivative
		\begin{align*}
			r\phi_{(w,x)}(\zeta)&=w^T\zeta\\
			&=L_{w^T}(\zeta);
		\end{align*}
		where $L_A(B)=AB$, i.e., left-multiplication by $A$.
		
		\item For any $(w,x)\in\R^{m\times n}\times\R^n$ and any $\eta\in T_w\R^{m\times n}$ we have that
		\begin{align*}
			d_1\phi_{(w,x)}(\eta)&=\eta x\\
			&=R_x(\eta);
		\end{align*}
		and for any $\zeta\in T_{\phi(w,x)}\R^m$, we have the reverse derivative
		\begin{align*}
		r_1\phi_{(w,x)}(\zeta)&=\zeta x^T\\
		&=R_{x^T}(\zeta);	
		\end{align*}
		where $R_A(B)=BA$, i.e, right-multiplication by $A$.

	\end{enumerate}
	
	\item Suppose $\psi:\R^n\times\R^n\to\R^n$ is given by
	$$\psi(b,x)=x+b.$$
	Then we again have two (identical) differentials to consider:
	\begin{enumerate}
		\item For any $(x,b)\in\R^n\times\R^n$ and any $\xi\in T_x\R^n$, we have that
		$$d\psi_{(b,x)}(\xi)=\xi;$$
		and for any $\zeta\in T_{\psi(b,x)}\R^n$, we have the reverse derivative
		$$r\psi_{(b,x)}(\zeta)=\zeta.$$
		
		\item For any $(x,b)\in\R^n\times\R^n$ and any $\eta\in T_b\R^n$, we have that
		$$d_1\psi_{(b,x)}(\eta)=\eta;$$
		and for any $\zeta\in T_{(\psi(b,x)}\R^n$, we have the reverse derivative
		$$\cl{r}_1\psi_{(b,x)}(\zeta)=\zeta.$$
	\end{enumerate}
\end{enumerate}





Returning to our neural network, for each point $(x_j,y_j)$ in our training set, we first let
$$F_j:=\L\circ\lay{G}{2}\circ\lay{\psi}{2}\circ\lay{\phi}{2}\circ\lay{G}{1}\circ\lay{\psi}{1}\circ\lay{\phi}{1},$$
and we have our cost function
$$\J:=\frac{1}{N}\sum_{j=1}^NF_j.$$

We use the following notation for our inputs and outputs of our respective functions:
\begin{itemize}
	\item $$\lay{\phi}{\ell}:(\lay{w}{\ell},\lay{a}{\ell-1}{_j})\mapsto \lay{u}{\ell}{_j},$$
	\item $$\lay{\psi}{\ell}:(\lay{b}{\ell},\lay{u}{\ell}{_j})\mapsto\lay{z}{\ell}{_j},$$
	\item $$\lay{G}{\ell}:\lay{z}{\ell}{_j}\mapsto\lay{a}{\ell}{_j}.$$
\end{itemize}

Let $p=(\lay{w}{1},\lay{b}{1},\lay{w}{2},\lay{b}{2})$ is a point in our parameter space.  Suppose we wish to apply gradient descent with learning rate $\alpha\in T_{\J(p)}\R$, we would define our parameter updates via
\begin{align*}
	\lay{w}{1}&:=\lay{w}{1}-r_1\J_p(\alpha)\\
	\lay{b}{1}&:=\lay{b}{1}-\cl{r}_1\J_p(\alpha)\\
	\lay{w}{2}&:=\lay{w}{2}-r_2\J_p(\alpha)\\
	\lay{b}{2}&:=\lay{b}{2}-\cl{r}_2\J_p(\alpha).
\end{align*}
Moreover, by linearity (and independence of our training data), we see that
$$r\J_p=\frac{1}{N}\sum_{j=1}^Nr(F_j)_p,$$
so we need only calculate the various reverse derivatives of $F_j$.

To this end, we suppress the index $j$ when we're working with the compositional function $F$.  We calculate the reverse derivatives in the order traversed in our back-propagating path along the network.

\begin{enumerate}
	\item $\cl{r}_2\J_p$:
	\begin{align*}
		\cl{r}_2F_p&=\cl{r}_2(\L\circ\lay{G}{2}\circ\lay{\psi}{2})_p\\
		&=\cl{r}_2\lay{\psi}{2}_p\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=\id\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}},
	\end{align*}
	and hence
	\begin{align*}
		\cl{r}_2\J_p&=\frac{1}{N}\sum_{j=1}^Nr\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}
	\end{align*}
	
	\item $r_2\J_p$:
	\begin{align*}
		r_2F_p&=r_2(\L\circ\lay{G}{2}\circ\lay{\psi}{2}\circ\lay{\phi}{2})_p\\
		&=r_2\lay{\phi}{2}_p\circ r\lay{\psi}{2}_{\lay{u}{2}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=R_{\lay{a}{1}{^T}}\circ\id\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=R_{\lay{a}{1}{^T}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}},
	\end{align*}
	and hence
	$$r_2\J_p=\frac{1}{N}\sum_{j=1}^NR_{\lay{a}{1}{^T}{_j}}\circ r\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}.$$
	Notice that this is not just a sum after matrix multiplication since we have composition with an operator, namely, $R_{\lay{a}{1}{^T}{_j}}$.  However, since the learning rate $\alpha\in T_{\J(p)}\R\cong\R$, which may pass through the aforementioned linear composition, we conclude that
	\begin{align*}
		r_2\J_p&=\frac{1}{N}\sum_{j=1}^NR_{\lay{a}{1}{^T}{_j}}\circ r\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}\\
		&=\frac{1}{N}\sum_{j=1}^Nr\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}\lay{a}{1}{^T}{_j}.
	\end{align*}
	
	\item $\cl{r}_1\J_p$:
	\begin{align*}
		\cl{r}_1F_p&=\cl{r}_1(\L\circ\lay{G}{2}\circ\lay{\psi}{2}\circ\lay{\phi}{2}\circ\lay{G}{1}\circ\lay{\psi}{1})_p\\
		&=\cl{r}_1\lay{\psi}{1}_p\circ r\lay{G}{1}_{\lay{z}{1}}\circ r\lay{\phi}{2}_{\lay{a}{1}}\circ r\lay{\psi}{2}_{\lay{u}{2}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=\id\circ r\lay{G}{1}_{\lay{z}{1}}\circ L_{\lay{w}{2}{^T}}\circ\id\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=r\lay{G}{1}_{\lay{z}{1}}\circ L_{\lay{w}{2}{^T}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}},
	\end{align*}
	and hence
	\begin{align*}
		\cl{r}_1\J_p&=\frac{1}{N}\sum_{j=1}^Nr\lay{G}{1}_{\lay{z}{1}{_j}}\cdot\lay{w}{2}{^T}\cdot r\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}.
	\end{align*}
	
	\item $r_1\J_p$:
	\begin{align*}
		r_1F_p&=r_1(\L\circ\lay{G}{2}\circ\lay{\psi}{2}\circ\lay{\phi}{2}\circ\lay{G}{1}\circ\lay{\psi}{1}\circ\lay{\phi}{1})_p\\
		&=r_1\lay{\phi}{1}_p\circ r\lay{\psi}{1}_{\lay{u}{1}}\circ r\lay{G}{1}_{\lay{z}{1}}\circ r\lay{\phi}{2}_{\lay{a}{1}}\circ r\lay{\psi}{2}_{\lay{u}{2}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=R_{x^T}\circ\id\circ r\lay{G}{1}_{\lay{z}{1}}\circ L_{\lay{w}{2}{^T}}\circ\id\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}\\
		&=R_{x^T}\circ r\lay{G}{1}_{\lay{z}{1}}\circ L_{\lay{w}{2}{^T}}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}},
	\end{align*}
	and hence
	\begin{align*}
		r_1\J_p&=\frac{1}{N}\sum_{j=1}^NR_{x_j^T}\circ r\lay{G}{1}_{\lay{z}{1}{_j}}\cdot\lay{w}{2}{^T}\cdot r\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}\\
		&=\frac{1}{N}\sum_{j=1}^Nr\lay{G}{1}_{\lay{z}{1}{_j}}\cdot\lay{w}{2}{^T}\cdot r\lay{G}{2}_{\lay{z}{2}{_j}}\cdot r\L_{\lay{a}{2}{_j}}\cdot x_j^T
	\end{align*}
\end{enumerate}










\begin{comment}


\subsection{Backward Propagation}\label{sec:basicBackPropDerivation}

Since we wish to optimize our model with respect to our parameter $\lay{w}{\ell}$ and $\lay{b}{\ell}$,  we consider a generic loss function $\L:\R\times\R\to\R$, $\L(\lay{a}{2},y)$, and by acknowledging the potential abuse of notation, we assume $y$ is fixed, and consider the aforementioned as a function of a single-variable
$$\L_y:\R^{m_2}\to\R, \qquad \L_y(\hat{y})=\L(\hat{y},y).$$
We also define the function
$$\Phi(A,u,\xi)=A\xi+u,$$
and note that we're suppressing a dependence on the layer $\ell$ which only affects our domain and range of $\Phi$ (and not the actual calculations involving the derivatives).  Moreover, in coordinates we see that
\begin{align*}
	\frac{\partial\Phi^i}{\partial A^\mu_\nu}&=\frac{\partial}{\partial A^\mu_\nu}(A^i_j\xi^j+u^i)\\
	&=(\delta^i_\mu\delta_j^\nu \xi^j)\\
	&=\delta^i_\mu \xi^\nu;
\end{align*}
\begin{align*}
	\frac{\partial\Phi^i}{\partial u^\mu}&=\frac{\partial}{\partial u^\mu}(A^i_j\xi^j+u^i)\\
	&=\delta^i_\mu;
\end{align*}
and
\begin{align*}
	\frac{\partial\Phi^i}{\xi^\mu}&=\frac{\partial}{\partial \xi^\mu}(A^i_j\xi^j+u^i)\\
	&=A^i_j\delta^j_\mu\\
	&=A^i_\mu.
\end{align*}





We now define the compositional function
$$F:\R^{m_2\times m_1}\times \R^{m_2}\times \R^{m_1\times m_0}\times \R^{m_1}\times \R^{m_0}\to\R$$
given by
$$F(C,c,B,b,x)=\L_y\circ\lay{g}{2}\circ\Phi\circ(\id_{\R^{m_2\times m_1}}\times\id_{\R^{m_2}}\times (\lay{g}{1}\circ\Phi))(C,c,B,b,x).$$
We first introduce an error term $\lay{\delta}{2}\in\R^{m_2}$ defined by
\begin{align*}
	\lay{\delta}{2}:&=\nabla (\L_y\circ\lay{g}{2})(\lay{z}{2})\\
	&=(d\L_y\circ\lay{g}{2})_{\lay{z}{2}})^T.
\end{align*}\HOX{$\lay{\delta}{2}=d_{\lay{z}{2}}F$}
Now we calculate the gradient $\frac{\partial F}{\partial C}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial C^\mu_\nu}&=\frac{\partial}{\partial C^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{a}{1})\right]\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\frac{\partial}{\partial C^\mu_\nu}(C^j_i\lay{a}{1}{^i}+c^j)\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\delta^j_\mu \lay{a}{1}{^\nu}\\
	&=\lay{\delta}{2}{_\mu}\lay{a}{1}{^\nu}\\
	&=[\lay{a}{1}\lay{\delta}{2}{^T}]_\mu^\nu
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial C}&=\left[
		\frac{\partial F}{\partial C^\mu_\nu}
	\right]^T\\
	&=\left[\lay{\delta}{2}_\mu \lay{a}{1}{^\nu}\right]^T\\
	&=\lay{\delta}{2}\lay{a}{1}{^T}.
\end{align*}
Moreover, we also calculate
\begin{align*}
	\frac{\partial F}{\partial c^\mu}&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\delta^j_\mu,
\end{align*}
and hence that
$$\frac{\partial F}{\partial c}=\lay{\delta}{2}.$$

Next we introduce another error term $\lay{\delta}{1}\in\R^{m_1}$ defined by
\begin{align*}
	\lay{\delta}{1}=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}
\end{align*}
\HOX{$\lay{\delta}{1}=d_{\lay{z}{1}}F$}
with coordinates
\begin{align*}
	(\lay{\delta}{1}{^\mu})^T&=\sum_{i=1}^{m_2}\sum_{j=1}^{m_1}\lay{\delta}{2}{^i}C^i_j\lay{g}{1}{'}(\lay{z}{1}{^j})\delta^j_\mu\\
	&=\sum_{i=1}^{m_2}\lay{\delta}{2}{^i}C^i_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})
\end{align*}

and now calculate the gradient $\frac{\partial F}{\partial B}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial B^\mu_\nu}&=\frac{\partial}{B^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{g}{1}(Bx+b))\right]\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{m_1}\frac{\partial \lay{a}{1}{^\rho}}{\partial \lay{z}{1}{^\lambda}}\frac{\partial \Phi^\lambda}{\partial B^\mu_\nu}\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{m_1}
	\delta^\rho_\lambda \lay{g}{1}{'}(\lay{z}{1}{^\rho})
	\delta^{\lambda}_\mu x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}C^j_\rho\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}C^j_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})x^\nu\\
	&=\lay{\delta}{1}{_\mu}x^\nu\\
	&=\left[x\lay{\delta}{1}{^T}\right]^\nu_\mu,
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial B}&=\left[\frac{\partial F}{\partial B^\mu_\nu}\right]^T\\
	&=\lay{\delta}{2}x^T.
\end{align*}
Moreover, from the above calculation, we immediately see that
\begin{align*}
	\frac{\partial F}{\partial b^\mu}&=\lay{\delta}{1}.
\end{align*}

In summary, we've computed the following gradients
\begin{align*}
	\frac{\partial F}{\partial\lay{W}{2}}&=\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial F}{\partial\lay{b}{2}}&=\lay{\delta}{2}\\
	\frac{\partial F}{\partial\lay{W}{1}}&=\lay{\delta}{1}x^T\\
	\frac{\partial F}{\partial\lay{b}{1}}&=\lay{\delta}{1},
\end{align*}
where
\begin{align*}
	\lay{\delta}{2}&=[d(\L_y\circ\lay{g}{2})_{\lay{z}{2}}]^T\\
	\lay{\delta}{1}&=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}.
\end{align*}

Finally, we recall that our cost function $\J$ is the average sum of our loss function $\L$ over our training set, we get that
\begin{align*}
	\J(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1})&=\frac{1}{n}\sum_{j=1}^nF(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x_j),
\end{align*}
and hence that
\begin{align*}
	\frac{\partial\J}{\partial\lay{W}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\lay{a}{1}{_j}^T=\frac{1}{n}\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial\J}{\partial\lay{b}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\\
	\frac{\partial\J}{\partial\lay{W}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}x_j^T=\frac{1}{n}\lay{\delta}{1}x^T\\
	\frac{\partial\J}{\partial\lay{b}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}
\end{align*}





\subsection{Binary Classification - An Example}
We return the network given by
\begin{align*}
	\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{m_0}
	\end{bmatrix}}_{\text{Layer } 0}&\layerfctn{\lay{\varphi}{1}}\underbrace{\begin{bmatrix}
		\lay{z}{1}{^1}\\
		\vdots\\
		\lay{z}{1}{^{m_1}}
	\end{bmatrix}\layerfctn{\lay{g}{1}}
	\begin{bmatrix}
		\lay{a}{1}{^1}\\
		\vdots\\
		\lay{a}{1}{^{m_1}}
	\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\varphi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}
	\end{bmatrix}\layerfctn{\lay{g}{2}}
	\begin{bmatrix}
		\lay{a}{2}
	\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\hat{y},
\end{align*}
and show how such a model would be trained using python below.  We assume layer-$2$ has the sigmoid function (since it's binary classification) as an activator and our hidden layer has the $\relu$ function as activators.

We note that $m_2=1$ since we're dealing with a single activator in this layer, and
$$\lay{a}{2}=\lay{g}{2}(\lay{z}{2})=\sigma(\lay{z}{2}),$$
with
$$d(\lay{g}{2})_{\lay{z}{2}}=\sigma'(\lay{z}{2})=\sigma(\lay{z}{2})(1-\sigma(\lay{z}{2}))=\lay{a}{2}(1-\lay{a}{2}).$$
In layer-$1$, we have that
$$\lay{a}{1}=\lay{g}{1}(\lay{z}{1})=\relu(\lay{z}{1}),$$
with
$$d(\lay{g}{1})_{\lay{z}{1}}=\left[\delta^\mu_\nu\chi_{[0,\infty)}(\lay{z}{1}{^\mu})\right]^\mu_\nu.$$
Finally, we choose our loss function $\L(\hat{y},y)$ to be the log-loss function (since we're using the sigmoid activator on the outer-layer), i.e.,
$$\L(\hat{y},y)=-y\log(\hat{y})-(1-y)\log(1-\hat{y}),$$
or rather
$$\L(x,y)=-y\log(\lay{a}{2})-(1-y)\log(1-\lay{a}{2}).$$
We then have the cost function $\J$ given by
\begin{align*}
	\J(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1})&=\frac{-1}{n}\sum_{j=1}^n\left(y_j\log(\lay{a}{2}{_j})+(1-y_j)\log(1-\lay{a}{2}{_j})\right)\\
	&=\frac{-1}{n}\left(\ip{y,\log(\lay{a}{2})}+\ip{1-y,\log(1-\lay{a}{2})}\right)
\end{align*}

Moreover, when using backpropagation, we see that
\begin{align*}
	\lay{\delta}{2}{^T_j}&=d(\L_{y_j})_{\lay{a}{2}}\cdot d(\lay{g}{2})_{\lay{z}{2}{_j}}\\
	&=\left(-\frac{y_j}{\lay{a}{2}{_j}}+\frac{1-y_j}{1-\lay{a}{2}{_j}}\right)\cdot(\lay{a}{2}{_j}(1-\lay{a}{2}{_j})\\
	&=\lay{a}{2}{_j}-y_j,
\end{align*}
or rather
$$\lay{\delta}{2}=\lay{a}{2}-y.$$
Similarly, we compute
\begin{align*}
	\lay{\delta}{1}{_j^T}&=\lay{\delta}{2}{_j^T}\lay{W}{2}[d\lay{g}{1}_{\lay{z}{1}{_j}}]\\
	&=\lay{\delta}{2}{_j^T}\lay{W}{2}[\delta^\mu_\nu\cdot\chi_{[0,\infty)}(\lay{z}{1}{^\mu_j})]
\end{align*}


\subsubsection{Random Initialization}
In the section that follows, we see that to begin gradient descent for a shallow neural network, we initialize our parameters $\lay{b}{\ell}$ to be $0$, but choose an arbitrarily small, but nonzero initialization for $\lay{W}{\ell}$.  Let's see why we choose $\lay{W}{\ell}$ to be nonzero.  Indeed, suppose we initialize with $\lay{b}{\ell}=0$ and $\lay{W}{\ell}=0$.  Then we see that
$$\lay{\delta}{1}{^T}=\lay{\delta}{2}\lay{W}{2}d\lay{g}{1}_{\lay{z}{1}}=0,$$
and so
$$\frac{\partial\J}{\partial\lay{W}{1}}=\frac{1}{n}\lay{\delta}{1}x^T=0.$$
Then we conclude that our parameter $\lay{W}{1}$ remains at $0$ during every iteration which is enough reason to not initialize $\lay{W}{2}$ at $0$.  Similarly, since
$$\lay{a}{1}=\tanh(\lay{W}{1}x+\lay{b}{1})=\tanh(0)=0,$$
we reach a similar conclusion about $\lay{W}{1}$ and $\lay{W}{2}$, respectively.
%\end{comment}