%$$\lay{z}{1}{^i_j}$$
%$$\tensor{z}{^{[1]}^i_j}$$
%$$\tensor{\lay{z}{1}}{^i_j}$$


\section{Neural Networks: A Single Hidden Layer}
Suppose we wish to consider the binary classification problem given the training set $(x,y)$ with $x\in\R^{m_0\times n}$ and $y\in\{0,1\}^{1\times n}$.  Usually with logistic regression we have the following type of structure:
$$[x^1,...,x^{m_0}]\layerfctn{\varphi}[z]\layerfctn{g}[a]\layerfctn{=}\hat{y},$$
where
$$z=\varphi(x)=w^Tx+b,$$
is our affine-linear transformation, and
$$a=g(z)=\sigma(z)$$
is our sigmoid function.  Such a structure will be called a \textit{network}, and the $[a]$ is known as the \textit{activation node}.  Logistic regression can be too simplistic of a model for many situations, e.g., if the dataset isn't linearly separable (i.e., there doesn't exist some well-defined decision boundary built from a linear-surface), then logistic regression won't give a high-accuracy model.  To modify this model to handle more complex situations, we introduce a new ``hidden layer'' of nodes with their own (possibly different) activation functions.  That is, we consider a network of the following form:
\begin{align*}
	\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{m_0}
	\end{bmatrix}}_{\text{Layer } 0}&\layerfctn{\lay{\varphi}{1}}\underbrace{\begin{bmatrix}
		\lay{z}{1}{^1}\\
		\vdots\\
		\lay{z}{1}{^{m_1}}
	\end{bmatrix}\layerfctn{\lay{g}{1}}
	\begin{bmatrix}
		\lay{a}{1}{^1}\\
		\vdots\\
		\lay{a}{1}{^{m_1}}
	\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\varphi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}
	\end{bmatrix}\layerfctn{\lay{g}{2}}
	\begin{bmatrix}
		\lay{a}{2}
	\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\hat{y},
\end{align*}
where
$$\lay{\varphi}{1}:\R^{m_0}\to\R^{m_1},\qquad \lay{\varphi}{1}(x)=\lay{W}{1}x+\lay{b}{1},$$
$$\lay{\varphi}{2}:\R^{m_1}\to\R,\qquad \lay{\varphi}{2}(x)=\lay{W}{2}x+\lay{b}{2},$$
and $\lay{W}{1}\in\R^{m_1\times m_0}, \lay{W}{2}\in\R^{1\times m_1}, \lay{b}{1}\in\R^{m_1}, \lay{b}{2}\in\R$, and $\lay{g}{\ell}$ is a \textit{broadcasted} activator function (e.g., the sigmoid function $\sigma(z)$, or $\tanh(z)$, or $\relu(z)$).  Such a network is called a $2$-layer neural network where $x$ is the input layer (called layer-$0$), $\lay{a}{1}$ is a hidden layer (called layer-$1$), and $\lay{a}{2}$ is the output layer (called layer-$2$).

\begin{defn}
	Suppose $g:\R\to\R$ is any function.  Then we say $G:\R^{m}\to\R^{m}$ is the \textbf{broadcast} of $g$ from $\R$ to $\R^m$ if
	\begin{align*}
		G(v)&=G(v^ie_i)\\
		&=g(v^i)e_i,
	\end{align*}
	where $v\in\R^{m}$ and $\{e_i:1\leq i\leq m\}$ is the standard basis for $\R^{m}$.  In practice, we will write $g=G$ for a broadcasted function, and let the context determine the meaning of $g$.
\end{defn}

\begin{lem}\label{lem: broadcastingDifferential}
	Suppose $g:\R\to\R$ is any smooth function and $G:\R^m\to\R^m$ is the broadcasting of $g$ from $\R$ to $\R^m$.  Then the differential $dG_z:T_z\R^m\to T_{G(z)}\R^m$ is given by
	$$dG_z(v)=[g'(z^i)]\odot [v^i],$$
	where $\odot$ is the Hadamard product (also know as component-wise multiplication), and has matrix-representation in $\R^{m\times m}$ given by
	$$[dG_z]^i_j =\delta^i_j g'(z^i).$$
\end{lem}

\begin{proof}
	We calculate
	\begin{align*}
		dG_z(v)&=\rest{\frac{d}{dt}}_{t=0}G(z+tv)\\
		&=\rest{\frac{d}{dt}}_{t=0}(g(z^i+tv^i))\\
		&=(g'(z^i)v^i)\\
		&=[g'(z^i)]\odot[v^i],
	\end{align*}
	and letting $e_1,...e_m$ denote the usual basis for $T_z\R^m$ (identified with $\R^m$), we see that
	\begin{align*}
		dG_z(e_j)&=[g'(z^i)]\odot e_j\\
		&=g'(z^j)e_j,
	\end{align*}
	from which conclude that $dG_z$ is diagonal with $(j,j)$-th entry $g'(z^j)$ as desired.
\end{proof}

Returning to our network, let us lay out all of these functions explicitly (in the Smooth Category) as to facilitate our later computations for our cost function and our gradients.
To this end:
\begin{align*}
	&\lay{\varphi}{1}:\R^{m_0}\to\R^{m_1},\qquad &d\lay{\varphi}{1}:T\R^{m_0}\to T\R^{m_1},\\
	&\lay{z}{1}=\lay{\varphi}{1}(x)=\lay{W}{1}x+\lay{b}{1},&d\lay{\varphi}{1}_x(v)=\lay{W}{1}v;
\end{align*}

\begin{align*}
	&\lay{g}{1}:\R^{m_1}\to\R^{m_1},\qquad &
	d\lay{g}{1}:T\R^{m_1}\to T\R^{m_1},\\
	&\lay{a}{1}=\lay{g}{1}(\lay{z}{1}),\qquad
	&\frac{\partial\lay{a}{1}{^\mu}}{\partial \lay{z}{1}{^\nu}}=\delta^\mu_\nu\lay{g}{1}{'}(\lay{z}{1}{^\mu});
\end{align*}

\begin{align*}
&\lay{\varphi}{2}:\R^{m_1}\to\R^{m_2},\qquad
&d\lay{\varphi}{2}:T\R^{m_1}\to T\R^{m_2},\\
&\lay{z}{2}=\lay{\varphi}{2}(\lay{a}{1})=\lay{W}{2}\lay{a}{1}+\lay{b}{2},\qquad &d\lay{\varphi}{2}_{\lay{a}{2}}(v)=\lay{W}{2}v;
\end{align*}

\begin{align*}
	&\lay{g}{2}:\R^{m_2}\to\R^{m_2},\qquad
	&d\lay{g}{2}:T\R^{m_2}\to T\R^{m_2},\\
	&\lay{a}{2}=\lay{g}{2}(\lay{z}{2}),\qquad
	&\frac{\partial\lay{a}{2}{^\mu}}{\partial \lay{z}{2}{^\nu}}=\delta^\mu_\nu\lay{g}{2}{'}(\lay{z}{2}{^\mu}).
\end{align*}
That is, given an input $x\in\R^{m_0}$, we get a predicted value $\hat{y}\in\R^{m_2}$ of the form
$$\hat{y}=\lay{g}{2}\circ\lay{\varphi}{2}\circ\lay{g}{1}\circ\lay{\varphi}{1}(x).$$
This compositional function is known as \textit{forward propagation}.

\begin{comment}
Since we wish to optimize our model with respect to our parameter $\lay{W}{\ell}$ and $\lay{b}{\ell}$,  we consider a generic loss function $\L:\R\times\R\to\R$, $\L(\hat{y},y)$, and by acknowledging the potential abuse of notation, we assume $y$ is fixed, and consider the aforementioned as a function of a single-variable
$$\L_y:\R\to\R, \qquad \L_y(\hat{y})=\L(\hat{y},y).$$
We now define the compositional function
$$F:\R^{m_0}\to\R,\qquad F(x)=\L_y\circ\lay{g}{2}\circ\lay{\varphi}{2}\circ\lay{g}{1}\circ\lay{\varphi}{1}(x).$$


As we mentioned before, we wish to optimize with respect to our parameters, but our above composition doesn't make this dependence explicit for computations.  To this end, we first previously considered the generic affine-linear transformation
$$\varphi:\R^m\to\R^k,\qquad\varphi(x)=Wx+b,$$
with $W\in\R^{k\times m}, b\in\R^k$.  We now change our point-of-view and consider the related
$$\phi:\R^{k\times m}\times\R^k\to\R^k,\qquad \phi(W, b)=Wx+b,$$
for some fixed $x\in\R^m$.  Then we see that
$$d\phi:T\R^{k\times m}\times T\R^k\to T\R^k,$$
\begin{align*}
	d\phi_{(W,b)}(V,v)&=\rest{\frac{d}{dt}}_{t=0}\phi(W+tV, b+tv)\\
	&=\rest{\frac{d}{dt}}_{t=0}(W+tV)x+(b+tv)\\
	&=Vx+v.
\end{align*}
This leads to the further decomposition of the form
$$\phi(W,b)=\psi(W)+\id_{\R^k}(b),$$
where $\id_{\R^k}:\R^k\to\R^k$ is the identity function, and $\psi:\R^{k\times m}\to\R^k$ is given by
$$\psi(W)=Wx.$$
Then by the above computation, we have that
$$d\psi_W(V)=Vx.$$
Moreover, suppose
$$\{E_1^1,E_1^2,...,E_1^m,E_2^1,E_2^2,...,E_2^m,...,E_k^1,E_k^2,...,E_k^m\}$$
is an ordering of the standard basis $\{E_\alpha^\beta\}$ for $\R^{k\times m}$ with
$$[E_\alpha^\beta]^i_j=\delta^i_\alpha\delta_j^\beta,$$ and
$$V=V^i_jE_i^j,$$
then $d\psi_W\in\R^{k\times mk}$ with matrix-representation
$$d\psi_W=\begin{bmatrix}
	x^T&0&0&\cdots &0\\
	0&x^T&0&\cdots &0\\
	0&0&x^T&\cdots &0\\
	\vdots &\vdots &\ddots &\ddots &\vdots\\
	0&0&0&\cdots &x^T
\end{bmatrix},$$
where each $0$ represents $0\in\R^{1\times m}$, and
\begin{align*}
	d\psi_W(V)&=\begin{bmatrix}
	x^T&0&0&\cdots &0\\
	0&x^T&0&\cdots &0\\
	0&0&x^T&\cdots &0\\
	\vdots &\vdots &\ddots &\ddots &\vdots\\
	0&0&0&\cdots &x^T
\end{bmatrix}\begin{bmatrix}
	V_1^1\\
	V_2^1\\
	\vdots\\
	V_m^1\\
	\vdots\\
	V_1^k\\
	V_2^k\\
	\vdots\\
	V_m^k
\end{bmatrix}\\
&=\begin{bmatrix}
	x^T(V^1)^T\\
	x^T(V^2)^T\\
	\vdots\\
	x^T(V^k)^T
\end{bmatrix}.
\end{align*}
Finally, since for any smooth $f:\R^m\to\R^n$, we have that






Taking this further, we now consider the map
$$\Phi:\R^{k\times m}\times\R^k\times\R^m\to\R^k,\qquad \Phi(W, b, x)=Wx+b,$$
and then computing our differential for
$$d\Phi:T\R^{k\times m}\times T\R^k\times T\R^m\to T\R^k,$$
yields
\begin{align*}
	d\Phi_{(W,b,x)}(V,v,p)&=\rest{\frac{d}{dt}}_{t=0}\Phi(W+tV, b+tv, x+tp)\\
	&=\rest{\frac{d}{dt}}_{t=0}\left((W+tV)(x+tp)+(b+tv)\right)\\
	&=\rest{\frac{d}{dt}}_{t=0}\left(Wx+tVx+tWv+t^2Vp+b+tv\right)\\
	&=Vx+v+Wp\\
	&=d\phi_{(W,b)}(V,v)+d\varphi_x(p)
\end{align*}
This function $\Phi$ is what we want in our compositional-function, and so we redefine $F$ as
\small
\begin{align*}
	F(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}\circ(\lay{W}{2},\lay{b}{2},\lay{g}{1}\circ\lay{\Phi}{1}\circ(\lay{W}{1},\lay{b}{1},x))
\end{align*}
\normalsize
Taking the exterior derivative, and noting the composition turns into matrix multiplication on the tangent space, we get
\small
\begin{align*}
	d&F_{(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x)}(U,u,V,v,p)\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\Phi}{2}_{(\lay{W}{2},\lay{b}{2},\lay{a}{1})}\cdot (U, u, d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\Phi}{1}_{(\lay{W}{1},\lay{b}{1},x)}(V,v,p))\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot(d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2})}(U,u)+d\lay{\varphi}{2}_{\lay{a}{1}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot(d\lay{\phi}{1}_{(\lay{W}{1},\lay{b}{1})}(V,v)+d\lay{\varphi}{1}_x(p)))\\
	&=d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2},\{\lay{a}{1}\})}(U,u)\\
	&\qquad +d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\varphi}{2}_{\lay{a}{1},\{\lay{W}{2},\lay{b}{2}\}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\phi}{1}_{(\lay{W}{1},\lay{b}{1}),\{x\}}(V,v)\\
	&\qquad+d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\varphi}{2}_{\lay{a}{1},\{\lay{W}{2},\lay{b}{2}\}}\cdot d\lay{g}{1}_{\lay{z}{1}}\cdot d\lay{\varphi}{1}_{x,\{\lay{W}{1},\lay{b}{1}\}}(p)\\
	&=:\lay{dF}{2}+\lay{dF}{1}+\lay{dF}{0},
\end{align*}
\normalsize
where $\lay{dF}{2}$ represents the differential with respect to the parameters going from layer-$1$ to layer-$2$, $\lay{dF}{1}$ represents the differential with respect to the parameters going from layer-$0$ to layer-$1$, and $\lay{dF}{0}$ represents the differential with respect to $x$.

Recalling that the gradient is the transpose of the exterior derivative in Euclidean space, we then conclude that
\begin{align*}
	\nabla F&=(dF)^T\\
	&=\left(\lay{dF}{2}+\lay{dF}{1}+\lay{dF}{0}\right)^T\\
	&=\nabla\lay{F}{2}+\nabla\lay{F}{1}+\nabla\lay{F}{0},
\end{align*}
and respectively,
\begin{align*}
	\nabla\lay{F}{2}&=\left(d(\L_y)_{\lay{a}{2}}\cdot d\lay{g}{2}_{\lay{z}{2}}\cdot d\lay{\phi}{2}_{(\lay{W}{2},\lay{b}{2},\{\lay{a}{1}\})}\right)^T
\end{align*}


\end{comment}

\subsection{Backward Propagation}\label{sec:backPropDerivation}

Since we wish to optimize our model with respect to our parameter $\lay{W}{\ell}$ and $\lay{b}{\ell}$,  we consider a generic loss function $\L:\R^{m_2}\times\R^{m_2}\to\R$, $\L(\hat{y},y)$, and by acknowledging the potential abuse of notation, we assume $y$ is fixed, and consider the aforementioned as a function of a single-variable
$$\L_y:\R^{m_2}\to\R, \qquad \L_y(\hat{y})=\L(\hat{y},y).$$
We also define the function
$$\Phi(A,u,\xi)=A\xi+u,$$
and note that we're suppressing a dependence on the layer $\ell$ which only affects our domain and range of $\Phi$ (and not the actual calculations involving the derivatives).  Moreover, in coordinates we see that
\begin{align*}
	\frac{\partial\Phi^i}{\partial A^\mu_\nu}&=\frac{\partial}{\partial A^\mu_\nu}(A^i_j\xi^j+u^i)\\
	&=(\delta^i_\mu\delta_j^\nu \xi^j)\\
	&=\delta^i_\mu \xi^\nu;
\end{align*}
\begin{align*}
	\frac{\partial\Phi^i}{\partial u^\mu}&=\frac{\partial}{\partial u^\mu}(A^i_j\xi^j+u^i)\\
	&=\delta^i_\mu;
\end{align*}
and
\begin{align*}
	\frac{\partial\Phi^i}{\xi^\mu}&=\frac{\partial}{\partial \xi^\mu}(A^i_j\xi^j+u^i)\\
	&=A^i_j\delta^j_\mu\\
	&=A^i_\mu.
\end{align*}





We now define the compositional function
$$F:\R^{m_2\times m_1}\times \R^{m_2}\times \R^{m_1\times m_0}\times \R^{m_1}\times \R^{m_0}\to\R$$
given by
$$F(C,c,B,b,x)=\L_y\circ\lay{g}{2}\circ\Phi\circ(\id_{\R^{m_2\times m_1}}\times\id_{\R^{m_2}}\times (\lay{g}{1}\circ\Phi))(C,c,B,b,x).$$
We first introduce an error term $\lay{\delta}{2}\in\R^{m_2}$ defined by
\begin{align*}
	\lay{\delta}{2}:&=\nabla (\L_y\circ\lay{g}{2})(\lay{z}{2})\\
	&=(d\L_y\circ\lay{g}{2})_{\lay{z}{2}})^T.
\end{align*}\HOX{$\lay{\delta}{2}=d_{\lay{z}{2}}F$}
Now we calculate the gradient $\frac{\partial F}{\partial C}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial C^\mu_\nu}&=\frac{\partial}{\partial C^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{a}{1})\right]\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\frac{\partial}{\partial C^\mu_\nu}(C^j_i\lay{a}{1}{^i}+c^j)\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\delta^j_\mu \lay{a}{1}{^\nu}\\
	&=\lay{\delta}{2}{_\mu}\lay{a}{1}{^\nu}\\
	&=[\lay{a}{1}\lay{\delta}{2}{^T}]_\mu^\nu
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial C}&=\left[
		\frac{\partial F}{\partial C^\mu_\nu}
	\right]^T\\
	&=\left[\lay{\delta}{2}_\mu \lay{a}{1}{^\nu}\right]^T\\
	&=\lay{\delta}{2}\lay{a}{1}{^T}.
\end{align*}
Moreover, we also calculate
\begin{align*}
	\frac{\partial F}{\partial c^\mu}&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\delta^j_\mu,
\end{align*}
and hence that
$$\frac{\partial F}{\partial c}=\lay{\delta}{2}.$$

Next we introduce another error term $\lay{\delta}{1}\in\R^{m_1}$ defined by
\begin{align*}
	\lay{\delta}{1}=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}
\end{align*}
\HOX{$\lay{\delta}{1}=d_{\lay{z}{1}}F$}
with coordinates
\begin{align*}
	(\lay{\delta}{1}{^\mu})^T&=\sum_{i=1}^{m_2}\sum_{j=1}^{m_1}\lay{\delta}{2}{^i}C^i_j\lay{g}{1}{'}(\lay{z}{1}{^j})\delta^j_\mu\\
	&=\sum_{i=1}^{m_2}\lay{\delta}{2}{^i}C^i_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})
\end{align*}

and now calculate the gradient $\frac{\partial F}{\partial B}$ in coordinates by
\begin{align*}
	\frac{\partial F}{\partial B^\mu_\nu}&=\frac{\partial}{B^\mu_\nu}\left[\L_y\circ\lay{g}{2}\circ\Phi(C,c,\lay{g}{1}(Bx+b))\right]\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{m_1}\frac{\partial \lay{a}{1}{^\rho}}{\partial \lay{z}{1}{^\lambda}}\frac{\partial \Phi^\lambda}{\partial B^\mu_\nu}\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\sum_{\lambda=1}^{m_1}
	\delta^\rho_\lambda \lay{g}{1}{'}(\lay{z}{1}{^\rho})
	\delta^{\lambda}_\mu x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}\frac{\partial\Phi^j}{\partial \xi^\rho}\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}\sum_{\rho=1}^{m_1}C^j_\rho\delta^\rho_\mu\lay{g}{1}{'}(\lay{z}{1}{^\rho})x^\nu\\
	&=\sum_{j=1}^{m_2}\lay{\delta}{2}{^j}C^j_\mu\lay{g}{1}{'}(\lay{z}{1}{^\mu})x^\nu\\
	&=\lay{\delta}{1}{_\mu}x^\nu\\
	&=\left[x\lay{\delta}{1}{^T}\right]^\nu_\mu,
\end{align*}
and hence that
\begin{align*}
	\frac{\partial F}{\partial B}&=\left[\frac{\partial F}{\partial B^\mu_\nu}\right]^T\\
	&=\lay{\delta}{2}x^T.
\end{align*}
Moreover, from the above calculation, we immediately see that
\begin{align*}
	\frac{\partial F}{\partial b^\mu}&=\lay{\delta}{1}.
\end{align*}

In summary, we've computed the following gradients
\begin{align*}
	\frac{\partial F}{\partial\lay{W}{2}}&=\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial F}{\partial\lay{b}{2}}&=\lay{\delta}{2}\\
	\frac{\partial F}{\partial\lay{W}{1}}&=\lay{\delta}{1}x^T\\
	\frac{\partial F}{\partial\lay{b}{1}}&=\lay{\delta}{1},
\end{align*}
where
\begin{align*}
	\lay{\delta}{2}&=[d(\L_y\circ\lay{g}{2})_{\lay{z}{2}}]^T\\
	\lay{\delta}{1}&=[d\lay{g}{1}_{\lay{z}{1}}]^TC^T\lay{\delta}{2}.
\end{align*}

Finally, we recall that our cost function $\J$ is the average sum of our loss function $\L$ over our training set, we get that
\begin{align*}
	\J(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1})&=\frac{1}{n}\sum_{j=1}^nF(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1},x_j),
\end{align*}
and hence that
\begin{align*}
	\frac{\partial\J}{\partial\lay{W}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\lay{a}{1}{_j}^T=\frac{1}{n}\lay{\delta}{2}\lay{a}{1}{^T}\\
	\frac{\partial\J}{\partial\lay{b}{2}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{2}{_j}\\
	\frac{\partial\J}{\partial\lay{W}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}x_j^T=\frac{1}{n}\lay{\delta}{1}x^T\\
	\frac{\partial\J}{\partial\lay{b}{1}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{1}{_j}
\end{align*}


\subsection{Activation Functions}
There are mainly only a handful of activating functions we consider for our non-linearity conditions.

\subsubsection{The Sigmoid Function}
We have the sigmoid function $\sigma(z)$ given by
$$\sigma:\R\to(0,1),\qquad \sigma(z)=\frac{1}{1+e^{-z}}.$$
We note that since
\begin{align*}
	1-\sigma(z)&=1-\frac{1}{1+e^{-z}}\\
	&=\frac{e^{-z}}{1+e^{-z}}
\end{align*}
\begin{align*}
	\sigma'(z)&=\frac{e^{-z}}{(1+e^{-z})^2}\\
	&=\frac{1}{1+e^{-z}}\cdot\frac{e^{-z}}{1+e^{-z}}\\
	&=\sigma(z)(1-\sigma(z))
\end{align*}

Moreover, suppose that $g:\R^m\to\R^m$ is the broadcasting of $\sigma$ from $\R$ to $\R^m$, then for $z=(z^1,...,z^m)\in\R^m$, we have that
$$g(z)=(\sigma(z^i)),$$
and $dg_z:T_z\R^m\to T_{g(z)}\R^m$ given by
\begin{align*}
	dg_z(v)&=\rest{\frac{d}{dt}}_{t=0}g(z+tv)\\
	&=\rest{\frac{d}{dt}}_{t=0}(\sigma(z^i+tv^i))\\
	&=(\sigma'(z^i)v^i)\\
	&=(\sigma(z^i)(1-\sigma(z^i))v^i)\\
	&=g(z)\odot(1-g(z))\odot v,
\end{align*}
where $\odot$ represents the Hadamard product (or component-wise multiplication); or rather, as as a matrix in $\R^{m\times m}$,
$$[dg_z]^\mu_\nu=\delta^\mu_\nu\sigma(z^\mu)(1-\sigma(z^\mu)).$$


\subsubsection{The Hyperbolic Tangent Function}
We have the hyperbolic tangent function $\tanh(z)$ given by
$$\tanh:\R\to(-1,1),\qquad\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}.$$
We then calculate
\begin{align*}
	\tanh'(z)&=\frac{(e^z+e^{-z})(e^z+e^{-z})-(e^z-e^{-z})(e^z-e^{-z})}{(e^z+e^{-z})^2}\\
	&=\frac{(e^z+e^{-z})^2}{(e^z+e^{-z})^2}-\frac{e^z-e^{-z})^2}{(e^z+e^{-z})^2}\\
	&=1-\tanh^2(z).
\end{align*}

Suppose $g:\R^m\to\R^m$ is the broadcasting of $\tanh$ from $\R$ to $\R^m$, then for $z=(z^1,...,z^m)\in\R^m$, we have that
$$g(z)=(\tanh(z^i)),$$
and $dg_z:T_z\R^m\to T_{g(z)}\R^m$ given by
\begin{align*}
	dg_z(v)&=[\tanh'(z^i)]\odot[v^i]\\
	&=[1-\tanh^2(z^i)]\odot[v^i]\\
	&=\delta^i_j(1-\tanh^2(z^i))v^j.
\end{align*}


\subsubsection{The Rectified Linear Unit Function}
We have the leaky-ReLU function $\relu(z;\beta)$ given by
$$\relu:\R\to\R,\qquad \relu(z;\beta)=\max\{\beta z, z\},$$
for some $\beta>0$ (typically chosen very small).

We have the rectified linear unit function $\relu(z)$ given by setting $\beta=0$ in the leaky-ReLu function, i.e.,
$$\relu:\R\to[0,\infty),\qquad\relu(z)=\relu(z;\beta=0)=\max\{0,z\}.$$
We then calculate
\begin{align*}
	\relu'(z;\beta)&=\begin{cases}
		\beta&z<0\\
		1&z\geq0
	\end{cases}\\
	&=\beta\chi_{(-\infty,0)}(z)+\chi_{[0,\infty)}(z),
\end{align*}
where
$$\chi_A(z)=\begin{cases}
	1&z\in A\\
	0&z\notin A
\end{cases},$$
is the indicator function.

Suppose $g:\R^m\to\R^m$ is the broadcasting of $\relu$ from $\R$ to $\R^m$.   Then for $z=(z^1,...,z^m)\in\R^m$, we have that
$$g(z)=\relu(z^i;\beta),$$
and $dg_z:T_z\R^m\to T_{g(z)}\R^m$ given by
\begin{align*}
	dg_z(v)&=[\relu'(z^i;\beta)]\odot[v^i]\\
	&=\delta^i_j(\beta\chi_{(-\infty,0)}(z^i)+\chi_{[0,\infty)}(z^i))v^j.
\end{align*}


\subsubsection{The Softmax Function}
We finally have the softmax function $\softmax(z)$ given by
$$\softmax:\R^m\to\R^m,\qquad \softmax(z)=\frac{1}{\sum_{j=1}^me^{z^j}}\begin{pmatrix}
	e^{z^1}\\
	e^{z^2}\\
	\vdots\\
	e^{z^m}
\end{pmatrix},$$
which we typically use on our outer-layer to obtain a probability distribution over our predicted labels.  Let
$$S^i=x^i\circ\softmax(z),$$
denote the $i$-th component of $\softmax(z)$, and so we calculate
\begin{align*}
	\frac{\partial S^i}{\partial z^j}&=\frac{\partial}{\partial z^j}\left[\left(\sum_{k=1}^me^{z^k}\right)^{-1}e^{z^i}\right]\\
	&=-\left(\sum_{k=1}^me^{z^k}\right)^{-2}\left(\sum_{k=1}^me^{z^k}\delta_j^k\right)e^{z^i}+\left(\sum_{k=1}^me^{z^k}\right)^{-1}e^{z^i}\delta^i_j\\
	&=-\left(\sum_{k=1}^me^{z^k}\right)^{-2}e^{z^j}e^{z^i}+S^i\delta^i_j\\
	&=-S^jS^i+S^i\delta^i_j\\
	&=S^i(\delta^i_j-S^j).
\end{align*}
That is, as a map $dS_z:T_z\R^m\to T_{S(z)}\R^m$, we have that
$$dS_z=[S^i(\delta^i_j-S_j)]^i_j,$$
and we make note that $dS_z$ is symmetric.





\subsection{Binary Classification - An Example}
We return the network given by
\begin{align*}
	\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{m_0}
	\end{bmatrix}}_{\text{Layer } 0}&\layerfctn{\lay{\varphi}{1}}\underbrace{\begin{bmatrix}
		\lay{z}{1}{^1}\\
		\vdots\\
		\lay{z}{1}{^{m_1}}
	\end{bmatrix}\layerfctn{\lay{g}{1}}
	\begin{bmatrix}
		\lay{a}{1}{^1}\\
		\vdots\\
		\lay{a}{1}{^{m_1}}
	\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\varphi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}
	\end{bmatrix}\layerfctn{\lay{g}{2}}
	\begin{bmatrix}
		\lay{a}{2}
	\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\hat{y},
\end{align*}
and show how such a model would be trained using python below.  We assume layer-$2$ has the sigmoid function (since it's binary classification) as an activator and our hidden layer has the $\relu$ function as activators.

We note that $m_2=1$ since we're dealing with a single activator in this layer, and
$$\lay{a}{2}=\lay{g}{2}(\lay{z}{2})=\sigma(\lay{z}{2}),$$
with
$$d(\lay{g}{2})_{\lay{z}{2}}=\sigma'(\lay{z}{2})=\sigma(\lay{z}{2})(1-\sigma(\lay{z}{2}))=\lay{a}{2}(1-\lay{a}{2}).$$
In layer-$1$, we have that
$$\lay{a}{1}=\lay{g}{1}(\lay{z}{1})=\relu(\lay{z}{1}),$$
with
$$d(\lay{g}{1})_{\lay{z}{1}}=\left[\delta^\mu_\nu\chi_{[0,\infty)}(\lay{z}{1}{^\mu})\right]^\mu_\nu.$$
Finally, we choose our loss function $\L(\hat{y},y)$ to be the log-loss function (since we're using the sigmoid activator on the outer-layer), i.e.,
$$\L(\hat{y},y)=-y\log(\hat{y})-(1-y)\log(1-\hat{y}),$$
or rather
$$\L(x,y)=-y\log(\lay{a}{2})-(1-y)\log(1-\lay{a}{2}).$$
We then have the cost function $\J$ given by
\begin{align*}
	\J(\lay{W}{2},\lay{b}{2},\lay{W}{1},\lay{b}{1})&=\frac{-1}{n}\sum_{j=1}^n\left(y_j\log(\lay{a}{2}{_j})+(1-y_j)\log(1-\lay{a}{2}{_j})\right)\\
	&=\frac{-1}{n}\left(\ip{y,\log(\lay{a}{2})}+\ip{1-y,\log(1-\lay{a}{2})}\right)
\end{align*}

Moreover, when using backpropagation, we see that
\begin{align*}
	\lay{\delta}{2}{^T_j}&=d(\L_{y_j})_{\lay{a}{2}}\cdot d(\lay{g}{2})_{\lay{z}{2}{_j}}\\
	&=\left(-\frac{y_j}{\lay{a}{2}{_j}}+\frac{1-y_j}{1-\lay{a}{2}{_j}}\right)\cdot(\lay{a}{2}{_j}(1-\lay{a}{2}{_j})\\
	&=\lay{a}{2}{_j}-y_j,
\end{align*}
or rather
$$\lay{\delta}{2}=\lay{a}{2}-y.$$
Similarly, we compute
\begin{align*}
	\lay{\delta}{1}{_j^T}&=\lay{\delta}{2}{_j^T}\lay{W}{2}[d\lay{g}{1}_{\lay{z}{1}{_j}}]\\
	&=\lay{\delta}{2}{_j^T}\lay{W}{2}[\delta^\mu_\nu\cdot\chi_{[0,\infty)}(\lay{z}{1}{^\mu_j})]
\end{align*}


\subsubsection{Random Initialization}
In the section that follows, we see that to begin gradient descent for a shallow neural network, we initialize our parameters $\lay{b}{\ell}$ to be $0$, but choose an arbitrarily small, but nonzero initialization for $\lay{W}{\ell}$.  Let's see why we choose $\lay{W}{\ell}$ to be nonzero.  Indeed, suppose we initialize with $\lay{b}{\ell}=0$ and $\lay{W}{\ell}=0$.  Then we see that
$$\lay{\delta}{1}{^T}=\lay{\delta}{2}\lay{W}{2}d\lay{g}{1}_{\lay{z}{1}}=0,$$
and so
$$\frac{\partial\J}{\partial\lay{W}{1}}=\frac{1}{n}\lay{\delta}{1}x^T=0.$$
Then we conclude that our parameter $\lay{W}{1}$ remains at $0$ during every iteration which is enough reason to not initialize $\lay{W}{2}$ at $0$.  Similarly, since
$$\lay{a}{1}=\tanh(\lay{W}{1}x+\lay{b}{1})=\tanh(0)=0,$$
we reach a similar conclusion about $\lay{W}{1}$ and $\lay{W}{2}$, respectively.
