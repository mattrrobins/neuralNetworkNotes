

\section{Batch Normalization}
\TOX{See \cite{DBLP:journals/corr/IoffeS15}.}

We recall feature-normalization:  Suppose $x\in\R^{n\times N}$ is some training data, and let
$$\mu=\E[X],\qquad\sigma^2=\E[(X-\mu)^2],$$
denote the mean and variance of the random-vector representation $X$ of $x$, respectively.  Then we consider the map
$$x_j\mapsto\frac{x_j-\mu}{\sigma}=:\hat{x}_j,$$
to be the \textit{normalization} of $x_j$.

This definition is so ``vanilla'', that it should be clear that this can be easily applied to each hidden-layer (we shall not use it on the output layer) of a neural network as well.  However, we first note that there is an ambiguous choice amongst the implementation, namely, do we normalize $\lay{z}{\ell}$ or $\lay{a}{\ell}$, i.e., does normalization occur before or after we compute the activation unit.  It seems more common to apply normalization to $\lay{z}{\ell}$, so that is what we do here without further mention of this choice.

Let $\gamma,\beta\in\R^n$, if we consider the map
$$\hat{x}_j\mapsto\gamma\odot\hat{x}_j+\beta:=\tilde{x}_j,$$
we can see fairly trivially that we can recover $x_j$ (thus allowing for identity activation units), indeed, let $\gamma=\sigma$ and $\beta=\mu$, and hence
\begin{align*}
	\tilde{x}_j&=\gamma\odot\hat{x}_j+\beta\\
	&=\gamma\odot\frac{x_j-\mu}{\sigma}+\beta\\
	&=x_j-\mu_\beta\\
	&=x_j
\end{align*}
as desired.  Moreover, we see that we can actually control what mean and variance we wish to impose on our input-vectors $x$.  Indeed, let $\hat{x}$ denote the normalized $x$, and consider
\begin{align*}
	\E[\gamma\odot \hat{X}+\beta]&=\frac{1}{n}\sum_{j=1}^n(\gamma\odot \hat{x}_j+\beta)\\
	&=\gamma\odot\E[\hat{X}]+\beta\\
	&=0+\beta\\
	&=\beta,
\end{align*}
and so the new mean would be given by $\beta$.  Similarly,
\begin{align*}
	\E[(\gamma\odot \hat{X}+\beta-\beta)^2]&=\frac{1}{n}\sum_{j=1}^n(\gamma\odot \hat{x}_j)^2\\
	&=\frac{1}{n}\sum_{j=1}^n(\gamma^2\odot \hat{x}_j^2)\\
	&=\gamma^2\odot\E[(\hat{X}-0)^2]\\
	&=\gamma^2\odot1\\
	&=\gamma^2
\end{align*}
and so we see the new variance would be given by $\gamma^2$.  Thus, we see that by composition, the act of normalization can be characterized by the new parameters $\gamma$ and $\beta$, and is mathematically-superfluous to consider both, but for computational considerations and algorithmic stability it shall be beneficial to keep both.  That is, suppose we're training on some batch $\X^k$ and focused on layer-$\ell$, with parameters $\lay{\gamma}{\ell},\lay{\beta}{\ell}\in\R^{\lay{n}{\ell}}$ and some $\epsilon>0$, arbitrarily small and prescribed for numerical stability, we define the \textit{batch-normalization} map $BN_{\lay{\gamma}{\ell},\lay{\beta}{\ell}}:\R^{\lay{n}{\ell}}\to\R^{\lay{n}{\ell}}$ given by the compositional-map
\begin{align*}
	\lay{z}{\ell}&\mapsto\frac{1}{|\X^k|}\sum_{x\in\X^k}\lay{z}{\ell}=:\lay{\mu}{\ell};\\
	(\lay{z}{\ell},\lay{\mu}{\ell})&\mapsto\frac{1}{|\X^k|}\sum_{x\in\X^k}(\lay{z}{\ell}-\lay{\mu}{\ell})^2=:\lay{\sigma}{\ell}{^2};\\
	(\lay{z}{\ell},\lay{\mu}{\ell},\lay{\sigma}{\ell},\epsilon)&\mapsto\frac{\lay{z}{\ell}-\lay{\mu}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}=:\lay{\hat{z}}{\ell};\\
	(\lay{\hat{z}}{\ell},\lay{\gamma}{\ell},\lay{\beta}{\ell})&\mapsto \lay{\gamma}{\ell}\odot\lay{\hat{z}}{\ell}+\lay{\beta}{\ell}=:\lay{\tilde{z}}{\ell}.
\end{align*}


Suppose we have an $L$-layer neural network, each layer with $\lay{n}{\ell}$ nodes, and we focus on the $\ell$-th layer specifically to expand:

{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{\ell}\times\lay{n}{\ell-1}}
		\arrow[d, swap, "\lay{w}{\ell}"]
		&{}
		&\R^{\lay{n}{\ell}}
		\arrow[d, swap, "\lay{\gamma}{\ell}"]
		&\R^{\lay{n}{\ell}}
		\arrow[d, swap, "\lay{\beta}{\ell}"]
		&{}
		\\
		\cdots
		\arrow[r, "\lay{a}{\ell-1}"]
		&\boxed{\lay{\phi}{\ell}}
		\arrow[r, "\lay{u}{\ell}"]
		&\boxed{\lay{N}{\ell}}
		\arrow[r, "\lay{\hat{u}}{\ell}"]
		&\boxed{\lay{\Gamma}{\ell}}
		\arrow[r, "\lay{v}{\ell}"]
		&\boxed{\lay{\psi}{\ell}}
		\arrow[r, "\lay{z}{\ell}"]
		&\boxed{\lay{G}{\ell}}
		\arrow[r, "\lay{a}{\ell}"]
		&\cdots
	\end{tikzcd}
\end{equation*}
}

We note that we've dropped the bias term $\lay{b}{\ell}$ in the above, forward-propagating diagram.  If we had included the term, the composition would result in the following
\begin{align*}
	\lay{a}{\ell-1}&\mapsto\lay{\gamma}{\ell}\odot\frac{\lay{w}{\ell}\lay{a}{\ell-1}+\lay{b}{\ell}-\lay{\mu}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}+\lay{\beta}{\ell}\\
	&=\frac{\lay{\gamma}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}\odot(\lay{w}{\ell}\lay{a}{\ell-1}-\lay{\mu}{\ell})+\lay{\beta}{\ell},
\end{align*}
after absorbing the $\lay{b}{\ell}$ into the parameter $\lay{\beta}{\ell}$.  That is, we have $3$ trainable parameters given by $\lay{w}{\ell}\in\R^{\lay{n}{\ell}\times \lay{n}{\ell-1}}$, $\lay{\gamma}{\ell},\lay{\beta}{\ell}\in\R^{\lay{n}{\ell}}.$




\subsection{Backward Propagation}

We consider $2$-layer, neural network utilizing batch normalization of the form
{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&{}
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{\gamma}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{\beta}{1}"]
		&{}
		\\
		\R^{\lay{n}{0}\times N}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{N}{1}}
		\arrow[r, "\lay{\hat{u}}{1}"]
		&\boxed{\lay{\Gamma}{1}}
		\arrow[r, "\lay{v}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\cdots\\
		{}
		&\cdots
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{\psi}{2}}
		\arrow[r, "\lay{z}{2}"]
		&\boxed{\lay{G}{2}}
		\arrow[r, "\lay{a}{2}"]
		&\boxed{\L}
		\arrow[r, "\text{cost}"]
		&\R
		\\
		{}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[u, swap, "\lay{w}{2}"]
		&\R^{\lay{n}{2}}
		\arrow[u, swap, "\lay{b}{2}"]
		&{}
		&\R^{\lay{n}{2}\times N}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}
where we have the functions
\begin{enumerate}
	\item $$\L:\R^{\lay{n}{2}\times N}\times\R^{\lay{n}{2}\times N}\to\R$$
	is the given loss function.  If we're working with a binary classification problem, then we have that
	\begin{align*}
		\L(y,\hat{y})&=-\frac{1}{N}\sum_{j=1}^n\left\{y_j\log{\hat{y}_j}+(1-y_j)\log(1-\hat{y}_j)\right\}\\
		&=-\frac{1}{N}\left[\ip{y,\log{y}}_{\R^N}+\ip{1-y,\log(1-\hat{y})}_{\R^N}\right].
	\end{align*}
	
	\item $$\lay{G}{\ell}:\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is the broadcasting of the activation unit $\lay{g}{\ell}:\R\to\R$.
	
	\item $$\lay{\phi}{\ell}:\R^{\lay{n}{\ell}\times\lay{n}{\ell-1}}\times\R^{\lay{n}{\ell-1}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\lay{\phi}{\ell}(w,x)=wx.$$
	
	\item $$\lay{\psi}{\ell}:\R^{\lay{n}{\ell}}\times\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\psi(b,x)=x+b\vec{1}^T,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^N.$$
	
	\item $$\lay{N}{1}:\R^{\lay{n}{1}\times N}\to\R^{\lay{n}{1}\times N}$$
	is the normalization operator given by
	$$\lay{N}{1}:x_j\mapsto \frac{x_j-\E[x]}{\sqrt{\V[x]+\epsilon}},$$
	where $\E$ is the expectation operator of a random vector, i.e.,
	$$\E[x]=\frac{1}{N}\sum_{j=1}^Nx_j,$$
	and $\V$ is the variance operator of a random vector, i.e.,
	$$\V[x]=\E[(x-\E[x]\vec{1}^T)^{\odot2}],$$
	where $\vec{1}\in\R^N$ and $\odot2$ represents the Hadamard-square.
	
	\item $$\lay{\Gamma}{\ell}:\R^{\lay{n}{\ell}}\times\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\Gamma(\gamma, x)=\gamma\vec{1}^T\odot x,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^N.$$
\end{enumerate}

We now consider back-propagating through the network via reverse differentiations as in the following diagram:


{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		&{}
		&\R^{\lay{n}{1}}
		&\R^{\lay{n}{1}}
		&{}
		\\
		\R^{\lay{n}{0}\times N}
		&\boxed{\lay{\phi}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "r_1"]
		&\boxed{\lay{N}{1}}
		\arrow[l, swap, "r"]
		&\boxed{\lay{\Gamma}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "\hat{r}_1"]
		&\boxed{\lay{\psi}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "\cl{r}_1"]
		&\boxed{\lay{G}{1}}
		\arrow[l, swap, "r"]
		&\cdots
		\arrow[l, swap, "r"]
		\\
		{}
		&\cdots
		&\boxed{\lay{\phi}{2}}
		\arrow[l, swap, "r"]
		\arrow[d, "r_2"]
		&\boxed{\lay{\psi}{2}}
		\arrow[l, swap, "r"]
		\arrow[d, "\cl{r}_2"]
		&\boxed{\lay{G}{2}}
		\arrow[l, swap, "r"]
		&\boxed{\L}
		\arrow[l, swap, "r"]
		&\R
		\arrow[l]
		\\
		{}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		&\R^{\lay{n}{2}}
		&{}
		&\R^{\lay{n}{2}\times N}
		\arrow[u]
		&{}
	\end{tikzcd}
\end{equation*}
}

We consider our individual derivatives:
\begin{enumerate}
	\item Suppose $G:\R^{m\times n}\to\R^{m\times n}$ is the broadcasting of $g:\R\to\R$.  Then for any $(x,\xi)\in T\R^{m\times n}$ we have that
	$$dG_x(\xi)=G'(x)\odot\xi.$$
	Then for any $\zeta\in T_{G(x)}\R^{m\times n}$, we have the reverse derivative is given by
	$$rG_x(\zeta)=G'(x)\odot\zeta.$$
	
	\item Suppose $\phi:\R^{m\times n}\times\R^{n\times N}\to\R^{m\times N}$ is given by
	$$\phi(w,x)=wx.$$
	Then we have two differential paths to consider:
	\begin{enumerate}
		\item For any $(w,x)\in \R^{m\times n}\times \R^{n\times N}$ and any $\xi\in T_x\R^{n\times N}$, we have that
		\begin{align*}
			d\phi_{(w,x)}(\xi)&=w\cdot\xi\\
			&=L_w(\xi),
		\end{align*}
		and for any $\zeta\in T_{\phi(w,x)}\R^{m\times N}$, we have the reverse differential
		\begin{align*}
			r\phi_{(w,x)}(\zeta)&=w^T\cdot\zeta\\
			&=L_{w^T}(\zeta).
		\end{align*}
		
		\item For any $(w,x)\in\R^{m\times n}\times \R^{n\times N}$ and any $\eta\in T_w\R^{m\times n}$, we have that
		\begin{align*}
			d_1\phi_{(w,x)}(\eta)&=\eta\cdot x\\
			R_x(\eta),
		\end{align*}
		and for any $\zeta\in T_{\phi(w,x)}\R^{m\times N}$, we have the reverse differential
		\begin{align*}
			r_1\phi_{(w,x)}(\zeta)&=\zeta\cdot x^T\\
			&=R_{x^T}(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item Suppose $\psi:\R^n\times\R^{n\times N}\to\R^{n\times N}$ is given by
	$$\psi(b,x)=x+b\vec{1}^T,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^N.$$
	Then we look at the two differential paths and for any $(b,x)\in\R^n\times\R^{n\times N}$ any any $\xi\in T_x\R^{n\times N}$, $\eta\in T_b\R^n$ and $\zeta\in T_{\psi(b,x)}\R^{n\times N}$:
	\begin{enumerate}
		\item In the network direction, we have that
		\begin{align*}
			d\psi_{(b,x)}(\xi)=\xi,
		\end{align*}
		with reverse differential
		$$r\psi_{(b,x)}(\zeta)=\zeta.$$
		
		\item In the parameter-space direction, we have that
		\begin{align*}
			\cl{d}\psi_{(b,x)}(\eta)&=\eta\cdot\vec{1}^T\\
			&=R_{\vec{1}^T}(\eta),
		\end{align*}
		with reverse differential
		\begin{align*}
			\cl{r}\psi_{(b,x)}(\zeta)&=\zeta\cdot\vec{1}\\
			&=R_{\vec{1}}(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item Suppose $\Gamma:\R^n\times\R^{n\times N}\to\R^{n\times N}$ is given by
	$$\Gamma(\gamma,x)=\gamma\vec{1}^T\odot x.$$
	The considering the two paths of differentiation, we have that for any $((\gamma,x), (\eta,\xi))\in T\R^n\oplus T\R^{n\times N}$ and $\zeta\in T_{\Gamma(\gamma,x)}\R^{n\times N}$ that:
	\begin{enumerate}
		\item In the network direction, we have that
		\begin{align*}
			d\Gamma_{(\gamma,x)}(\xi)&=\gamma\vec{1}^T\odot\xi,
		\end{align*}
		with reverse differential
		\begin{align*}
			r\Gamma_{(\gamma,x)}(\zeta)&=\gamma\vec{1}^T\odot\zeta.
		\end{align*}
		
		\item In the parameter-space direction, we have that
		\begin{align*}
			\hat{d}\Gamma_{(\gamma,x)}(\eta)&=\eta\vec{1}^T\odot x\\
			&=\odot_x\circ R_{\vec{1}^T}(\eta),
		\end{align*}
		with reverse differential
		\begin{align*}
			\hat{r}\Gamma_{(\gamma,x)}(\zeta)&=(x\odot\zeta)\cdot\vec{1}\\
			&=R_{\vec{1}}\circ\odot_x(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item For a coordinate-free derivation of the normalization operator, see \cref{sec:normOp}.  Otherwise, we let 
	$$N:\R^{n\times N}\to\R^{n\times N}, \qquad y:=N(x),\qquad\qquad y^\alpha_\beta=\frac{x^\alpha_\beta-\mu^\alpha}{\sqrt{\sigma^{2\alpha}+\epsilon}},$$
	and note that
	\begin{align*}
		\frac{\partial\mu^\alpha}{\partial x^i_j}&=\frac{1}{N}\delta^\alpha_i,
	\end{align*}
	and
	\begin{align*}
		\frac{\partial \sigma^{2\alpha}}{\partial x^i_j}&=\frac{2}{N}(x^\alpha_j-\mu^\alpha)\delta_i^\alpha.
	\end{align*}
	Hence,
	\begin{align*}
		\frac{\partial y^\alpha_\beta}{\partial x^i_j}&=-\frac{1}{2}(\sigma^{2\alpha}+\epsilon)^{-\frac{3}{2}}\left(\frac{\partial \sigma^{2\alpha}}{\partial x^i_j}\right)(x^\alpha_\beta-\mu^\alpha)+(\sigma^{2\alpha}+\epsilon)^{-\frac{1}{2}}\left(\frac{\partial x^\alpha_\beta}{\partial x^i_j}-\frac{\partial\mu^\alpha}{\partial x^i_j}\right)\\
		&=-\frac{1}{N}(\sigma^{2\alpha}+\epsilon)^{-\frac{3}{2}}(x^\alpha_j-\mu^\alpha)\delta^\alpha_i(x^\alpha_\beta-\mu^\alpha)+(\sigma^{2\alpha}+\epsilon)^{-\frac{1}{2}}\left(\delta_i^\alpha\delta_\beta^j-\frac{1}{N}\delta_i^\alpha\right)\\
		&=-\frac{1}{N}(\sigma^{2\alpha}+\epsilon)^{-\frac{1}{2}}y^\alpha_jy^\alpha_\beta\delta_i^\alpha+(\sigma^{2\alpha}+\epsilon)^{-\frac{1}{2}}\left(\delta^j_\beta-\frac{1}{N}\right)\delta_i^\alpha\\
		&=\frac{\delta^\alpha_i}{\sqrt{\sigma^{2\alpha}+\epsilon}}\left(\delta^j_\beta-\frac{1}{N}-\frac{1}{N}y^\alpha_jy^\alpha_\beta\right)\\
		&=\frac{\delta^\alpha_i}{\sqrt{\sigma^{2\alpha}+\epsilon}}\left(\delta^j_\beta-\frac{1}{N}(1+y^\alpha_jy^\alpha_\beta)\right).
	\end{align*}
	Thus for $(x,\xi^i_j)\in T\R^{n\times N}$, if we let $\mathcal{F}{{^\alpha}{_\beta}{_i}{^j}}$ denote the rank $(2,2)$-tensor representation for the forward differential, we have that
	\begin{align*}
		dN_x(\xi)&=\frac{\partial y^\alpha_\beta}{\partial x^i_j}\xi^i_j\\
		&=\mathcal{F}{{^\alpha}{_\beta}{_i}{^j}}\xi^i_j,
	\end{align*}
	and for $\zeta^\alpha_\beta\in T_y\R^{n\times N}$, if we let $\mathcal{R}{{^i}{_j}{_\alpha}{^\beta}}$ denote the rank $(2,2)$-tensor representation for the reverse differential, we have that
	\begin{align*}
		rN_x(\zeta)&=\sum_{\alpha=1}^n\sum_{\beta=1}^N\frac{\partial y^\alpha_\beta}{\partial x^i_j}\zeta^\alpha_\beta\\
		&=\mathcal{R}{{^i}{_j}{_\alpha}{^\beta}}\zeta^\alpha_\beta.
	\end{align*}
	
	\item For the loss function $\L:\R^N\times\R^N\to\R$ given by
	$$\L(y,\hat{y})=-\frac{1}{N}[\ip{y,\log{\hat{y}}}+\ip{1-y,\log(1-\hat{y})}],$$
	we fix $y,\hat{y}\in\R^N$ and for $\xi\in T_{\hat{y}}\R^N$, we see that
	\begin{align*}
		d\L_{(y,\hat{y})}(\xi)&=-\frac{1}{N}\sum_{j=1}^N\left[\frac{y_j}{\hat{y}_j}-\frac{1-y_j}{1-\hat{y}_j}\right]\xi_j\\
		&=-\frac{1}{N}\ip{\frac{y}{\hat{y}}-\frac{1-y}{1-\hat{y}},\xi},
	\end{align*}
	and hence for $\zeta\in T_{\L(y,\hat{y})}\R$, it follows that
	$$r\L_{(y,\hat{y})}(\zeta)=-\frac{1}{N}\left[\frac{y}{\hat{y}}-\frac{1-y}{1-\hat{y}}\right]\zeta,$$
	where the division is taken in the Hadamard sense.
	
\end{enumerate}

We're now ready to compute our various gradients of our cost function.  That is, if we let
$$\J:\R^{\lay{n}{2}}\times \R^{\lay{n}{2}\times\lay{n}{1}}\times\R^{\lay{n}{1}}\times\R^{\lay{n}{1}}\times\R^{\lay{n}{1}\times\lay{n}{0}}\to\R$$
be given by
{\small
\begin{align*}
	\J(p)&=\L(y,\lay{G}{2}\circ\lay{\psi}{2}(\lay{b}{2},\lay{\phi}{2}(\lay{w}{2},\lay{G}{2}\circ\lay{\psi}{2}(\lay{\beta}{1},\lay{\Gamma}{1}(\lay{\gamma}{1},\lay{N}{1}\circ\lay{\phi}{1}(\lay{w}{1},x)))))),
\end{align*}
}
where $p=(\lay{w}{2},\lay{\gamma}{1},\lay{\beta}{1},\lay{w}{2},\lay{b}{2})$ is a point in our parameter-space and we compute the reverse differentials for a learning rate $\alpha\in T_{\J(p)}\R$ with the assumption that our second activator function is the sigmoid function.  Indeed,

\begin{align*}
	r(\L\circ\lay{G}{2})_{\lay{z}{2}}(\alpha)&=r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{\lay{a}{2}}(\alpha)\\
	&=-\frac{\alpha}{N}\lay{G}{2}{'}(\lay{z}{2})\odot\left[\frac{y}{\lay{a}{2}}-\frac{1-y}{1-\lay{a}{2}}\right]\\
	&=-\frac{\alpha}{N}\lay{a}{2}(1-\lay{a}{2})\left[\frac{y}{\lay{a}{2}}-\frac{1-y}{1-\lay{a}{2}}\right]\\
	&=-\frac{\alpha}{N}[y(1-\lay{a}{2})-\lay{a}{2}(1-y)]\\
	&=-\frac{\alpha}{N}[y-\lay{a}{2}]\\
	&=\frac{\alpha}{N}(\lay{a}{2}-y).
\end{align*}

This leads us to
\begin{align*}
	\cl{r}_2\J_p(\alpha)&=\cl{r}_2(\lay{\psi}{2})_{(\lay{b}{2},\lay{u}{2})}\circ r\lay{G}{2}_{\lay{z}{2}}\circ r\L_{(y,\lay{a}{2})}\\
	&=\frac{\alpha}{N}R_{\vec{1}}(\lay{a}{2}-y)\\
	&=\frac{\alpha}{N}\sum_{j=1}^N(\lay{a}{2}{_j}-y_j);
\end{align*}

\begin{align*}
	r_2\J_p(\alpha)&=r_2\lay{\phi}{2}_{(\lay{w}{2},\lay{a}{1})}\circ r\lay{\psi}{2}_{(\lay{b}{2},\lay{u}{2})}\left(\frac{\alpha}{N}(\lay{a}{2}-y)\right)\\
	&=r_2\lay{\phi}{2}_{(\lay{w}{2},\lay{a}{1})}\left(\frac{\alpha}{N}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}(\lay{a}{2}-y)\lay{a}{1}{^T};
\end{align*}

\begin{align*}
	\cl{r}_1\J_p(\alpha)&=\cl{r}_1\lay{\psi}{1}_{(\lay{\beta}{1},\lay{v}{1})}\circ r\lay{G}{1}_{\lay{z}{1}}\circ r\lay{\phi}{2}_{(\lay{w}{2},\lay{a}{2})}\circ r\lay{\psi}{2}_{(\lay{b}{2},\lay{u}{2}}\circ r(\L\circ\lay{G}{2})_{\lay{z}{2}}(\alpha)\\
	&=\frac{\alpha}{N}\cl{r}_1\lay{\psi}{1}_{(\lay{\beta}{1},\lay{v}{1})}\circ r\lay{G}{1}_{\lay{z}{1}}\circ r\lay{\phi}{2}_{(\lay{w}{2},\lay{a}{2})}(\lay{a}{2}-y)\\
	&=\frac{\alpha}{N}\cl{r}_1\lay{\psi}{1}_{(\lay{\beta}{1},\lay{v}{1})}\circ r\lay{G}{1}_{\lay{z}{1}}\left(\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}\cl{r}_1\lay{\psi}{1}_{(\lay{\beta}{1},\lay{v}{1})}\left(\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}\sum_{j=1}^N\lay{g}{1}{'}(\lay{z}{1}{_j})\odot\lay{w}{2}{^T}(\lay{a}{2}{_j}-y_j);
\end{align*}

\begin{align*}
	\hat{r}_1\J_p(\alpha)&=\frac{\alpha}{N}\hat{r}_1\lay{\Gamma}{1}_{(\lay{\gamma}{1},\lay{\hat{u}}{1})}\left(\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}R_{\vec{1}}\left(\lay{\hat{u}}{1}\odot \left(\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right) \right)\\
	&=\frac{\alpha}{N}\sum_{j=1}^n\lay{\hat{u}}{1}{_j}\odot\lay{g}{1}{'}(\lay{z}{1}{_j})\odot\lay{w}{2}{^T}(\lay{a}{2}{_j}-y_j); 
\end{align*}
and finally,
\begin{align*}
	r_1\J_p(\alpha)&=\frac{\alpha}{N}r_1\lay{\phi}{1}_{(\lay{w}{1},x)}\circ r\lay{N}{1}_{\lay{u}{1}}\circ r\lay{\Gamma}{1}_{(\lay{\gamma}{1},\lay{\hat{u}}{1})}\left(\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}r_1\lay{\phi}{1}_{(\lay{w}{1},x)}\circ r\lay{N}{1}_{\lay{u}{1}}\left(\gamma\vec{1}^T\odot\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}R_{x^T}\circ r\lay{N}{1}_{\lay{u}{1}}\left(\gamma\vec{1}^T\odot\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}(\lay{a}{2}-y)\right)\\
	&=\frac{\alpha}{N}\sum_{j,l=1}^N\sum_{i=1}^{\lay{n}{1}}\mathcal{R}{{^k}{_l}{_i}{^j}}\gamma^i\lay{g}{1}{'}(\lay{z}{1}{^i{_j}})\lay{w}{2}{_i}(\lay{a}{2}{_j}-y_j)x^m_l.
\end{align*}

In general, to simply the construction in python, we utilize the auxiliary $\delta$'s as before.  To this, suppose we have an arbitrary $L$-layer neural network utilizing batch normalization on all hidden layers as in the following diagram:
{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&{}
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{\gamma}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{\beta}{1}"]
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[d, swap, "\lay{w}{2}"]
		&{}
		&{}
		&{}
		\\
		\R^{\lay{n}{0}\times N}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{N}{1}}
		\arrow[r, "\lay{\hat{u}}{1}"]
		&\boxed{\lay{\Gamma}{1}}
		\arrow[r, "\lay{v}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{N}{2}}
		\arrow[r, "\lay{\hat{u}}{2}"]
		&\cdots
		\\
		\cdots
		\arrow[r, "\lay{\hat{u}}{L-1}"]
		&\boxed{\lay{\Gamma}{L-1}}
		\arrow[r, "\lay{v}{L-1}"]
		&\boxed{\lay{\psi}{L-1}}
		\arrow[r, "\lay{z}{L-1}"]
		&\boxed{\lay{G}{L-1}}
		\arrow[r, "\lay{a}{L-1}"]
		&\boxed{\lay{\phi}{L}}
		\arrow[r, "\lay{u}{L}"]
		&\boxed{\lay{\psi}{L}}
		\arrow[r, "\lay{z}{L}"]
		&\boxed{\lay{G}{L}}
		\arrow[r, "\lay{a}{L}"]
		&\boxed{\L}
		\arrow[r, "\text{cost}"]
		&\R
		\\
		{}
		&{}
		&\R^{\lay{n}{L-1}}
		\arrow[u, swap, "\lay{\gamma}{L-1}"]
		&\R^{\lay{n}{L-1}}
		\arrow[u, swap, "\lay{\beta}{L-1}"]
		&{}
		&\R^{\lay{n}{L}\times\lay{n}{L-1}}
		\arrow[u, swap, "\lay{w}{L}"]
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "\lay{b}{L}"]
		&\R^{\lay{n}{L}\times N}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}

Then we build our $\delta$-differentials recursively starting at $\L$.  That is,
\begin{align*}
	\lay{\delta}{L}&:=r(\L\circ\lay{G}{L})_{\lay{z}{L}},\\
	\lay{\delta}{L-1}&:=\lay{G}{L-1}{'}(\lay{z}{L-1})\odot\lay{w}{L}{^T}\lay{\delta}{L},\\
	\lay{\delta}{L-2}&:=\lay{G}{L-2}{'}(\lay{z}{L-2})\odot\lay{w}{L-1}{^T}r\lay{N}{L-1}_{\lay{u}{L-1}}(\lay{\gamma}{L-1}\vec{1}^T\odot\lay{\delta}{L-1}),\\
	&\vdots\\
	\lay{\delta}{\ell}&:=\lay{G}{\ell}{'}(\lay{z}{\ell})\odot\lay{w}{\ell+1}{^T}r\lay{N}{\ell+1}_{\lay{u}{\ell+1}}(\lay{\gamma}{\ell+1}\vec{1}^T\lay{\delta}{\ell+1}),\\
	&\vdots\\
	\lay{\delta}{1}&:=\lay{G}{1}{'}(\lay{z}{1})\odot\lay{w}{2}{^T}r\lay{N}{2}_{\lay{u}{2}}(\lay{\gamma}{2}\vec{1}^T\lay{\delta}{2});
\end{align*}
and compute our gradients via
\begin{align*}
	\frac{\partial\J}{\partial\lay{b}{L}}&=\sum_{j=1}^N\lay{\delta}{L}{_j},\\
	\frac{\partial\J}{\partial\lay{w}{L}}&=\lay{\delta}{L}\lay{a}{L-1}{^T},\\
	\frac{\partial\J}{\partial\lay{\beta}{\ell}}&=\sum_{j=1}^N\lay{\delta}{\ell},\qquad\ell\in\{1,...,L-1\},\\
	\frac{\partial\J}{\partial\lay{\gamma}{\ell}}&=\sum_{j=1}^N\lay{\hat{u}}{\ell}{_j}\odot\lay{\delta}{\ell}{_j},\qquad\ell\in\{1,...,L-1\},\\
	\frac{\partial\J}{\partial\lay{w}{\ell}}&=r\lay{N}{\ell}_{\lay{u}{\ell}}(\lay{\gamma}{\ell}\vec{1}^T\lay{\delta}{\ell})\lay{a}{\ell-1}{^T},\qquad\ell\in\{1,...,L-1\}.
\end{align*}







\begin{comment}

\subsection{Backward Propagation - OLD}

We now show how batch normalization affects the backward propagation algorithm.  For illustrative purposes, we assume a $2$-layer neural network with arbitrary activation functions and generic loss function.  We recall the setup (without bias $\lay{b}{\ell}$) used in \cref{sec:backPropDerivation}
\begin{align*}
	&\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{m_0}
		\end{bmatrix}}_{\text{Layer } 0}
	\layerfctn{\lay{\Phi}{1}}\underbrace{\begin{bmatrix}
			\lay{z}{1}{^1}\\
			\vdots\\
			\lay{z}{1}{^{m_1}}
			\end{bmatrix}
			\layerfctn{BN_{\gamma,\beta}}
		\begin{bmatrix}
			\lay{\tilde{z}}{\ell}{^1}\\
			\vdots\\
			\lay{\tilde{z}}{\ell}{_{m_\ell}}
		\end{bmatrix}
			\layerfctn{\lay{g}{1}}
			\begin{bmatrix}
			\lay{a}{1}{^1}\\
			\vdots\\
			\lay{a}{1}{^{m_1}}
			\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\Phi}{2}}\cdots\\
	&\cdots\layerfctn{\lay{\Phi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}{^1}\\
		\vdots\\
		\lay{z}{2}{^{m_2}}
		\end{bmatrix}\layerfctn{\lay{g}{2}}
		\begin{bmatrix}
			\lay{a}{2}{^1}\\
			\vdots\\
			\lay{a}{2}{^{m_2}}
		\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\begin{bmatrix}
		\hat{y}^1\\
		\vdots\\
		\hat{y}^{m_2}
	\end{bmatrix},
\end{align*}
where
$$\lay{\Phi}{1}:\R^{m_1\times m_0}\times\R^{m_0}\to\R^{m_1},\qquad\lay{\Phi}{1}(A,x)=Ax;$$
and
$$\lay{\Phi}{2}:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\to\R^{m_2},\qquad\lay{\Phi}{2}(A,b,x)=Ax+b.$$
\HOX{Since we don't use batch normalization on the output layer, the bias term still exists.}
Define the compositional function
$$G:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\times\R^{m_1}\times\R^{m_1\times m_0}\times\R^{m_0}\to\R,$$
given by
\begin{align*}
	G(B,b,\gamma,\beta,A,x)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_{\gamma,\beta}(\lay{\Phi}{1}(A,x))).
\end{align*}

This leads to compute some auxiliary differentials before continuing further.



\begin{lem}
Let $\X=\{x_1,...,x_N\}\subset\R^m$ which gives rise to the mean
$$\mu=\E[x]=\frac{1}{N}\sum_{j=1}^Nx_j,$$
and the (component-wise) variance
$$\sigma^2=\E[(x-\mu)^2]=\frac{1}{N}\sum_{j=1}^N(x_j-\mu)^2.$$
For $x\in\X$, define the normalization $\hat{x}\in\R^m$ by
$$\hat{x}=(\sigma^2+\epsilon)^{-1/2}\odot(x-\mu).$$
\end{lem}

\begin{proof}
We first note that since $\sigma^2$ depends on $\mu$ that
	\begin{align*}
		\frac{\partial(\sigma^2)^i}{\partial \mu^\nu}&=\frac{2}{N}\sum_{j=1}^N(x_j^i-\mu^i)(-\delta^i_\nu)\\
		&=-2\delta^i_\nu\left(\frac{1}{N}\sum_{j=1}^Nx_j^i-\mu^i\right)\\
		&=-2\delta^i_\nu(\mu^i-\mu^i)\\
		&=0.
	\end{align*}
Moreover,
\begin{align*}
	\frac{d\mu^i}{d x^\nu_\lambda}&=\frac{1}{N}\sum_{j=1}^N\delta^i_\nu\delta_j^\lambda\\
	&=\frac{1}{N}\delta^i_\nu,
\end{align*}
and
\begin{align*}
	\frac{d(\sigma^2)^i}{dx^\nu_\lambda}&=\frac{\partial(\sigma^2)^i}{\partial\mu^\rho}\frac{\partial\mu^\rho}{\partial x^\nu_\lambda}+\frac{\partial(\sigma^2)^i}{\partial x^\nu_\lambda}\\
	&=0+\frac{\partial}{\partial x^\nu_\lambda}\left(\frac{1}{N}\sum_{j=1}^N(x_j^i-\mu^i)^2\right)\\
	&=\frac{2}{N}\sum_{j=1}^N\left((x_j^i-\mu^i)\delta_j^\lambda\delta_\nu^i\right)\\
	&=\frac{2}{N}(x_\lambda^i-\mu^i)\delta^i_\nu.
\end{align*}
Thus for $x_{j_0}\in\X$,
\begin{align*}
	\rest{\frac{d \hat{x}^i}{d x^\nu_\lambda}}_{x=x_{j_0}}&=\frac{\partial\hat{x}^i}{\partial (\sigma^2)^\rho}\frac{d(\sigma^2)^\rho}{dx^\nu_\lambda}+\frac{\partial \hat{x}^i}{\partial \mu^\rho}\frac{d\mu^\rho}{dx^\nu_\lambda}+\frac{\partial \hat{x}^i}{\partial x^\nu_\lambda}\\
	&=\left(-\frac{1}{2}((\sigma^2)^i+\epsilon)^{-3/2}(x^i-\mu^i)\delta^i_\rho\right)\left(\frac{2}{N}(x^\rho_\lambda-\mu^\rho)\delta^\rho_\nu\right)\\
	&\qquad+\left(((\sigma^2)^i+\epsilon)^{-1/2}(-\delta^i_\rho)\right)\left(\frac{1}{N}\delta^\rho_\nu\right)\\
	&\qquad +\left(((\sigma^2)^i+\epsilon)^{-1/2})\delta_{j_0}^\lambda\delta_\nu^i\right)\\
	&=\left(-\frac{1}{N}((\sigma^2)^i+\epsilon)^{-3/2}(x^i-\mu^i)\delta^i_\nu(x^\nu_\lambda-\mu^\nu)\right)\\
	&\qquad+\left(\frac{-1}{N}((\sigma^2)^i+\epsilon)^{-1/2}\delta^i_\nu\right)+\left(((\sigma^2)^i+\epsilon)^{-1/2})\delta_{j_0}^\lambda\delta_\nu^i\right)
\end{align*}
\end{proof}





\begin{lem}
	For $N\in\N$, we define the expectation function $\E:\R^N\to\R$ given by
	$$\E[(x_1,...,x_N)]=\frac{1}{N}\sum_{j=1}^Nx_j.$$
	Let $z=\{z_1,...,z_N\}\subset\R$ be fixed, and define the mean
	$$\mu:=\E[z]=\frac{1}{N}\sum_{j=1}^Nz_j.$$
	Then as a differential, we have that $d\E_z:T_z\R^N\to T_\mu\R$ given by
	$$d\E_z=\frac{1}{N}\sum_{j=1}^N\rest{dx_j}_{x=z},\qquad d\E_z(v)=\frac{1}{N}\sum_{j=1}^Nv^j.$$
	Moreover, for $\alpha=1,...,N$, let
	$\iota_{z_\alpha}:\R\to\R^N$ denote the inclusion
	$$\iota_{z_\alpha}(x)=(z_1,...,z_{\alpha-1},x,z_{\alpha+1},...,z_N).$$
	Then the differentials
	$$d_\alpha\E_{z_\alpha}:=d(\E\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R\to T_\mu\R,$$
	are given by
	\begin{align*}
		d_\alpha\E_{z_\alpha}&=d(\E\circ\iota_{z_\alpha})_{z_\alpha}\\
		&=d\E_z\cdot d(\iota_{z_\alpha})_{z_\alpha}\\
		&=\frac{1}{N}dx_{z_\alpha}.
	\end{align*}
	
	
	
	Similarly, we define the variance function $\V:\R^N\to\R$ given by
	$$\V[(x_1,...,x_N)]=\frac{1}{N}\sum_{j=1}^N(x_j-\E[(x_1,...,x_N)])^2.$$
	For fixed $z$, define the variance
	$$\sigma^2=\V[z].$$
	Then as a differential, we have that $d\V_z:T_z\R^N\to T_{\sigma^2}\R$ given by
	$$d\V_z=\frac{2}{N}\sum_{j=1}^N(z_j-\mu)\rest{dx^j}_{x=z},\qquad d\V_z(v)=\frac{2}{N}\sum_{j=1}^N(z_j-\mu)v^j.$$
	Moreover, for $\alpha=1,...,N$, the differentials
	$$d_\alpha\V_{z_\alpha}:=d(\V\circ\iota_{z_\alpha})_{z_{\alpha}}:T_{z_\alpha}\R\to T_{\sigma^2}\R$$
	are given by
	\begin{align*}
		d_\alpha\V_{z_\alpha}&=d(\V\circ\iota_{z_\alpha})_{z_\alpha}\\
		&=d\V_z\cdot d(\iota_{z_\alpha})_{z_\alpha}\\
		&=\frac{2}{N}(z_\alpha-\mu)dx_{z_\alpha}
	\end{align*}
\end{lem}

\begin{proof}
	Immediate from direct calculation.
\end{proof}


\begin{lem}
	Let $\mathcal{N}:\R^N\to\R^N$ denote the normalization transformation with $\hat{x}=\mathcal{N}(x)$.  That is,
	\begin{align*}
		\hat{x}&:=\mathcal{N}(x)\\
		&=\frac{x^j-\E[x]}{\sqrt{\V[x]+\epsilon}}\vec{e}_j,
	\end{align*}
	for some $\epsilon>0$ sufficiently small.  Fix $z\in\R^N$, and let
	$$\mu:=\E[z],\qquad\sigma^2:=\V[z],$$
	Then as a differential, we have that $d\mathcal{N}_z:T_z\R^N\to T_{\hat{z}}\R^N$ given by
	\begin{align*}
		d\mathcal{N}_z&=\begin{bmatrix}
			d\hat{x}^1_z\\
			\vdots\\
			d\hat{x}^N_z
		\end{bmatrix},
	\end{align*}
	where
	$$d\hat{x}^\alpha_z=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-d\E_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}d\V_z\right).$$
\end{lem}

\begin{proof}
	We note that
	\begin{align*}
		d\hat{x}^\alpha_z&=d\left((x^\alpha-\E[x])(\V[x]+\epsilon)^{-1/2}\right)_z\\
		&=(dx^\alpha_z-d\E_z)(\sigma^2+\epsilon)^{-1/2})-\frac{1}{2}(\sigma^2+\epsilon)^{-3/2}(z^\alpha-\mu)d\V_z\\
		&=\frac{dx^\alpha_z}{\sqrt{\sigma^2+\epsilon}}-\frac{d\E_z}{\sqrt{\sigma^2+\epsilon}}-\frac{1}{2}\frac{z^\alpha-\mu}{(\sigma^2+\epsilon)^{3/2}}d\V_z\\
		&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-d\E_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}d\V_z\right)
		%&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-\frac{1}{N}\sum_{j=1}^Ndx^j_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}\frac{2}{N}\sum_{j=1}^N(z^j-\mu)dx^j_z\right)\\
		%&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-\frac{1}{N}\sum_{j=1}^N\left(1+\frac{z^\alpha-\mu}{\sigma^2+\epsilon}(z^j-\mu)\right)dx^j_z\right)
	\end{align*}
\end{proof}



\begin{cor}
	For $\alpha=1,...,N$, let $\mathcal{N}_\alpha:\R^{m\times N}\to\R^m$ denote the $\alpha$-th component of the vector-valued, normalization transformation.  That is,
		$$\hat{x}_\alpha=\mathcal{N}_\alpha(x_1,...,x_N),$$
		with
		\begin{align*}
			\hat{x}_\alpha^i&=\frac{\pi_\alpha(x^i)-\E[x^i]}{(\V[x^i]+\epsilon)^{\frac{1}{2}}},
		\end{align*}
		where $\pi_\alpha:\R^N\to\R$ is the projection onto the $\alpha$-th coordinate
		$$\pi_\alpha(x_1,...,x_N)=x_\alpha.$$

	Fix $z_1,...,z_N\in\R^m$, let $\mu=\E[z]\in\R^m$ denote vector-mean and let $\sigma^2=\V[z]\in\R^m$ denote the component-wise, vector-variation (i.e., $(\sigma^2)^i=\V[z^i]$).   Then the differentials 
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}:=d(\mathcal{N}_\alpha\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R^m\to T_{\hat{z}_\alpha}\R^m$$ are given by the diagonal matrices
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}=\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{1}{N}\frac{(z_\alpha^i-\mu^i)^2}{((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)\delta^i_j.$$
	
\end{cor}

\begin{proof}
	We compute directly after noting that
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}=\begin{bmatrix}
		d_\alpha(\hat{x}_\alpha^1)_{z_\alpha^1}&\cdots&0\\
		0&\ddots&0\\
		0&\cdots&d_\alpha(\hat{x}_\alpha^m)_{z_\alpha^m}
	\end{bmatrix}$$  
	To this end, fix $1\leq i\leq m$ and we compute
	\begin{align*}
		d_\alpha(\hat{x}_\alpha^i)_{z_\alpha^i}&=d_\alpha(\mathcal{N}_\alpha^i)_{z_\alpha^i}\\
		&=\frac{d_\alpha(\pi_\alpha)_{z_\alpha^i}-d_\alpha\E_{z_\alpha^i}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{z_\alpha^i-\mu^i}{2((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}d_\alpha\V_{z_\alpha^i}\\
		&=\frac{dx_{z_\alpha^i}-\frac{1}{N}dx_{z_\alpha^i}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{z_\alpha^i-\mu^i}{2((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\left(\frac{2}{N}(z_\alpha^i-\mu^i)dx_{z_\alpha^i}\right)\\
		&=\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{(z_\alpha^i-\mu^i)^2}{N((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)dz_\alpha^i,
	\end{align*}
	as desired.
\end{proof}


\begin{prop}
	Let $\mathcal{N}:\R^{m\times N}\to\R^{m\times N}$ denote the usual normalization transformation with $\hat{x}_\alpha=\mathcal{N}_\alpha(x)$.
	Let $BN:\R^m\times\R^m\times\R^{m\times N}\to\R^{m\times N}$ denote the batch normalization transformation $[x_j]\mapsto[\tilde{x}_j]$, i.e.,
	\begin{align*}
		\tilde{x}^i_j&=\gamma^i\hat{x}^i_j+\beta^i,
	\end{align*}
	where $x^i\in\R^N$.  Moreover, given $\gamma,\beta\in\R^m$, for $\alpha\in\{1,...,N\}$, let
	$$BN^{\gamma,\beta}_\alpha:\R^{m\times N}\to\R^m$$
	denote
	$$BN^{\gamma,\beta}_\alpha(x)=\gamma\odot\mathcal{N}_\alpha(x)+\beta.$$
	
	
	Fix $z_1,...,z_N,\in\R^m$, and let
	$$\hat{z}_\alpha=\mathcal{N}_\alpha(z_1,...,z_N)\in\R^m,\qquad \mu^i=\E[z^i]\in\R,\qquad(\sigma^2)^i=\V[z^i]\in\R.$$
	
	For $\alpha\in\{1,...,N\}$, $z\in\R^{m\times N}$ and for $\gamma,\beta\in\R^m$, we have the differentials:
	\begin{itemize}	
		\item $d(BN_\alpha^{\beta,z})_\gamma:T_\gamma\R^m\to T_{\tilde{z}}\R^m,$ is given by
		$$d(BN_\alpha^{\beta,z})_\gamma(v)=\hat{z}_\alpha\odot v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial\gamma^j}=\hat{z}_\alpha^i\delta^i_j.$$
		\item $d(BN_\alpha^{\gamma,z})_\beta:T_\beta\R^m\to T_{\tilde{z}}\R^m$ is given by
		$$d(BN_\alpha^{\gamma,z})_\beta(v)=v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial \beta^j}=\delta^i_j.$$
		\item $d(BN_\alpha^{\gamma,\beta})_{\hat{z}_\alpha}:T_{\hat{z}_\alpha}\R^m\to T_{\tilde{z}}\R^m$ is given by
		$$d(BN_\alpha^{\gamma,\beta})_{\hat{z}_\alpha}(v)=\gamma\odot v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial\hat{z}_\alpha^j}=\gamma^i\delta^i_j.$$
		\item $d_\alpha(BN_\alpha^{\gamma,\beta})_{z_\alpha}:=d(BN_\alpha^{\gamma,\beta}\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R^m\to T_{\tilde{z}_\alpha}\R^m$ is given by
		$$d_\alpha(BN_\alpha^{\gamma,\beta})_{z_\alpha}=(\gamma\odot)d_\alpha(\mathcal{N}_\alpha)_{z_\alpha},$$
		\begin{align*}
			\frac{\partial\tilde{z}^i_\alpha}{\partial z^j_\alpha}&=\gamma^i\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{(z_\alpha^i-\mu^i)^2}{N((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)\delta^i_j
		\end{align*}
	\end{itemize}
\end{prop}

\begin{proof}
	Follows immediately from the previous Corollary.
\end{proof}



We now return to considering the compositional function
$$G:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\times\R^{m_1}\times\R^{m_1\times m_0}\times\R^{m_0}\to\R,$$
given by
\begin{align*}
	G(B,b,\gamma,\beta,A,x_\alpha)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\gamma,\beta}(\lay{\Phi}{1}(A,x))).
\end{align*}
We compute (and since $\alpha\in\{1,...,N\}$ is fixed, we ignore implied summation for the moment)
\begin{itemize}
	\item \begin{align*}
		d_BG_B(V)&=d_B(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2})_B(V)\\
		&=\rest{\frac{d}{dt}}_{t=0}\L_y\circ\lay{g}{2}((B+tV)\lay{a}{1}{_\alpha}+b)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho\rest{\frac{d}{dt}}_{t=0}\left[(B^\rho_\lambda+tV^\rho_\lambda)\lay{a}{1}{^\lambda_\alpha} + b^\rho)\right]\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho V^\rho_\lambda\lay{a}{1}{^\lambda_\alpha}\\
		&=(\lay{a}{1}{_\alpha}\lay{\delta}{2}{_\alpha}{^T})_\rho^\lambda V_\lambda^\rho,
	\end{align*}
	and hence
	$$d_BG_B=\lay{a}{1}{_\alpha}\lay{\delta}{2}{_\alpha}{^T},\qquad \frac{\partial G}{\partial B}=\lay{\delta}{2}{_\alpha}\lay{a}{1}{_\alpha}{^T}.$$
	
	\item \begin{align*}
		d_bG_b(v)&=d_B(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2})_b(v)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho\rest{\frac{d}{dt}}_{t=0}\left[B^\rho_\lambda\lay{a}{1}{^\lambda_\alpha}+(b^\rho+tv^\rho)\right]\\
		&=\lay{\delta}{2}{_\alpha}{^T}v
	\end{align*}
	yielding
	$$d_bG_b=\lay{\delta}{2}{_\alpha}{^T},\qquad\frac{\partial G}{\partial b}=\lay{\delta}{2}{_\alpha}.$$
	
	\item \begin{align*}
		d_\gamma G_\gamma(\xi)&=d_\gamma(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\beta,\lay{z}{1}{_\alpha}}))_\gamma(\xi)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})\cdot B\cdot d\lay{g}{1}_{\lay{\tilde{z}}{1}{_\alpha}}(\hat{z}_\alpha\odot\xi)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})\cdot B\cdot d\lay{g}{1}_{\lay{\tilde{z}}{1}{_\alpha}}\diag(\lay{\hat{z}}{1}_\alpha)\xi\\
		&=\lay{\delta}{1}{_\alpha}{^T}\diag(\lay{\hat{z}}{1}{_\alpha})\xi,
	\end{align*}
	and so
	$$d_\gamma G_\gamma=\lay{\delta}{1}{_\alpha}{^T}\diag(\lay{\hat{z}}{1}{_\alpha}),\qquad\frac{\partial G}{\partial\gamma}=\diag(\lay{\hat{z}}{1}{_\alpha})\lay{\delta}{1}{_\alpha}.$$
	
	\item \begin{align*}
		d_\beta G_\beta(\eta)&=d_\beta(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\gamma,\lay{z}{1}{_\alpha}}))_\beta(\eta)\\
		&=\lay{\delta}{1}{_\alpha}{^T}\eta,
	\end{align*}
	thus
	$$d_\beta G_\beta=\lay{\delta}{1}{_\alpha}{^T},\qquad\frac{\partial G}{\partial\beta}=\lay{\delta}{1}{_\alpha}.$$
	
	\item \begin{align*}
		d_AG_A(V)&=\lay{\delta}{1}{_\alpha}{^T}\cdot d_\alpha (BN_\alpha^{\gamma,\beta})_{\lay{z}{1}{_\alpha}}d\lay{\Phi}{1}_A(V)\\
		&=\lay{\delta}{1}{_\alpha}{^T}\diag(\gamma) d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}}Vx_\alpha,
	\end{align*}
	and hence
	$$d_AG_A=x_\alpha\lay{\delta}{1}{_\alpha}{^T}\diag(\gamma)d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}},$$
	$$\frac{\partial G}{\partial A}=\diag(\gamma)d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}}\lay{\delta}{1}{_\alpha}x_\alpha{^T}.$$
\end{itemize}

Finally, since
$$\J(\lay{W}{2},\lay{b}{2},\gamma,\beta,\lay{W}{1})=\frac{1}{N}\sum_{\alpha=1}^NG(\lay{W}{2},\lay{b}{2},\gamma,\beta,\lay{W}{1},x_\alpha),$$
we've described our desired gradients after summation.






	



Similar to earlier, we define
\begin{align*}
	\lay{\delta}{1}&=[d\lay{g}{1}_{\lay{\tilde{z}}{1}}]^TB^T\lay{\delta}{2}\\
	&=\frac{\partial G}{\partial \lay{\tilde{z}}{1}}\\
	&=[dG_{\lay{\tilde{z}}{1}}]^T,
\end{align*}
and so
\begin{align*}
	dG_\gamma&=dG_{\lay{\tilde{z}}{1}}\cdot d(\lay{\tilde{z}}{1})_\gamma
\end{align*}







For backward propagation, we compute derivatives in coordinates
\begin{align*}
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{z}{\ell}{^\nu}}&=\frac{\lay{\gamma}{\ell}{^i}}{\lay{\sigma}{\ell}{^i}}\delta^i_\nu,\\
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{\gamma}{\ell}{^\nu}}&=\frac{\lay{z}{\ell}{^i}-\lay{\mu}{\ell}{^i}}{\lay{\sigma}{\ell}{^i}}\delta^i_\nu,\\
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{\beta}{\ell}{^\nu}}&=\delta^i_\nu.
\end{align*}

\begin{remark}
	Note that it may seem that $\mu$ and $\sigma$ depend on the variable $z$, but they actually depend on the full set of potential values, and are hence constants.  NOT TRUE
\end{remark}






	
\end{comment}



\subsection{Inferencing}
We note that in our computation for forward propagation, that our normalization transforms change with our batches.  This leads to ambiguity when predicting a label for a new example.  To fix this ambiguity, we will use exponentially moving averages to accumulate the means and variances of a given layer.  That is, given a mini-batch partition $\{\X^k:1\leq k \leq B\}$ and epoch $i$, we let
$$t:=iB + k,$$
denote the iteration.  Fixing a layer $\ell$, we let
$$\lay{\cl{\mu}}{\ell}{_0}=0,\qquad\lay{\cl{\sigma}^2}{\ell}{_0}=0,$$
and for some momentum $\beta\in[0,1]$ we define
$$\lay{\cl{\mu}}{\ell}{_t}=\beta\lay{\cl{\mu}}{\ell}{_{t-1}}+(1-\beta)\lay{\mu}{\ell}{_t},$$
and
$$\lay{\cl{\sigma}^2}{\ell}{_t}=\beta\lay{\cl{\sigma}^2}{\ell}{_{t-1}}+(1-\beta)\lay{\sigma^2}{\ell}{_t}.$$
After convergence, we use $\lay{\cl{\mu}}{\ell}$ and $\lay{\cl{\sigma}^2}{\ell}$ in evaluation on our tests sets.




\begin{comment}
\subsection{Inferencing - OLD}
We note that in our computation for forward propagation, that our normalization transforms change with our batches.  This leads to ambiguity when predicting a label for a new example.  One fix would be to average our means and variances over our batches.  That is, suppose during our iteration process, we have training-batches of the form $\{\X^k:1\leq k\leq K\}$, where each $\X^k$ has cardinality $|\X^k|=n$.  Then for each hidden-layer $\ell\in\{1,...,L-1\}$, we obtain the means
$$\lay{\mu}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}\lay{z}{\ell},$$
and the variances
$$\lay{\sigma^2}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}(\lay{z}{\ell}-\lay{\mu}{\ell}{_k})^2.$$
That is, for each hidden-layer $\ell$, we have the collection
$$\{\lay{\mu}{\ell}{_k}:1\leq k\leq K\}$$
from which we average again to obtain
$$\lay{\mu}{\ell}:=\frac{1}{K}\sum_{k=1}^K\lay{\mu}{\ell}{_k},$$
and the collection
$$\{\lay{\sigma^2}{\ell}{_k}:1\leq k\leq K\},$$
from which we use the unbiased estimate
$$\lay{\sigma^2}{\ell}:=\frac{n}{n-1}\frac{1}{K}\sum_{k=1}^K\lay{\sigma^2}{\ell}{_k}.$$
These quantities are what we use when computing the batch-normalization transforms of the hidden units for new examples.
\end{comment}




\subsection{Algorithm Outline}
Suppose we have a training set $\X$ with which we wish to train a binary classification via an $L$-layer neural network.  Let $N=|\X|$ and let $n=2^p$ be the batch size with $B=\lceil\frac{N}{n}\rceil$ batches per epoch.  Then our algorithm would be as follows:
\begin{enumerate}[1.]
	\item Set hyper-parameters. Initialize parameters and running statistics.
	\item For $0\leq i\leq\texttt{num\_epochs}$:
	\begin{enumerate}[a.]
		\item Generate batches $\{\X^k:1\leq k\leq B\}$.
		\item For $1\leq k\leq B$:
		\begin{enumerate}[i.]
			\item $$t=iB + k.$$
			\item Perform forward propagation on $\X^k$:
			\begin{itemize}
				\item $$\lay{u}{1}=\lay{w}{1}x$$
				\item For $\ell\in\{1,...,L-1\}$:
				\begin{itemize}
					\item $$\lay{u}{\ell}=\lay{w}{\ell}\lay{a}{\ell-1}$$
					\item $$\lay{\mu}{\ell}{_t}=\frac{1}{n}\sum_{x\in\X^k}\lay{u}{\ell}$$
					\item $$\lay{\cl{\mu}}{\ell}{_t}=\beta\lay{\cl{\mu}}{\ell}{_{t-1}}+(1-\beta)\lay{\mu}{\ell}{_t}$$
					\item $$\lay{\sigma^2}{\ell}{_t}=\frac{1}{n}\sum_{x\in\X^k}(\lay{u}{\ell}-\lay{\mu}{\ell}{_t})^2$$
					\item $$\lay{\cl{\sigma}^2}{\ell}{_t}=\beta\lay{\cl{\sigma}^2}{\ell}{_{t-1}}+(1-\beta)\lay{\sigma^2}{\ell}{_t}$$
					\item $$\lay{\hat{u}}{\ell}=(\lay{\sigma^2}{\ell}{_t}+\epsilon)^{-\frac{1}{2}}\odot(\lay{u}{\ell}-\lay{\mu}{\ell}{_t})$$
					\item $$\lay{z}{\ell}=\lay{\gamma}{\ell}\odot\lay{\hat{u}}{\ell}+\lay{\beta}{\ell}$$
					\item $$\lay{a}{\ell}=\lay{g}{\ell}(\lay{z}{\ell})$$
				\end{itemize}
				\item $$\lay{z}{L}=\lay{w}{L}\lay{a}{L-1}+b$$
				\item $$\lay{a}{L}=\lay{g}{L}(\lay{z}{L})$$
			\end{itemize}
			\item Compute cost $\J$ on $\X^k$.
			\item Apply backwards propagation on $\X^k$ to obtain
			$$\frac{\partial\J}{\partial\lay{w}{\ell}},\quad\frac{\partial\J}{\partial b},\quad\frac{\partial\J}{\partial\lay{\gamma}{\ell}},\quad\frac{\partial\J}{\partial\lay{\beta}{\ell}}.$$
			\item Update parameters.
		\end{enumerate}
	\end{enumerate}
	\item Return
	$$\lay{w}{\ell},\quad b,\quad\lay{\gamma}{\ell},\quad\lay{\beta}{\ell},\quad\lay{\cl{\mu}}{\ell},\quad\lay{\cl{\sigma}^2}{\ell}.$$
\end{enumerate}








\subsection{Python Implementation via \texttt{numpy}}

Below we implement batch normalization with our usual mini-batch gradient descent optimization.  To adapt other optimization techniques an update to the optimization classes is required to account for the trainable parameters.  This is not difficult, but is not included in this implementation for brevity.
\lstinputlisting[firstline=1, lastline=407]{src/python/batchNormalization/npBatchNormalization.py}








