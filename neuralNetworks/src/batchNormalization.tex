

\section{Batch Normalization}
\TOX{See \cite{DBLP:journals/corr/IoffeS15}.}

We recall feature-normalization:  Suppose $x\in\R^{m\times n}$ is some training data, and let
$$\mu=\E[X],\qquad\sigma^2=\E[(X-\mu)^2],$$
denote the mean and variance of the random-vector representation $X$ of $x$, respectively.  Then we consider the map
$$x_j\mapsto\frac{x_j-\mu}{\sigma}=:\hat{x}_j,$$
to be the \textit{normalization} of $x_j$.

This definition is so ``vanilla'', that it should be clear that this can be easily applied to each hidden-layer (we shall not use it on the output layer) of a neural network as well.  However, we first note that there is an ambiguous choice amongst the implementation, namely, do we normalize $\lay{z}{\ell}$ or $\lay{a}{\ell}$, i.e., does normalization occur before or after we compute the activation unit.  It seems more common to apply normalization to $\lay{z}{\ell}$, so that is what we do here without further mention of this choice.

Let $\gamma,\beta\in\R^m$, if we consider the map
$$\hat{x}_j\mapsto\gamma\odot\hat{x}_j+\beta:=\tilde{x}_j,$$
we can see fairly trivially that we can recover $x_j$ (thus allowing for identity activation units), indeed, let $\gamma=\sigma$ and $\beta=\mu$, and hence
\begin{align*}
	\tilde{x}_j&=\gamma\odot\hat{x}_j+\beta\\
	&=\gamma\odot\frac{x_j-\mu}{\sigma}+\beta\\
	&=x_j-\mu_\beta\\
	&=x_j
\end{align*}
as desired.  Moreover, we see that we can actually control what mean and variance we wish to impose on our input-vectors $x$.  Indeed, let $\hat{x}$ denote the normalized $x$, and consider
\begin{align*}
	\E[\gamma\odot \hat{X}+\beta]&=\frac{1}{n}\sum_{j=1}^n(\gamma\odot \hat{x}_j+\beta)\\
	&=\gamma\odot\E[\hat{X}]+\beta\\
	&=0+\beta\\
	&=\beta,
\end{align*}
and so the new mean would be given by $\beta$.  Similarly,
\begin{align*}
	\E[(\gamma\odot \hat{X}+\beta-\beta)^2]&=\frac{1}{n}\sum_{j=1}^n(\gamma\odot \hat{x}_j)^2\\
	&=\frac{1}{n}\sum_{j=1}^n(\gamma^2\odot \hat{x}_j^2)\\
	&=\gamma^2\odot\E[(\hat{X}-0)^2]\\
	&=\gamma^2\odot1\\
	&=\gamma^2
\end{align*}
and so we see the new variance would be given by $\gamma^2$.  Thus, we see that by composition, the act of normalization can be characterized by the new parameters $\gamma$ and $\beta$, and is mathematically-superfluous to consider both, but for computational considerations and algorithmic stability it shall be beneficial to keep both.  That is, suppose we're training on some batch $\X^k$ and focused on layer-$\ell$, with parameters $\lay{\gamma}{\ell},\lay{\beta}{\ell}\in\R^{m_\ell}$ and some $\epsilon>0$, arbitrarily small and prescribed for numerical stability, we define the \textit{batch-normalization} map $BN_{\lay{\gamma}{\ell},\lay{\beta}{\ell}}:\R^{m_\ell}\to\R^{m_\ell}$ given by the compositional-map
\begin{align*}
	\lay{z}{\ell}&\mapsto\frac{1}{|\X^k|}\sum_{x\in\X^k}\lay{z}{\ell}=:\lay{\mu}{\ell};\\
	(\lay{z}{\ell},\lay{\mu}{\ell})&\mapsto\frac{1}{|\X^k|}\sum_{x\in\X^k}(\lay{z}{\ell}-\lay{\mu}{\ell})^2=:\lay{\sigma}{\ell}{^2};\\
	(\lay{z}{\ell},\lay{\mu}{\ell},\lay{\sigma}{\ell},\epsilon)&\mapsto\frac{\lay{z}{\ell}-\lay{\mu}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}=:\lay{\hat{z}}{\ell};\\
	(\lay{\hat{z}}{\ell},\lay{\gamma}{\ell},\lay{\beta}{\ell})&\mapsto \lay{\gamma}{\ell}\odot\lay{\hat{z}}{\ell}+\lay{\beta}{\ell}=:\lay{\tilde{z}}{\ell}.
\end{align*}


Suppose we have an $L$-layer neural network, each layer with $m_\ell$ nodes, and we focus on the $\ell$-th layer specifically to expand:
\begin{equation*}
	\cdots\layerfctn{\lay{\varphi}{\ell}}
	\underbrace{
		\begin{bmatrix}
			\lay{z}{1}{^1}\\
			\vdots\\
			\lay{z}{1}{^{m_\ell}}
		\end{bmatrix}
		\layerfctn{BN_{\lay{\gamma}{\ell},\lay{\beta}{\ell}}}
		\begin{bmatrix}
			\lay{\tilde{z}}{\ell}{^1}\\
			\vdots\\
			\lay{\tilde{z}}{\ell}{_{m_\ell}}
		\end{bmatrix}
		\layerfctn{\lay{g}{\ell}}
		\begin{bmatrix}
			\lay{a}{1}{^1}\\
			\vdots\\
			\lay{a}{1}{^{m_\ell}}
		\end{bmatrix}
		}_{\text{Layer } \ell}
	\layerfctn{\lay{\varphi}{\ell+1}}\cdots
\end{equation*}
The procedure for forward propagation should be immediately obvious from the closer look at layer-$\ell$. However, we notice that
\begin{align*}
	\lay{a}{\ell-1}&\mapsto\lay{\gamma}{\ell}\odot\frac{\lay{W}{\ell}\lay{a}{\ell-1}+\lay{b}{\ell}-\lay{\mu}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}+\lay{\beta}{\ell}\\
	&=\frac{\lay{\gamma}{\ell}}{\sqrt{\lay{\sigma}{\ell}{^2}+\epsilon}}(\lay{W}{\ell}\lay{a}{\ell-1}-\lay{\mu}{\ell})+\lay{\beta}{\ell},
\end{align*}
after absorbing the $\lay{b}{\ell}$ into the parameter $\lay{\beta}{\ell}$.  That is, we have $3$ trainable parameters given by $\lay{W}{\ell}\in\R^{m_{\ell}\times m_{\ell-1}}$, $\lay{\gamma}{\ell},\lay{\beta}{\ell}\in\R^{m_\ell}.$




\subsection{Backward Propagation}

We now show how batch normalization affects the backward propagation algorithm.  For illustrative purposes, we assume a $2$-layer neural network with arbitrary activation functions and generic loss function.  We recall the setup (without bias $\lay{b}{\ell}$) used in \cref{sec:backPropDerivation}
\begin{align*}
	&\underbrace{\begin{bmatrix}
		x^1\\
		\vdots\\
		x^{m_0}
		\end{bmatrix}}_{\text{Layer } 0}
	\layerfctn{\lay{\Phi}{1}}\underbrace{\begin{bmatrix}
			\lay{z}{1}{^1}\\
			\vdots\\
			\lay{z}{1}{^{m_1}}
			\end{bmatrix}
			\layerfctn{BN_{\gamma,\beta}}
		\begin{bmatrix}
			\lay{\tilde{z}}{\ell}{^1}\\
			\vdots\\
			\lay{\tilde{z}}{\ell}{_{m_\ell}}
		\end{bmatrix}
			\layerfctn{\lay{g}{1}}
			\begin{bmatrix}
			\lay{a}{1}{^1}\\
			\vdots\\
			\lay{a}{1}{^{m_1}}
			\end{bmatrix}}_{\text{Layer } 1}\layerfctn{\lay{\Phi}{2}}\cdots\\
	&\cdots\layerfctn{\lay{\Phi}{2}}
	\underbrace{\begin{bmatrix}
		\lay{z}{2}{^1}\\
		\vdots\\
		\lay{z}{2}{^{m_2}}
		\end{bmatrix}\layerfctn{\lay{g}{2}}
		\begin{bmatrix}
			\lay{a}{2}{^1}\\
			\vdots\\
			\lay{a}{2}{^{m_2}}
		\end{bmatrix}}_{\text{Layer } 2}\layerfctn{=}\begin{bmatrix}
		\hat{y}^1\\
		\vdots\\
		\hat{y}^{m_2}
	\end{bmatrix},
\end{align*}
where
$$\lay{\Phi}{1}:\R^{m_1\times m_0}\times\R^{m_0}\to\R^{m_1},\qquad\lay{\Phi}{1}(A,x)=Ax;$$
and
$$\lay{\Phi}{2}:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\to\R^{m_2},\qquad\lay{\Phi}{2}(A,b,x)=Ax+b.$$
\HOX{Since we don't use batch normalization on the output layer, the bias term still exists.}
Define the compositional function
$$G:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\times\R^{m_1}\times\R^{m_1\times m_0}\times\R^{m_0}\to\R,$$
given by
\begin{align*}
	G(B,b,\gamma,\beta,A,x)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_{\gamma,\beta}(\lay{\Phi}{1}(A,x))).
\end{align*}

This leads to compute some auxiliary differentials before continuing further.





\begin{comment}
\begin{lem}
Let $\X=\{x_1,...,x_N\}\subset\R^m$ which gives rise to the mean
$$\mu=\E[x]=\frac{1}{N}\sum_{j=1}^Nx_j,$$
and the (component-wise) variance
$$\sigma^2=\E[(x-\mu)^2]=\frac{1}{N}\sum_{j=1}^N(x_j-\mu)^2.$$
For $x\in\X$, define the normalization $\hat{x}\in\R^m$ by
$$\hat{x}=(\sigma^2+\epsilon)^{-1/2}\odot(x-\mu).$$
\end{lem}

\begin{proof}
We first note that since $\sigma^2$ depends on $\mu$ that
	\begin{align*}
		\frac{\partial(\sigma^2)^i}{\partial \mu^\nu}&=\frac{2}{N}\sum_{j=1}^N(x_j^i-\mu^i)(-\delta^i_\nu)\\
		&=-2\delta^i_\nu\left(\frac{1}{N}\sum_{j=1}^Nx_j^i-\mu^i\right)\\
		&=-2\delta^i_\nu(\mu^i-\mu^i)\\
		&=0.
	\end{align*}
Moreover,
\begin{align*}
	\frac{d\mu^i}{d x^\nu_\lambda}&=\frac{1}{N}\sum_{j=1}^N\delta^i_\nu\delta_j^\lambda\\
	&=\frac{1}{N}\delta^i_\nu,
\end{align*}
and
\begin{align*}
	\frac{d(\sigma^2)^i}{dx^\nu_\lambda}&=\frac{\partial(\sigma^2)^i}{\partial\mu^\rho}\frac{\partial\mu^\rho}{\partial x^\nu_\lambda}+\frac{\partial(\sigma^2)^i}{\partial x^\nu_\lambda}\\
	&=0+\frac{\partial}{\partial x^\nu_\lambda}\left(\frac{1}{N}\sum_{j=1}^N(x_j^i-\mu^i)^2\right)\\
	&=\frac{2}{N}\sum_{j=1}^N\left((x_j^i-\mu^i)\delta_j^\lambda\delta_\nu^i\right)\\
	&=\frac{2}{N}(x_\lambda^i-\mu^i)\delta^i_\nu.
\end{align*}
Thus for $x_{j_0}\in\X$,
\begin{align*}
	\rest{\frac{d \hat{x}^i}{d x^\nu_\lambda}}_{x=x_{j_0}}&=\frac{\partial\hat{x}^i}{\partial (\sigma^2)^\rho}\frac{d(\sigma^2)^\rho}{dx^\nu_\lambda}+\frac{\partial \hat{x}^i}{\partial \mu^\rho}\frac{d\mu^\rho}{dx^\nu_\lambda}+\frac{\partial \hat{x}^i}{\partial x^\nu_\lambda}\\
	&=\left(-\frac{1}{2}((\sigma^2)^i+\epsilon)^{-3/2}(x^i-\mu^i)\delta^i_\rho\right)\left(\frac{2}{N}(x^\rho_\lambda-\mu^\rho)\delta^\rho_\nu\right)\\
	&\qquad+\left(((\sigma^2)^i+\epsilon)^{-1/2}(-\delta^i_\rho)\right)\left(\frac{1}{N}\delta^\rho_\nu\right)\\
	&\qquad +\left(((\sigma^2)^i+\epsilon)^{-1/2})\delta_{j_0}^\lambda\delta_\nu^i\right)\\
	&=\left(-\frac{1}{N}((\sigma^2)^i+\epsilon)^{-3/2}(x^i-\mu^i)\delta^i_\nu(x^\nu_\lambda-\mu^\nu)\right)\\
	&\qquad+\left(\frac{-1}{N}((\sigma^2)^i+\epsilon)^{-1/2}\delta^i_\nu\right)+\left(((\sigma^2)^i+\epsilon)^{-1/2})\delta_{j_0}^\lambda\delta_\nu^i\right)
\end{align*}
\end{proof}

\end{comment}



\begin{lem}
	For $N\in\N$, we define the expectation function $\E:\R^N\to\R$ given by
	$$\E[(x_1,...,x_N)]=\frac{1}{N}\sum_{j=1}^Nx_j.$$
	Let $z=\{z_1,...,z_N\}\subset\R$ be fixed, and define the mean
	$$\mu:=\E[z]=\frac{1}{N}\sum_{j=1}^Nz_j.$$
	Then as a differential, we have that $d\E_z:T_z\R^N\to T_\mu\R$ given by
	$$d\E_z=\frac{1}{N}\sum_{j=1}^N\rest{dx_j}_{x=z},\qquad d\E_z(v)=\frac{1}{N}\sum_{j=1}^Nv^j.$$
	Moreover, for $\alpha=1,...,N$, let
	$\iota_{z_\alpha}:\R\to\R^N$ denote the inclusion
	$$\iota_{z_\alpha}(x)=(z_1,...,z_{\alpha-1},x,z_{\alpha+1},...,z_N).$$
	Then the differentials
	$$d_\alpha\E_{z_\alpha}:=d(\E\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R\to T_\mu\R,$$
	are given by
	\begin{align*}
		d_\alpha\E_{z_\alpha}&=d(\E\circ\iota_{z_\alpha})_{z_\alpha}\\
		&=d\E_z\cdot d(\iota_{z_\alpha})_{z_\alpha}\\
		&=\frac{1}{N}dx_{z_\alpha}.
	\end{align*}
	
	
	
	Similarly, we define the variance function $\V:\R^N\to\R$ given by
	$$\V[(x_1,...,x_N)]=\frac{1}{N}\sum_{j=1}^N(x_j-\E[(x_1,...,x_N)])^2.$$
	For fixed $z$, define the variance
	$$\sigma^2=\V[z].$$
	Then as a differential, we have that $d\V_z:T_z\R^N\to T_{\sigma^2}\R$ given by
	$$d\V_z=\frac{2}{N}\sum_{j=1}^N(z_j-\mu)\rest{dx^j}_{x=z},\qquad d\V_z(v)=\frac{2}{N}\sum_{j=1}^N(z_j-\mu)v^j.$$
	Moreover, for $\alpha=1,...,N$, the differentials
	$$d_\alpha\V_{z_\alpha}:=d(\V\circ\iota_{z_\alpha})_{z_{\alpha}}:T_{z_\alpha}\R\to T_{\sigma^2}\R$$
	are given by
	\begin{align*}
		d_\alpha\V_{z_\alpha}&=d(\V\circ\iota_{z_\alpha})_{z_\alpha}\\
		&=d\V_z\cdot d(\iota_{z_\alpha})_{z_\alpha}\\
		&=\frac{2}{N}(z_\alpha-\mu)dx_{z_\alpha}
	\end{align*}
\end{lem}

\begin{proof}
	Immediate from direct calculation.
\end{proof}



\begin{comment}

\begin{lem}
	Let $\mathcal{N}:\R^N\to\R^N$ denote the normalization transformation with $\hat{x}=\mathcal{N}(x)$.  That is,
	\begin{align*}
		\hat{x}&:=\mathcal{N}(x)\\
		&=\frac{x^j-\E[x]}{\sqrt{\V[x]+\epsilon}}\vec{e}_j,
	\end{align*}
	for some $\epsilon>0$ sufficiently small.  Fix $z\in\R^N$, and let
	$$\mu:=\E[z],\qquad\sigma^2:=\V[z],$$
	Then as a differential, we have that $d\mathcal{N}_z:T_z\R^N\to T_{\hat{z}}\R^N$ given by
	\begin{align*}
		d\mathcal{N}_z&=\begin{bmatrix}
			d\hat{x}^1_z\\
			\vdots\\
			d\hat{x}^N_z
		\end{bmatrix},
	\end{align*}
	where
	$$d\hat{x}^\alpha_z=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-d\E_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}d\V_z\right).$$
\end{lem}

\begin{proof}
	We note that
	\begin{align*}
		d\hat{x}^\alpha_z&=d\left((x^\alpha-\E[x])(\V[x]+\epsilon)^{-1/2}\right)_z\\
		&=(dx^\alpha_z-d\E_z)(\sigma^2+\epsilon)^{-1/2})-\frac{1}{2}(\sigma^2+\epsilon)^{-3/2}(z^\alpha-\mu)d\V_z\\
		&=\frac{dx^\alpha_z}{\sqrt{\sigma^2+\epsilon}}-\frac{d\E_z}{\sqrt{\sigma^2+\epsilon}}-\frac{1}{2}\frac{z^\alpha-\mu}{(\sigma^2+\epsilon)^{3/2}}d\V_z\\
		&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-d\E_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}d\V_z\right)
		%&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-\frac{1}{N}\sum_{j=1}^Ndx^j_z-\frac{z^\alpha-\mu}{2(\sigma^2+\epsilon)}\frac{2}{N}\sum_{j=1}^N(z^j-\mu)dx^j_z\right)\\
		%&=\frac{1}{\sqrt{\sigma^2+\epsilon}}\left(dx^\alpha_z-\frac{1}{N}\sum_{j=1}^N\left(1+\frac{z^\alpha-\mu}{\sigma^2+\epsilon}(z^j-\mu)\right)dx^j_z\right)
	\end{align*}
\end{proof}

\end{comment}

\begin{cor}
	For $\alpha=1,...,N$, let $\mathcal{N}_\alpha:\R^{m\times N}\to\R^m$ denote the $\alpha$-th component of the vector-valued, normalization transformation.  That is,
		$$\hat{x}_\alpha=\mathcal{N}_\alpha(x_1,...,x_N),$$
		with
		\begin{align*}
			\hat{x}_\alpha^i&=\frac{\pi_\alpha(x^i)-\E[x^i]}{(\V[x^i]+\epsilon)^{\frac{1}{2}}},
		\end{align*}
		where $\pi_\alpha:\R^N\to\R$ is the projection onto the $\alpha$-th coordinate
		$$\pi_\alpha(x_1,...,x_N)=x_\alpha.$$

	Fix $z_1,...,z_N\in\R^m$, let $\mu=\E[z]\in\R^m$ denote vector-mean and let $\sigma^2=\V[z]\in\R^m$ denote the component-wise, vector-variation (i.e., $(\sigma^2)^i=\V[z^i]$).   Then the differentials 
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}:=d(\mathcal{N}_\alpha\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R^m\to T_{\hat{z}_\alpha}\R^m$$ are given by the diagonal matrices
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}=\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{1}{N}\frac{(z_\alpha^i-\mu^i)^2}{((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)\delta^i_j.$$
	
\end{cor}

\begin{proof}
	We compute directly after noting that
	$$d_\alpha(\mathcal{N}_\alpha)_{z_\alpha}=\begin{bmatrix}
		d_\alpha(\hat{x}_\alpha^1)_{z_\alpha^1}&\cdots&0\\
		0&\ddots&0\\
		0&\cdots&d_\alpha(\hat{x}_\alpha^m)_{z_\alpha^m}
	\end{bmatrix}$$  
	To this end, fix $1\leq i\leq m$ and we compute
	\begin{align*}
		d_\alpha(\hat{x}_\alpha^i)_{z_\alpha^i}&=d_\alpha(\mathcal{N}_\alpha^i)_{z_\alpha^i}\\
		&=\frac{d_\alpha(\pi_\alpha)_{z_\alpha^i}-d_\alpha\E_{z_\alpha^i}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{z_\alpha^i-\mu^i}{2((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}d_\alpha\V_{z_\alpha^i}\\
		&=\frac{dx_{z_\alpha^i}-\frac{1}{N}dx_{z_\alpha^i}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{z_\alpha^i-\mu^i}{2((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\left(\frac{2}{N}(z_\alpha^i-\mu^i)dx_{z_\alpha^i}\right)\\
		&=\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{(z_\alpha^i-\mu^i)^2}{N((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)dz_\alpha^i,
	\end{align*}
	as desired.
\end{proof}


\begin{prop}
	Let $\mathcal{N}:\R^{m\times N}\to\R^{m\times N}$ denote the usual normalization transformation with $\hat{x}_\alpha=\mathcal{N}_\alpha(x)$.
	Let $BN:\R^m\times\R^m\times\R^{m\times N}\to\R^{m\times N}$ denote the batch normalization transformation $[x_j]\mapsto[\tilde{x}_j]$, i.e.,
	\begin{align*}
		\tilde{x}^i_j&=\gamma^i\hat{x}^i_j+\beta^i,
	\end{align*}
	where $x^i\in\R^N$.  Moreover, given $\gamma,\beta\in\R^m$, for $\alpha\in\{1,...,N\}$, let
	$$BN^{\gamma,\beta}_\alpha:\R^{m\times N}\to\R^m$$
	denote
	$$BN^{\gamma,\beta}_\alpha(x)=\gamma\odot\mathcal{N}_\alpha(x)+\beta.$$
	
	
	Fix $z_1,...,z_N,\in\R^m$, and let
	$$\hat{z}_\alpha=\mathcal{N}_\alpha(z_1,...,z_N)\in\R^m,\qquad \mu^i=\E[z^i]\in\R,\qquad(\sigma^2)^i=\V[z^i]\in\R.$$
	
	For $\alpha\in\{1,...,N\}$, $z\in\R^{m\times N}$ and for $\gamma,\beta\in\R^m$, we have the differentials:
	\begin{itemize}	
		\item $d(BN_\alpha^{\beta,z})_\gamma:T_\gamma\R^m\to T_{\tilde{z}}\R^m,$ is given by
		$$d(BN_\alpha^{\beta,z})_\gamma(v)=\hat{z}_\alpha\odot v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial\gamma^j}=\hat{z}_\alpha^i\delta^i_j.$$
		\item $d(BN_\alpha^{\gamma,z})_\beta:T_\beta\R^m\to T_{\tilde{z}}\R^m$ is given by
		$$d(BN_\alpha^{\gamma,z})_\beta(v)=v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial \beta^j}=\delta^i_j.$$
		\item $d(BN_\alpha^{\gamma,\beta})_{\hat{z}_\alpha}:T_{\hat{z}_\alpha}\R^m\to T_{\tilde{z}}\R^m$ is given by
		$$d(BN_\alpha^{\gamma,\beta})_{\hat{z}_\alpha}(v)=\gamma\odot v,\qquad\frac{\partial\tilde{z}_\alpha^i}{\partial\hat{z}_\alpha^j}=\gamma^i\delta^i_j.$$
		\item $d_\alpha(BN_\alpha^{\gamma,\beta})_{z_\alpha}:=d(BN_\alpha^{\gamma,\beta}\circ\iota_{z_\alpha})_{z_\alpha}:T_{z_\alpha}\R^m\to T_{\tilde{z}_\alpha}\R^m$ is given by
		$$d_\alpha(BN_\alpha^{\gamma,\beta})_{z_\alpha}=(\gamma\odot)d_\alpha(\mathcal{N}_\alpha)_{z_\alpha},$$
		\begin{align*}
			\frac{\partial\tilde{z}^i_\alpha}{\partial z^j_\alpha}&=\gamma^i\left(\frac{1-\frac{1}{N}}{\sqrt{(\sigma^2)^i+\epsilon}}-\frac{(z_\alpha^i-\mu^i)^2}{N((\sigma^2)^i+\epsilon)^{\frac{3}{2}}}\right)\delta^i_j
		\end{align*}
	\end{itemize}
\end{prop}

\begin{proof}
	Follows immediately from the previous Corollary.
\end{proof}



We now return to considering the compositional function
$$G:\R^{m_2\times m_1}\times\R^{m_2}\times\R^{m_1}\times\R^{m_1}\times\R^{m_1\times m_0}\times\R^{m_0}\to\R,$$
given by
\begin{align*}
	G(B,b,\gamma,\beta,A,x_\alpha)&=\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\gamma,\beta}(\lay{\Phi}{1}(A,x))).
\end{align*}
We compute (and since $\alpha\in\{1,...,N\}$ is fixed, we ignore implied summation for the moment)
\begin{itemize}
	\item \begin{align*}
		d_BG_B(V)&=d_B(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2})_B(V)\\
		&=\rest{\frac{d}{dt}}_{t=0}\L_y\circ\lay{g}{2}((B+tV)\lay{a}{1}{_\alpha}+b)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho\rest{\frac{d}{dt}}_{t=0}\left[(B^\rho_\lambda+tV^\rho_\lambda)\lay{a}{1}{^\lambda_\alpha} + b^\rho)\right]\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho V^\rho_\lambda\lay{a}{1}{^\lambda_\alpha}\\
		&=(\lay{a}{1}{_\alpha}\lay{\delta}{2}{_\alpha}{^T})_\rho^\lambda V_\lambda^\rho,
	\end{align*}
	and hence
	$$d_BG_B=\lay{a}{1}{_\alpha}\lay{\delta}{2}{_\alpha}{^T},\qquad \frac{\partial G}{\partial B}=\lay{\delta}{2}{_\alpha}\lay{a}{1}{_\alpha}{^T}.$$
	
	\item \begin{align*}
		d_bG_b(v)&=d_B(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2})_b(v)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})_\rho\rest{\frac{d}{dt}}_{t=0}\left[B^\rho_\lambda\lay{a}{1}{^\lambda_\alpha}+(b^\rho+tv^\rho)\right]\\
		&=\lay{\delta}{2}{_\alpha}{^T}v
	\end{align*}
	yielding
	$$d_bG_b=\lay{\delta}{2}{_\alpha}{^T},\qquad\frac{\partial G}{\partial b}=\lay{\delta}{2}{_\alpha}.$$
	
	\item \begin{align*}
		d_\gamma G_\gamma(\xi)&=d_\gamma(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\beta,\lay{z}{1}{_\alpha}}))_\gamma(\xi)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})\cdot B\cdot d\lay{g}{1}_{\lay{\tilde{z}}{1}{_\alpha}}(\hat{z}_\alpha\odot\xi)\\
		&=(\lay{\delta}{2}{_\alpha}{^T})\cdot B\cdot d\lay{g}{1}_{\lay{\tilde{z}}{1}{_\alpha}}\diag(\lay{\hat{z}}{1}_\alpha)\xi\\
		&=\lay{\delta}{1}{_\alpha}{^T}\diag(\lay{\hat{z}}{1}{_\alpha})\xi,
	\end{align*}
	and so
	$$d_\gamma G_\gamma=\lay{\delta}{1}{_\alpha}{^T}\diag(\lay{\hat{z}}{1}{_\alpha}),\qquad\frac{\partial G}{\partial\gamma}=\diag(\lay{\hat{z}}{1}{_\alpha})\lay{\delta}{1}{_\alpha}.$$
	
	\item \begin{align*}
		d_\beta G_\beta(\eta)&=d_\beta(\L_y\circ\lay{g}{2}\circ\lay{\Phi}{2}(B,b,\lay{g}{1}\circ BN_\alpha^{\gamma,\lay{z}{1}{_\alpha}}))_\beta(\eta)\\
		&=\lay{\delta}{1}{_\alpha}{^T}\eta,
	\end{align*}
	thus
	$$d_\beta G_\beta=\lay{\delta}{1}{_\alpha}{^T},\qquad\frac{\partial G}{\partial\beta}=\lay{\delta}{1}{_\alpha}.$$
	
	\item \begin{align*}
		d_AG_A(V)&=\lay{\delta}{1}{_\alpha}{^T}\cdot d_\alpha (BN_\alpha^{\gamma,\beta})_{\lay{z}{1}{_\alpha}}d\lay{\Phi}{1}_A(V)\\
		&=\lay{\delta}{1}{_\alpha}{^T}\diag(\gamma) d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}}Vx_\alpha,
	\end{align*}
	and hence
	$$d_AG_A=x_\alpha\lay{\delta}{1}{_\alpha}{^T}\diag(\gamma)d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}},$$
	$$\frac{\partial G}{\partial A}=\diag(\gamma)d_\alpha(\mathcal{N}_\alpha)_{\lay{z}{1}{_\alpha}}\lay{\delta}{1}{_\alpha}x_\alpha{^T}.$$
\end{itemize}

Finally, since
$$\J(\lay{W}{2},\lay{b}{2},\gamma,\beta,\lay{W}{1})=\frac{1}{N}\sum_{\alpha=1}^NG(\lay{W}{2},\lay{b}{2},\gamma,\beta,\lay{W}{1},x_\alpha),$$
we've described our desired gradients after summation.







\begin{comment}
	



Similar to earlier, we define
\begin{align*}
	\lay{\delta}{1}&=[d\lay{g}{1}_{\lay{\tilde{z}}{1}}]^TB^T\lay{\delta}{2}\\
	&=\frac{\partial G}{\partial \lay{\tilde{z}}{1}}\\
	&=[dG_{\lay{\tilde{z}}{1}}]^T,
\end{align*}
and so
\begin{align*}
	dG_\gamma&=dG_{\lay{\tilde{z}}{1}}\cdot d(\lay{\tilde{z}}{1})_\gamma
\end{align*}







For backward propagation, we compute derivatives in coordinates
\begin{align*}
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{z}{\ell}{^\nu}}&=\frac{\lay{\gamma}{\ell}{^i}}{\lay{\sigma}{\ell}{^i}}\delta^i_\nu,\\
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{\gamma}{\ell}{^\nu}}&=\frac{\lay{z}{\ell}{^i}-\lay{\mu}{\ell}{^i}}{\lay{\sigma}{\ell}{^i}}\delta^i_\nu,\\
	\frac{\partial\lay{\tilde{z}}{\ell}{^i}}{\partial\lay{\beta}{\ell}{^\nu}}&=\delta^i_\nu.
\end{align*}

\begin{remark}
	Note that it may seem that $\mu$ and $\sigma$ depend on the variable $z$, but they actually depend on the full set of potential values, and are hence constants.  NOT TRUE
\end{remark}






\end{comment}




\subsection{Inferencing}
We note that in our computation for forward propagation, that our normalization transforms change with out batches.  This leads to ambiguity when predicting a label for a new example.  One fix would be to average our means and variances over our batches.  That is, suppose during our iteration process, we have training-batches of the form $\{\X^k:1\leq k\leq K\}$, where each $\X^k$ has cardinality $|\X^k|=n$.  Then for each hidden-layer $\ell\in\{1,...,L-1\}$, we obtain the means
$$\lay{\mu}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}\lay{z}{\ell},$$
and the variances
$$\lay{\sigma^2}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}(\lay{z}{\ell}-\lay{\mu}{\ell}{_k})^2.$$
That is, for each hidden-layer $\ell$, we have the collection
$$\{\lay{\mu}{\ell}{_k}:1\leq k\leq K\}$$
from which we average again to obtain
$$\lay{\mu}{\ell}:=\frac{1}{K}\sum_{k=1}^K\lay{\mu}{\ell}{_k},$$
and the collection
$$\{\lay{\sigma^2}{\ell}{_k}:1\leq k\leq K\},$$
from which we use the unbiased estimate
$$\lay{\sigma^2}{\ell}:=\frac{n}{n-1}\frac{1}{K}\sum_{k=1}^K\lay{\sigma^2}{\ell}{_k}.$$
These quantities are what we use when computing the batch-normalization transforms of the hidden units for new examples.





\subsection{Algorithm Outline}
Suppose we have a training set $\X$ with which we wish to train a binary classification via an $L$-layer neural network.  Let $N=|\X|$ and let $n=2^p$ be the batch size with $K=\lceil\frac{N}{n}\rceil$ batches per epoch.  Then our algorithm would be as follows:
\begin{enumerate}[1.]
	\item Set hyper-parameters. Initialize parameters.
	\item For $0\leq i\leq\texttt{num\_iters}$:
	\begin{enumerate}[a.]
		\item Generate batches $\{\X^k:1\leq k\leq K\}$.
		\item For $1\leq k\leq K$:
		\begin{enumerate}[i.]
			\item Perform forward propagation on $\X^k$:
			\begin{itemize}
				\item $$\lay{z}{1}=\lay{W}{1}x$$
				\item For $\ell\in\{1,...,L-1\}$:
				\begin{itemize}
					\item $$\lay{z}{\ell}=\lay{W}{\ell}\lay{a}{\ell-1}$$
					\item $$\lay{\mu}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}\lay{z}{\ell}$$
					\item $$\lay{\sigma^2}{\ell}{_k}=\frac{1}{n}\sum_{x\in\X^k}(\lay{z}{\ell}-\lay{\mu}{\ell}{_k})^2$$
					\item $$\lay{\hat{z}}{\ell}=(\lay{\sigma^2}{\ell}{_k}+\epsilon)^{-\frac{1}{2}}\odot(\lay{z}{\ell}-\lay{\mu}{\ell}{_k})$$
					\item $$\lay{\tilde{z}}{\ell}=\lay{\gamma}{\ell}\odot\lay{\hat{z}}{\ell}+\lay{\beta}{\ell}$$
					\item $$\lay{a}{\ell}=\lay{g}{\ell}(\lay{\tilde{z}}{\ell})$$
				\end{itemize}
				\item $$\lay{z}{L}=\lay{W}{L}\lay{a}{L-1}+b$$
				\item $$\lay{a}{L}=\lay{g}{L}(\lay{z}{L})$$
			\end{itemize}
			\item Compute cost $\J$ on $\X^k$.
			\item Apply backwards propagation on $\X^k$ to obtain
			$$\frac{\partial\J}{\partial\lay{W}{\ell}},\quad\frac{\partial\J}{\partial b},\quad\frac{\partial\J}{\partial\lay{\gamma}{\ell}},\quad\frac{\partial\J}{\partial\lay{\beta}{\ell}}.$$
			\item Update parameters.
		\end{enumerate}
	\end{enumerate}
	\item Compute
	$$\lay{\mu}{\ell}=\E[\lay{\mu}{\ell}{_k}],$$
	$$\lay{\sigma^2}{\ell}=\frac{n}{n-1}\E[\lay{\sigma^2}{\ell}{_k}]$$
	\item Return
	$$\lay{W}{\ell},\quad b,\quad\lay{\gamma}{\ell},\quad\lay{\beta}{\ell},\quad\lay{\mu}{\ell},\quad\lay{\sigma^2}{\ell}.$$
\end{enumerate}





\subsection{Better Backpropagation}

We consider a neural network utilizing batch normalization of the form
{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d]
		&{}
		&\R^{\lay{n}{1}}
		\arrow[d]
		&\R^{\lay{n}{1}}
		\arrow[d]
		&{}
		\\
		\R^{\lay{n}{0}\times N}
		\arrow[r]
		&\boxed{\lay{\phi}{1}}
		\arrow[r]
		&\boxed{\lay{N}{1}}
		\arrow[r]
		&\boxed{\lay{\Gamma}{1}}
		\arrow[r]
		&\boxed{\lay{\psi}{1}}
		\arrow[r]
		&\boxed{\lay{G}{1}}
		\arrow[r]
		&\cdots\\
		{}
		&\cdots
		\arrow[r]
		&\boxed{\lay{\phi}{2}}
		\arrow[r]
		&\boxed{\lay{\psi}{2}}
		\arrow[r]
		&\boxed{\lay{G}{2}}
		\arrow[r]
		&\boxed{\L}
		\arrow[r]
		&\R
		\\
		{}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[u]
		&\R^{\lay{n}{2}}
		\arrow[u]
		&{}
		&\R^{\lay{n}{2}\times N}
		\arrow[u]
		&{}
	\end{tikzcd}
\end{equation*}
}
where we have the functions
\begin{enumerate}
	\item $$\lay{G}{\ell}:\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is the broadcasting of the activation unit $\lay{g}{\ell}:\R\to\R$.
	
	\item $$\lay{\phi}{\ell}:\R^{\lay{n}{\ell}\times\lay{n}{\ell-1}}\times\R^{\lay{n}{\ell-1}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\lay{\phi}{\ell}(W,x)=Wx.$$
	
	\item $$\lay{\psi}{\ell}:\R^{\lay{n}{\ell}}\times\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\psi(b,x)=x+b\vec{1}^T,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^{\lay{n}{\ell}}.$$
	
	\item $$\lay{N}{1}:\R^{\lay{n}{1}\times N}\to\R^{\lay{n}{1}\times N}$$
	is the normalization operator given by
	$$\lay{N}{1}:x^i_j\mapsto \frac{x^i_j-\E[x^i]}{\sqrt{\V[x^i]+\epsilon}},$$
	where $\E$ is the expectation operator, i.e.,
	$$\E[x^i]=\frac{1}{N}\sum_{j=1}^Nx^i_j,$$
	and $\V$ is the variance operator, i.e.,
	$$\V[x^i]=\E[(x^i-\E[x^i])^2].$$
	
	\item $$\lay{\Gamma}{\ell}:\R^{\lay{n}{\ell}}\times\R^{\lay{n}{\ell}\times N}\to\R^{\lay{n}{\ell}\times N}$$
	is given by
	$$\Gamma(\gamma, x)=\gamma\vec{1}^T\odot x,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^{\lay{n}{\ell}}.$$
\end{enumerate}

We now consider back-propagating through the network via reverse differentiations as in the following diagram:


{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		&{}
		&\R^{\lay{n}{1}}
		&\R^{\lay{n}{1}}
		&{}
		\\
		\R^{\lay{n}{0}\times N}
		&\boxed{\lay{\phi}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "r_1"]
		&\boxed{\lay{N}{1}}
		\arrow[l, swap, "r"]
		&\boxed{\lay{\Gamma}{1}}
		\arrow[l, swap, "r"]
		\arrow[u, "\hat{r}_1"]
		&\boxed{\lay{B}{2}}
		\arrow[l, swap, "r"]
		\arrow[u, "\tilde{r}_1"]
		&\boxed{\lay{G}{1}}
		\arrow[l, swap, "r"]
		&\cdots
		\arrow[l, swap, "r"]
		\\
		{}
		&\cdots
		&\boxed{\lay{\phi}{2}}
		\arrow[l, swap, "r"]
		\arrow[d, "r_2"]
		&\boxed{\lay{\psi}{2}}
		\arrow[l, swap, "r"]
		\arrow[d, "\cl{r}_2"]
		&\boxed{\lay{G}{2}}
		\arrow[l, swap, "r"]
		&\boxed{\L}
		\arrow[l, swap, "r"]
		&\R
		\arrow[l]
		\\
		{}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		&\R^{\lay{n}{2}}
		&{}
		&\R^{\lay{n}{2}\times N}
		\arrow[u]
		&{}
	\end{tikzcd}
\end{equation*}
}

We now need to consider our individual derivatives:
\begin{enumerate}
	\item Suppose $G:\R^{m\times n}\to\R^{m\times n}$ is the broadcasting of $g:\R\to\R$.  Then for any $(x,\xi)\in T\R^{m\times n}$ we have that
	$$dG_x(\xi)=G'(x)\odot\xi.$$
	Then for any $\zeta\in T_{G(x)}\R^{m\times n}$, we have the reverse derivative is given by
	$$rG_x(\zeta)=G'(x)\odot\zeta.$$
	
	\item Suppose $\phi:\R^{m\times n}\times\R^{n\times N}\to\R^{m\times N}$ is given by
	$$\phi(W,x)=Wx.$$
	Then we have two differential paths to consider:
	\begin{enumerate}
		\item For any $(W,x)\in \R^{m\times n}\times \R^{n\times N}$ and any $\xi\in T_x\R^{n\times N}$, we have that
		\begin{align*}
			d\phi_{(W,x)}(\xi)&=W\cdot\xi\\
			&=L_W(\xi),
		\end{align*}
		and for any $\zeta\in T_{\phi(W,x)}\R^{m\times N}$, we have the reverse differential
		\begin{align*}
			r\phi_{(W,x)}(\zeta)&=W^T\cdot\zeta\\
			&=L_{W^T}(\zeta).
		\end{align*}
		
		\item For any $(W,x)\in\R^{m\times n}\times \R^{n\times N}$ and any $Z\in T_W\R^{m\times n}$, we have that
		\begin{align*}
			d_1\phi_{(W,x)}(Z)&=Z\cdot x\\
			R_x(Z),
		\end{align*}
		and for any $\zeta\in T_{\phi(W,x)}\R^{m\times N}$, we have the reverse differential
		\begin{align*}
			r_1\phi_{(W,x)}(\zeta)&=\zeta\cdot x^T\\
			&=R_{x^T}(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item Suppose $\psi:\R^n\times\R^{n\times N}\to\R^{n\times N}$ is given by
	$$\psi(b,x)=x+b\vec{1}^T,$$
	where
	$$\vec{1}^T=\begin{bmatrix}
		1&1&\cdots&1
	\end{bmatrix}\in\R^N.$$
	Then we look at the two differential paths and for any $(b,x)\in\R^n\times\R^{n\times N}$ any any $\xi\in T_x\R^{n\times N}$, $\eta\in T_b\R^n$ and $\zeta\in T_{\psi(b,x)}\R^{n\times N}$:
	\begin{enumerate}
		\item In the network direction, we have that
		\begin{align*}
			d\psi_{(b,x)}(\xi)=\xi,
		\end{align*}
		with reverse differential
		$$r\psi_{(b,x)}(\zeta)=\zeta.$$
		
		\item In the parameter-space direction, we have that
		\begin{align*}
			\cl{d}\psi_{(b,x)}(\eta)&=\eta\cdot\vec{1}^T\\
			&=R_{\vec{1}^T}(\eta),
		\end{align*}
		with reverse differential
		\begin{align*}
			\cl{r}\psi_{(b,x)}(\zeta)&=\zeta\cdot\vec{1}\\
			&=R_{\vec{1}}(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item Suppose $\Gamma:\R^n\times\R^{n\times N}\to\R^{n\times N}$ is given by
	$$\Gamma(\gamma,x)=\gamma\vec{1}^T\odot x.$$
	The considering the two paths of differentiation, we have that for any $((\gamma,x), (\eta,\xi))\in T(\R^n\times\R^{n\times N})$ and $\zeta\in T_{\Gamma(\gamma,x)}\R^{n\times N}$ that:
	\begin{enumerate}
		\item In the network direction, we have that
		\begin{align*}
			d\Gamma_{(\gamma,x)}(\xi)&=\gamma\vec{1}^T\odot\xi,
		\end{align*}
		with reverse differential
		\begin{align*}
			r\Gamma_{(\gamma,x)}(\zeta)&=\gamma\vec{1}^T\odot\zeta.
		\end{align*}
		
		\item In the parameter-space direction, we have that
		\begin{align*}
			\hat{d}\Gamma_{(\gamma,x)}(\eta)&=\eta\vec{1}^T\odot x\\
			&=\odot_x\circ R_{\vec{1}^T}(\eta),
		\end{align*}
		with reverse differential
		\begin{align*}
			\hat{r}\Gamma_{(\gamma,x)}(\zeta)&=(x\odot\zeta)\cdot\vec{1}\\
			&=R_{\vec{1}}\circ\odot_x(\zeta).
		\end{align*}
	\end{enumerate}
	
	\item As the normalization operator is quite involved, we move its computation to the following section.
\end{enumerate}




\begin{comment}
	

\subsection{Python Implementation}

\TOX{Work in Progress}


\end{comment}


