


\section{Deep Neural Networks}

In this section we discuss a general ``deep'' neural network, which consist of $L$ layers.  That is, we have a network of the form:


{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{b}{1}"]
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[d, swap, "\lay{w}{2}"]
		&\R^{\lay{n}{2}}
		\arrow[d, swap, "\lay{b}{2}"]
		\\
		\R^{\lay{n}{0}}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{\psi}{2}}
		\arrow[r, "\lay{z}{2}"]
		&\cdots\\
		\cdots
		\arrow[r, swap, "\lay{z}{L-1}"]
		&\boxed{\lay{G}{L-1}}
		\arrow[r, swap, "\lay{a}{L-1}"]
		&\boxed{\lay{\phi}{L}}
		\arrow[r, swap, "\lay{u}{L}"]
		&\boxed{\lay{\psi}{L}}
		\arrow[r, swap, "\lay{z}{L}"]
		&\boxed{\lay{G}{L}}
		\arrow[r, swap, "\lay{a}{L}"]
		&\boxed{\L}
		\arrow[r, swap, "\text{loss}"]
		&\R
		\\
		{}
		&{}
		&\R^{\lay{n}{L}\times\lay{n}{L-1}}
		\arrow[u, swap, "\lay{w}{L}"]
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "\lay{b}{L}"]
		&{}
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}


In general nothing fundamentally changes when adding more layers to a network.  We may have different activator functions for each layer, but the general outline of computing forward propagation via composition, and then apply gradient descent by using reverse differentiation to ``backtrack'' through the network.  Here we give a more general outline for computing our desired gradients.

To this end, we reverse our network to use reverse differentiation:

{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		&\R^{\lay{n}{1}}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		&\R^{\lay{n}{2}}
		\\
		\R^{\lay{n}{0}}
		&\boxed{\lay{\phi}{1}}
		\arrow[u, "r_1"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{\psi}{1}}
		\arrow[u, "\cl{r}_1"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{G}{1}}
		\arrow[l, swap, "r"]
		&\boxed{\lay{\phi}{2}}
		\arrow[u,  "r_2"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{\psi}{2}}
		\arrow[u, "\cl{r}_2"]
		\arrow[l, swap, "r"]
		&\cdots
		\arrow[l, swap, "r"]\\
		\cdots
		&\boxed{\lay{G}{L-1}}
		\arrow[l, "r"]
		&\boxed{\lay{\phi}{L}}
		\arrow[d, "r_L"]
		\arrow[l, "r"]
		&\boxed{\lay{\psi}{L}}
		\arrow[d, "\cl{r}_L"]
		\arrow[l, "r"]
		&\boxed{\lay{G}{L}}
		\arrow[l, "r"]
		&\boxed{\L}
		\arrow[l, "r"]
		&\R
		\arrow[l]
		\\
		{}
		&{}
		&\R^{\lay{n}{L}\times\lay{n}{L-1}}
		&\R^{\lay{n}{L}}
		&{}
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}

We note that we essentially have four fundamental reverse differentials to calculate:
\begin{align*}
	&r_\ell\J_{(\lay{w}{\ell},\lay{a}{\ell-1})},\\
	&\cl{r}_\ell\J_{(\lay{b}{\ell},\lay{u}{\ell})}.\\
	&r\J_{(\lay{w}{\ell},\lay{a}{\ell-1}},\\
	&r\J_{(\lay{b}{\ell},\lay{u}{\ell})},
\end{align*}
where the former two are our actual gradients we use for gradient descent, and the latter two are facilitating computations to lead to the next layer down the network.

Moreover, once we've chosen our loss function $\L$, its derivative calculation is done once and it's known.  Similarly, once we've chosen our activating functions $\lay{G}{\ell}$, their differentials are known via \cref{lem: broadcastingDifferential}.




\begin{comment}
	







\subsection{Backward Propagation}
As the general derivation for backpropagation can be easily (if not tediously) generalized from \cref{sec:backPropDerivation} using induction, we give the general outline for computational purposes.

Let $\L:\R^{m_L}\times\R^{m_L}\to\R$ be a generic loss function, and suppose our cost function is given by the usual
$$\J(W,b)=\frac{1}{n}\sum_{j=1}^n\L(\hat{y}_j,y_j).$$
Then from previous computations, we have the following gradients for any $\ell\in\{1,2,...,L\}$, that
\begin{align*}
	\frac{\partial\J}{\partial\lay{W}{\ell}}&=\frac{1}{n}\lay{\delta}{\ell}\lay{a}{\ell-1}{^T}\\
	\frac{\partial\J}{\partial\lay{b}{\ell}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{\ell}{_j}
\end{align*}
where we impose the notation of
$$\lay{a}{0}:=x.$$
So we need only give a full characterization of $\lay{\delta}{\ell}.$. To this end, we define recursively starting at layer-$L$ by
\begin{align*}
	\lay{\delta}{L}{^T}&:=d(\L_y)_{\lay{a}{L}}\cdot d\lay{g}{L}_{\lay{z}{L}},\\
	\lay{\delta}{L-1}{^T}&:=\lay{\delta}{L}{^T}\cdot\lay{W}{L}\cdot d\lay{g}{L-1}_{\lay{z}{L-1}},\\
	&\vdots\\
	\lay{\delta}{\ell}{^T}&:=\lay{\delta}{\ell+1}{^T}\lay{W}{\ell+1}d\lay{g}{\ell}_{\lay{z}{\ell}},\\
	&\vdots\\
	\lay{\delta}{1}{^T}&:=\lay{\delta}{2}{^T}\lay{W}{2}d\lay{g}{1}_{\lay{z}{1}},
\end{align*}
as desired.






%% Python Implementations


\subsection{Implementation in Python via \texttt{numpy}}
We implement a neural network with an arbitrary number of layers and nodes, with the $\relu$ function as the activator on all hidden nodes and the sigmoid function on the output layer for binary classification with the log-loss function.

\lstinputlisting[lastline=208]{src/python/deepNeuralNetworks/npDeepNeuralNetwork.py}


\subsection{Implementation in Python via \texttt{tensorflow}}
We implement a neural network using \texttt{tensorflow.keras}.

\lstinputlisting[lastline=66]{src/python/deepNeuralNetworks/tfDeepNeuralNetwork.py}






\end{comment}


