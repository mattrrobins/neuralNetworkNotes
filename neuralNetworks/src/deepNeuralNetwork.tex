


\section{Deep Neural Networks}

In this section we discuss a general ``deep'' neural network, which consist of $L$ layers.  That is, we have a network of the form:


{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		\arrow[d, swap, "\lay{w}{1}"]
		&\R^{\lay{n}{1}}
		\arrow[d, swap, "\lay{b}{1}"]
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		\arrow[d, swap, "\lay{w}{2}"]
		&\R^{\lay{n}{2}}
		\arrow[d, swap, "\lay{b}{2}"]
		\\
		\R^{\lay{n}{0}}
		\arrow[r, "\lay{a}{0}:=x"]
		&\boxed{\lay{\phi}{1}}
		\arrow[r, "\lay{u}{1}"]
		&\boxed{\lay{\psi}{1}}
		\arrow[r, "\lay{z}{1}"]
		&\boxed{\lay{G}{1}}
		\arrow[r, "\lay{a}{1}"]
		&\boxed{\lay{\phi}{2}}
		\arrow[r, "\lay{u}{2}"]
		&\boxed{\lay{\psi}{2}}
		\arrow[r, "\lay{z}{2}"]
		&\cdots\\
		\cdots
		\arrow[r, swap, "\lay{z}{L-1}"]
		&\boxed{\lay{G}{L-1}}
		\arrow[r, swap, "\lay{a}{L-1}"]
		&\boxed{\lay{\phi}{L}}
		\arrow[r, swap, "\lay{u}{L}"]
		&\boxed{\lay{\psi}{L}}
		\arrow[r, swap, "\lay{z}{L}"]
		&\boxed{\lay{G}{L}}
		\arrow[r, swap, "\lay{a}{L}"]
		&\boxed{\L}
		\arrow[r, swap, "\text{loss}"]
		&\R
		\\
		{}
		&{}
		&\R^{\lay{n}{L}\times\lay{n}{L-1}}
		\arrow[u, swap, "\lay{w}{L}"]
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "\lay{b}{L}"]
		&{}
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}


In general nothing fundamentally changes when adding more layers to a network.  We may have different activator functions for each layer, but the general outline of computing forward propagation via composition, and then apply gradient descent by using reverse differentiation to ``backtrack'' through the network.  Here we give a more general outline for computing our desired gradients.

To this end, we reverse our network to use reverse differentiation:

{\tiny
\begin{equation*}
	\begin{tikzcd}
		{}
		&\R^{\lay{n}{1}\times\lay{n}{0}}
		&\R^{\lay{n}{1}}
		&{}
		&\R^{\lay{n}{2}\times\lay{n}{1}}
		&\R^{\lay{n}{2}}
		\\
		\R^{\lay{n}{0}}
		&\boxed{\lay{\phi}{1}}
		\arrow[u, "r_1"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{\psi}{1}}
		\arrow[u, "\cl{r}_1"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{G}{1}}
		\arrow[l, swap, "r"]
		&\boxed{\lay{\phi}{2}}
		\arrow[u,  "r_2"]
		\arrow[l, swap, "r"]
		&\boxed{\lay{\psi}{2}}
		\arrow[u, "\cl{r}_2"]
		\arrow[l, swap, "r"]
		&\cdots
		\arrow[l, swap, "r"]\\
		\cdots
		&\boxed{\lay{G}{L-1}}
		\arrow[l, "r"]
		&\boxed{\lay{\phi}{L}}
		\arrow[d, "r_L"]
		\arrow[l, "r"]
		&\boxed{\lay{\psi}{L}}
		\arrow[d, "\cl{r}_L"]
		\arrow[l, "r"]
		&\boxed{\lay{G}{L}}
		\arrow[l, "r"]
		&\boxed{\L}
		\arrow[l, "r"]
		&\R
		\arrow[l]
		\\
		{}
		&{}
		&\R^{\lay{n}{L}\times\lay{n}{L-1}}
		&\R^{\lay{n}{L}}
		&{}
		&\R^{\lay{n}{L}}
		\arrow[u, swap, "y"]
		&{}
	\end{tikzcd}
\end{equation*}
}

We compute differentials recursively as follows:
\begin{enumerate}
	\item Define $\lay{\delta}{L}{_j}\in\R^{\lay{n}{L}}$ by
	\begin{align*}
		\lay{\delta}{L}{_j}&:=r(\L\circ\lay{G}{L})_{\lay{z}{L}{_j}}\\
		&=r\lay{G}{L}_{\lay{z}{L}{_j}}\circ r\L_{(\lay{a}{L}{_j},y_j)}\\
		&=\lay{G}{L}{'}(\lay{z}{L}{_j})\odot r\L_{(\lay{a}{L}{_j},y_j)}.
	\end{align*}
	
	\item Compute
	\begin{align*}
		\frac{\partial\J}{\partial\lay{b}{L}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{L}{_j},
	\end{align*}
	and
	\begin{align*}
		\frac{\partial\J}{\partial\lay{w}{L}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{L}{_j}\lay{a}{L-1}{^T}{_j}\\
		&=\frac{1}{N}\lay{\delta}{L}\lay{a}{L-1}{^T}.
	\end{align*}
	
	\item Define $\lay{\delta}{L-1}{_j}\in\R^{\lay{n}{L-1}}$ by
	\begin{align*}
		\lay{\delta}{L-1}{_j}&:=r(\L\circ\lay{G}{L}\circ\lay{\psi}{L}\circ\lay{\phi}{L}\circ\lay{G}{L-1})_{\lay{z}{L-1}{_j}}\\
		&=r\lay{G}{L-1}_{\lay{z}{L-1}{_j}}\circ r\lay{\phi}{L}_{(\lay{w}{L},\lay{a}{L-1}{_j})}\circ r\lay{\psi}{L}_{(\lay{b}{L},\lay{u}{L}{_j})}\circ r\lay{G}{L}_{\lay{z}{L}_j}\circ r\L_{(\lay{a}{L}{_j},y_j)}\\
		&=\lay{G}{L-1}{'}({\lay{z}{L-1}{_j}})\odot\lay{w}{L}{^T}\cdot\lay{\delta}{L}{_j}.
	\end{align*}
	
	\item Compute
	\begin{align*}
		\frac{\partial\J}{\partial\lay{b}{L-1}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{L-1}{_j}
	\end{align*}
	and
	\begin{align*}
		\frac{\partial\J}{\partial\lay{w}{L-1}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{L-1}{_j}\lay{a}{L-2}{^T}{_j}\\
		&=\frac{1}{N}\lay{\delta}{L-1}\lay{a}{L-2}{^T}.
	\end{align*}
	
	\item Given $\lay{\delta}{\ell+1}{_j}\in\R^{\lay{n}{\ell+1}}$, define $\lay{\delta}{\ell}{_j}\in\R^{\lay{n}{\ell}}$ by
	\begin{align*}
		\lay{\delta}{\ell}{_j}:=\lay{G}{\ell}{'}(\lay{z}{\ell}{_j})\odot\lay{w}{\ell+1}{^T}\lay{\delta}{\ell+1}{_j}.
	\end{align*}
	
	\item Compute
	\begin{align*}
		\frac{\partial\J}{\partial\lay{b}{\ell}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{\ell}{_j}
	\end{align*}
	and
	\begin{align*}
		\frac{\partial\J}{\partial\lay{w}{\ell}}&=\frac{1}{N}\sum_{j=1}^N\lay{\delta}{\ell}{_j}\lay{a}{\ell-1}{^T}{_j}\\
		&=\frac{1}{N}\lay{\delta}{\ell}\lay{a}{\ell-1}{^T},
	\end{align*}
	with the caveat that if $\ell=1$, $\lay{a}{0}:=x$, and we've completed the recursion.
\end{enumerate}






\begin{comment}
	







\subsection{Backward Propagation}
As the general derivation for backpropagation can be easily (if not tediously) generalized from \cref{sec:backPropDerivation} using induction, we give the general outline for computational purposes.

Let $\L:\R^{m_L}\times\R^{m_L}\to\R$ be a generic loss function, and suppose our cost function is given by the usual
$$\J(W,b)=\frac{1}{n}\sum_{j=1}^n\L(\hat{y}_j,y_j).$$
Then from previous computations, we have the following gradients for any $\ell\in\{1,2,...,L\}$, that
\begin{align*}
	\frac{\partial\J}{\partial\lay{W}{\ell}}&=\frac{1}{n}\lay{\delta}{\ell}\lay{a}{\ell-1}{^T}\\
	\frac{\partial\J}{\partial\lay{b}{\ell}}&=\frac{1}{n}\sum_{j=1}^n\lay{\delta}{\ell}{_j}
\end{align*}
where we impose the notation of
$$\lay{a}{0}:=x.$$
So we need only give a full characterization of $\lay{\delta}{\ell}.$. To this end, we define recursively starting at layer-$L$ by
\begin{align*}
	\lay{\delta}{L}{^T}&:=d(\L_y)_{\lay{a}{L}}\cdot d\lay{g}{L}_{\lay{z}{L}},\\
	\lay{\delta}{L-1}{^T}&:=\lay{\delta}{L}{^T}\cdot\lay{W}{L}\cdot d\lay{g}{L-1}_{\lay{z}{L-1}},\\
	&\vdots\\
	\lay{\delta}{\ell}{^T}&:=\lay{\delta}{\ell+1}{^T}\lay{W}{\ell+1}d\lay{g}{\ell}_{\lay{z}{\ell}},\\
	&\vdots\\
	\lay{\delta}{1}{^T}&:=\lay{\delta}{2}{^T}\lay{W}{2}d\lay{g}{1}_{\lay{z}{1}},
\end{align*}
as desired.



\end{comment}



%% Python Implementations


\subsection{Implementation in Python via \texttt{numpy}}
We implement a neural network with an arbitrary number of layers and nodes, with the $\relu$ function as the activator on all hidden nodes and the sigmoid function on the output layer for binary classification with the log-loss function.

\lstinputlisting[lastline=208]{src/python/deepNeuralNetworks/npDeepNeuralNetwork.py}


\subsection{Implementation in Python via \texttt{tensorflow}}
We implement a neural network using \texttt{tensorflow.keras}.

\lstinputlisting[lastline=66]{src/python/deepNeuralNetworks/tfDeepNeuralNetwork.py}








