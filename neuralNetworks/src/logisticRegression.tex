

\section{Logistic Regression}

We begin with a review of binary classification and logistic regression.  To this end, suppose we have we have training examples $x\in\R^{m\times n}$ with binary labels $y\in\{0,1\}^{1\times n}$.  We desire to train a model which yields an output $a$ which represents
$$a=\P(y=1|x).$$
To this end, let $\sigma:\R\to(0,1)$ denote the sigmoid function, i.e.,
$$\sigma(z)=\frac{1}{1+e^{-z}},$$
and let $w\in\R^m$, $b\in\R$, and let
$$a=\sigma(w^Tx+b).$$
To analyze the accuracy of model, we need a way to compare $y$ and $a$, and ideally this functional comparison can be optimized with respect to $(w,b)$ in such a way to minimize the error.  To this end, we note that
$$\P(y|x)=a^y(1-a)^{1-y},$$
or rather
$$\P(y=1|x)=a,\qquad\P(y=0|x)=1-a,$$
so $\P(y|x)$ represents the corrected probability.    Now since we want
$$a\approx 1\quad\text{when } y=1,$$
and
$$a\approx 0\quad\text{when } y=0,$$
and $0\leq a\leq 1$, any error using differences won't be refined enough to analyze when tuning the model.  Moreover, since introducing the sigmoid function, our usual mean-squared-error function won't be convex.  This leads us to apply the $\log$ function, which when restricted to $(0,1)$ is a bijective mapping of $(0,1)\to(-\infty,0)$.  This leads us to define our log-loss function
\begin{align*}
	\L(a,y)&=-\log(\P(y|x))\\
	&=-\log\left(a^y(1-a)^{1-y}\right)\\
	&=-\left[y\log(a)+(1-y)\log(1-a)\right],
\end{align*}
and finally, since we wish to analyze how our model performs on the entire training set, we need to average our log-loss functions to obtain our cost function $\J$ defined by
\begin{align*}
	\J(w,b)&=\frac{1}{n}\sum_{j=1}^n\L(a_j,y_j)\\
	&=-\frac{1}{n}\sum_{j=1}^n\left[y_j\log(a_j)+(1-y_j)\log(1-a_j)\right]\\
	&=-\frac{1}{n}\sum_{j=1}^n\left[y_j\log(\sigma(w^Tx_j+b))+(1-y_j)\log(1-\sigma(w^Tx_j+b))\right].
\end{align*}

\subsection{The Gradient}

To compute the gradient of our cost function $\J$, we first write $\J$ as a sum of compositions as follows:  We have the log-loss function considered as a map $\L:(0,1)\times\R\to\R$,
$$\L(a,y)=-\left[y\log(a)+(1-y)\log(1-a)\right],$$
we have the sigmoid function $\sigma:\R\to(0,1)$ with $\sigma(z)=a$ and $\sigma'(z)=a(1-a)$, and we have the collection of affine-functionals $\phi_x:\R^m\times\R\to\R$ given by
$$\phi_x(w,b)=w^Tx+b,$$
for which we fix an arbitrary $x\in\R^m$ and write $\phi=\phi_x$, and set $z=\phi(w,b)$.  Finally, we introduce the auxiliary function $\mathcal{L}:\R^m\times\R\to\R$ given by
$$\mathcal{L}(w,b)=\L(\sigma(\phi(w,b)),y).$$
Then by the chain rule, we have that
\begin{align*}
	d\mathcal{L}&=d_a\L(a,y)\circ d\sigma(z)\circ d_w\phi(w,b)\\
	&=\left[-\frac{y}{a}+\frac{1-y}{1-a}\right]\cdot a(1-a)\cdot\begin{bmatrix}
		x^T&1
	\end{bmatrix}\\
	&=[-y(1-a)+a(1-y)]\cdot \begin{bmatrix}
		x^T&1
	\end{bmatrix}\\
	&=(a-y)\begin{bmatrix}
		x^T&1
	\end{bmatrix}
\end{align*}\HOX{Composition turns into matrix multiplication in the tangent space.}

Moreover, for function $f:\R^N\to\R$ in Euclidean space, we have that $\nabla f = (df)^T$, and hence that
$$\nabla\mathcal{L}(w,b)=(a-y)\begin{bmatrix}
	x\\
	1
\end{bmatrix},$$
or rather
$$\partial_w\L(a,y)=(a-y)x,\qquad \partial_b\L(a,y)=a-y.$$
Finally, since our cost function $\J$ is the sum-log-loss, we have by linearity that
\begin{align*}
	\partial_w\J(w,b)&=\frac{1}{n}\sum_{j=1}^n(a_j-y_j)x_j\\
	&=\frac{1}{n}((a-y)\cdot x^T)^T\\
	&=\frac{1}{n}x\cdot(a-y)^T
\end{align*}
and
$$\partial_b\J(w,b)=\frac{1}{n}\sum_{j=1}^n(a_j-y_j).$$


\subsection{Vectorization in Python}
Here we include the general code to train a model using logistic regression with $L_2$-regularization.  We also include the \\\texttt{sklearn.linear\_model.LogisticRegression} class to compare.

\lstinputlisting[lastline=324]{src/python/logisticRegression/logisticRegression.py}
