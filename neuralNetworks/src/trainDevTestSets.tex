

\section{Training, Development and Test Sets}

Let $\D=\{(x_j,y_j)\in\R^m\times\R^K:1\leq j\leq N\}$ denote a dataset.  Then we partition $\D$ into three distinct sets
$$\D=\X+\mathcal{D}+\mathcal{T},$$
where $\X$ is called our \textit{training set}, $\mathcal{D}$ is called our \textit{development, or cross-validation set}, and $\mathcal{T}$ is called our \textit{test set}. We make this partition randomly, however, if $N=|\D|\leq 10^4$, we see a partition following the following ratios:
$$n_X:=|\X|\approx \frac{3}{5}N,$$
$$n_D:=|\mathcal{D}|\approx\frac{1}{5}N,$$
and
$$n_T:=|\mathfrak{T}|\approx\frac{1}{5}N.$$
If however, we have a very large dataset (i.e., $N>10^4$), then we assume a much smaller ratio of something similar to
$$\frac{n_X}{N}\approx0.98,\qquad\frac{n_D}{N}\approx 0.01,\qquad\frac{n_T}{N}\approx 0.01.$$

In general, we use our training set $\X$ to train our parameters $\lay{W}{\ell}$ and $\lay{b}{\ell}$, we use our development set $\mathcal{D}$ to tune our hyper-parameters (i.e., learning rate, number of layers, number of nodes per layer, activation function, number of iterations to perform gradient descent, regularization parameters, etc), and we use our test set $\mathcal{T}$ to evaluate the accuracy of our model.  Since we're partitioning our dataset to better increase the accuracy of our model, we need to define an error function.  To this end, define $\mathcal{E}:2^\D\to[0,1]$ by
\begin{align*}
	\mathcal{E}(\mathcal{A})&=\frac{1}{|\mathcal{A}|}\sum_{(x,y)\in\mathcal{A}}\varepsilon(x,y),
\end{align*}
where $\varepsilon:\D\to\{0,1\}$ is defined by
$$\varepsilon(x,y)=\begin{cases}
	1&\text{if }y = \hat{y}(x)\\
	0&\text{else.}
\end{cases}$$

From our partition and error function we can make several claims of the fitting of our model to our data.  Indeed, let $\epsilon>0$ be a small percentage (with exact value depending on specific examples), then:
\begin{itemize}
	\item If $\mathcal{E}(\X)<\epsilon$ and $\mathcal{E}(\X)<\mathcal{E}(\mathcal{D})<\sim10\epsilon$, then we say our model has \textit{high variance} since our model is overfitting the data.
	\item If $\mathcal{E}(\X)\approx\mathcal{E}(\mathcal{D})>\sim10\epsilon$, then we say our model has \textit{high bias} since our model is underfitting the data.
	\item If $10\epsilon\sim<\mathcal{E}(\X)\ll\mathcal{E}(\mathcal{D})$, then we say our model has both high bias (since it doesn't fit our training data well) and high variance (because the model fits the training data better than the development data).
	\item If $\mathcal{E}(\X),\mathcal{E}(\mathcal{D})<\epsilon$, then we say the model has both low bias and low variance.
\end{itemize}

\begin{remark}
	The interpretations of our error percentage is based on two crucial assumptions:
	\begin{itemize}
		\item $\mathcal{D}$ and $\mathcal{T}$ come from samplings with the same distribution of outputs (i.e., if we're determining whether a collection of images contain a cat, we should never have that $\mathcal{D}$ is mostly cat pictures, and $\mathcal{T}$ is mostly non-cat pictures).
		\item The optimal error for the model is approximately $0\%$.  That is, if a human were looking at the data, they could determine the correct response with negligible error.  This is sometimes called the \textit{Bayes error}.
	\end{itemize}
	If either of these assumptions fail to hold, other methods of analysis may be required to obtain meaningful insights for the performance of our model.
\end{remark}

A methodology for using errors could be as follows
\begin{enumerate}[1.]
	\item Check $\mathcal{E}(\X)$ for high bias.
		\begin{enumerate}[a.]
			\item If ``Yes'', then we can try a bigger network, we can train longer, or we can change the neural network architecture. Then we return to (1.).
			\item If ``No'', then we move to (2.).
		\end{enumerate}
	\item Check $\mathcal{E}(\mathcal{D})$ for high variance.
		\begin{enumerate}[a.]
			\item If ``Yes'', then we can try to get more data, try regularization, or try changing the neural network architecture.  Then we return to (1.).
			\item If ``No'', then we're done.
		\end{enumerate}
\end{enumerate}


\subsubsection{Python Implementation}

To implement a partitioning we could do something like the following:
\lstinputlisting[firstline=1,lastline=40]{src/py/miscSnippets.py}


